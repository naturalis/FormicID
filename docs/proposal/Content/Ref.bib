@article{Marcus2018,
abstract = {Although deep learning has historical roots going back decades, neither the term "deep learning" nor the approach was popular just over five years ago, when the field was reignited by papers such as Krizhevsky, Sutskever and Hinton's now classic (2012) deep network model of Imagenet. What has the field discovered in the five subsequent years? Against a background of considerable progress in areas such as speech recognition, image recognition, and game playing, and considerable enthusiasm in the popular press, I present ten concerns for deep learning, and suggest that deep learning must be supplemented by other techniques if we are to reach artificial general intelligence.},
archivePrefix = {arXiv},
arxivId = {1801.00631},
author = {Marcus, Gary},
eprint = {1801.00631},
file = {:Users/nijram13/Documents/Mendeley Desktop/Marcus/2018/Deep Learning A Critical Appraisal/2018 - Marcus - Deep Learning A Critical Appraisal - arXiv preprint arXiv1801.00631.pdf:pdf},
journal = {arXiv preprint arXiv:1801.00631},
mendeley-groups = {CNN internship/Background info},
month = {jan},
pages = {1--27},
title = {{Deep Learning: A Critical Appraisal}},
url = {http://arxiv.org/abs/1801.00631},
year = {2018}
}
@inproceedings{Dodge2016,
abstract = {Image quality is an important practical challenge that is often overlooked in the design of machine vision systems. Commonly, machine vision systems are trained and tested on high quality image datasets, yet in practical applications the input images can not be assumed to be of high quality. Recently, deep neural networks have obtained state-of-the-art performance on many machine vision tasks. In this paper we provide an evaluation of 4 state-of-the-art deep neural network models for image classification under quality distortions. We consider five types of quality distortions: blur, noise, contrast, JPEG, and JPEG2000 compression. We show that the existing networks are susceptible to these quality distortions, particularly to blur and noise. These results enable future work in developing deep neural networks that are more invariant to quality distortions.},
archivePrefix = {arXiv},
arxivId = {1604.04004},
author = {Dodge, Samuel and Karam, Lina},
booktitle = {arXiv preprint arXiv:1604.04004},
doi = {10.1109/QoMEX.2016.7498955},
eprint = {1604.04004},
file = {:Users/nijram13/Documents/Mendeley Desktop/Dodge, Karam/2016/Understanding how image quality affects deep neural networks/2016 - Dodge, Karam - Understanding how image quality affects deep neural networks - arXiv preprint arXiv1604.04004.pdf:pdf},
mendeley-groups = {CNN internship/Background info},
month = {jun},
pages = {1--6},
publisher = {IEEE},
title = {{Understanding how image quality affects deep neural networks}},
url = {http://ieeexplore.ieee.org/document/7498955/},
year = {2016}
}
@article{Ba2013,
abstract = {Currently, deep neural networks are the state of the art on problems such as speech recognition and computer vision. In this extended abstract, we show that shallow feed-forward networks can learn the complex functions previously learned by deep nets and achieve accuracies previously only achievable with deep models. Moreover, in some cases the shallow neural nets can learn these deep functions using a total number of parameters similar to the original deep model. We evaluate our method on the TIMIT phoneme recognition task and are able to train shallow fully-connected nets that perform similarly to complex, well-engineered, deep convolutional architectures. Our success in training shallow neural nets to mimic deeper models suggests that there probably exist better algorithms for training shallow feed-forward nets than those currently available.},
archivePrefix = {arXiv},
arxivId = {1312.6184},
author = {Ba, Lei Jimmy and Caruana, Rich},
doi = {10.1038/nature14539},
eprint = {1312.6184},
file = {:Users/nijram13/Documents/Mendeley Desktop/Ba, Caruana/2013/Do Deep Nets Really Need to be Deep/2013 - Ba, Caruana - Do Deep Nets Really Need to be Deep - arXiv preprint arXiv1312.6184.pdf:pdf},
isbn = {3135786504},
issn = {0028-0836},
journal = {arXiv preprint arXiv:1312.6184},
mendeley-groups = {CNN internship/Background info},
title = {{Do Deep Nets Really Need to be Deep?}},
url = {http://arxiv.org/abs/1312.6184 http://www.nature.com/articles/nature14539},
year = {2013}
}
@article{Schulman2017,
abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
archivePrefix = {arXiv},
arxivId = {1707.06347},
author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
eprint = {1707.06347},
file = {:Users/nijram13/Documents/Mendeley Desktop/Schulman et al/2017/Proximal Policy Optimization Algorithms/2017 - Schulman et al. - Proximal Policy Optimization Algorithms - arXiv preprint arXiv1707.06347.pdf:pdf},
journal = {arXiv preprint arXiv:1707.06347},
mendeley-groups = {CNN internship/Optimization},
month = {jul},
pages = {1--12},
title = {{Proximal Policy Optimization Algorithms}},
url = {http://arxiv.org/abs/1707.06347},
year = {2017}
}
@article{Mishkin2017,
abstract = {The paper systematically studies the impact of a range of recent advances in CNN architectures and learning methods on the object categorization (ILSVRC) problem. The evalution tests the influence of the following choices of the architecture: non-linearity (ReLU, ELU, maxout, compatibility with batch normalization), pooling variants (stochastic, max, average, mixed), network width, classifier design (convolutional, fully-connected, SPP), image pre-processing, and of learning parameters: learning rate, batch size, cleanliness of the data, etc. The performance gains of the proposed modifications are first tested individually and then in combination. The sum of individual gains is bigger than the observed improvement when all modifications are introduced, but the "deficit" is small suggesting independence of their benefits. We show that the use of 128x128 pixel images is sufficient to make qualitative conclusions about optimal network structure that hold for the full size Caffe and VGG nets. The results are obtained an order of magnitude faster than with the standard 224 pixel images.},
archivePrefix = {arXiv},
arxivId = {1606.02228},
author = {Mishkin, Dmytro and Sergievskiy, Nikolay and Matas, Jiri},
doi = {10.1016/j.cviu.2017.05.007},
eprint = {1606.02228},
file = {:Users/nijram13/Documents/Mendeley Desktop/Mishkin, Sergievskiy, Matas/2017/Systematic evaluation of CNN advances on the Imagenet/2017 - Mishkin, Sergievskiy, Matas - Systematic evaluation of CNN advances on the Imagenet - arXiv preprint arXiv1606.02228.pdf:pdf},
journal = {arXiv preprint arXiv:1606.02228},
keywords = {benchmark,cnn,imagenet,non-linearity,pooling},
mendeley-groups = {CNN internship/Background info},
month = {aug},
pages = {11--19},
title = {{Systematic evaluation of CNN advances on the Imagenet}},
url = {http://arxiv.org/abs/1606.02228{\%}0Ahttp://dx.doi.org/10.1016/j.cviu.2017.05.007 http://linkinghub.elsevier.com/retrieve/pii/S1077314217300814},
volume = {161},
year = {2017}
}
@article{Schuettpelz2017,
author = {Schuettpelz, Eric and Frandsen, Paul and Dikow, Rebecca and Brown, Abel and Orli, Sylvia and Peters, Melinda and Metallo, Adam and Funk, Vicki and Dorr, Laurence},
doi = {10.3897/BDJ.5.e21139},
file = {:Users/nijram13/Documents/Mendeley Desktop/Schuettpelz et al/2017/Applications of deep convolutional neural networks to digitized natural history collections/2017 - Schuettpelz et al. - Applications of deep convolutional neural networks to digitized natural history collections - Biodiversity D.pdf:pdf},
issn = {1314-2828},
journal = {Biodiversity Data Journal},
keywords = {Background Info,FormicID CNN},
mendeley-groups = {CNN internship/Background info},
mendeley-tags = {Background Info,FormicID CNN},
month = {nov},
number = {e21139},
title = {{Applications of deep convolutional neural networks to digitized natural history collections}},
url = {https://bdj.pensoft.net/articles.php?id=21139},
volume = {5},
year = {2017}
}
@article{Koushik2016,
abstract = {In this paper we propose a simple and efficient method for improving stochastic gradient descent methods by using feedback from the objective function. The method tracks the relative changes in the objective function with a running average, and uses it to adaptively tune the learning rate in stochastic gradient descent. We specifically apply this idea to modify Adam, a popular algorithm for training deep neural networks. We conduct experiments to compare the resulting algorithm, which we call Eve, with state of the art methods used for training deep learning models. We train CNNs for image classification, and RNNs for language modeling and question answering. Our experiments show that Eve outperforms all other algorithms on these benchmark tasks. We also analyze the behavior of the feedback mechanism during the training process.},
archivePrefix = {arXiv},
arxivId = {1611.01505},
author = {Koushik, Jayanth and Hayashi, Hiroaki},
eprint = {1611.01505},
file = {:Users/nijram13/Documents/Mendeley Desktop/Koushik, Hayashi/2016/Improving Stochastic Gradient Descent with Feedback/2016 - Koushik, Hayashi - Improving Stochastic Gradient Descent with Feedback - arXiv preprint arXiv1611.01505v1.pdf:pdf},
journal = {arXiv preprint arXiv:1611.01505v1},
keywords = {FormicID CNN,Optimization},
mendeley-groups = {CNN internship/Optimization},
mendeley-tags = {FormicID CNN,Optimization},
month = {nov},
title = {{Improving Stochastic Gradient Descent with Feedback}},
url = {http://arxiv.org/abs/1611.01505},
year = {2016}
}
@article{George2017,
abstract = {Learning from few examples and generalizing to dramatically different situations are capabilities of human visual intelligence that are yet to be matched by leading machine learning models. By drawing inspiration from systems neuroscience, we introduce a probabilistic generative model for vision in which message-passing based inference handles recognition, segmentation and reasoning in a unified way. The model demonstrates excellent generalization and occlusion-reasoning capabilities, and outperforms deep neural networks on a challenging scene text recognition benchmark while being 300-fold more data efficient. In addition, the model fundamentally breaks the defense of modern text-based CAPTCHAs by generatively segmenting characters without CAPTCHA-specific heuristics. Our model emphasizes aspects like data efficiency and compositionality that may be important in the path toward general artificial intelligence.},
author = {George, Dileep and Lehrach, Wolfgang and Kansky, Ken and L{\'{a}}zaro-Gredilla, M. and Laan, Christopher and Marthi, Bhaskara and Lou, Xinghua and Meng, Zaoshi and Liu, Yi and Wang, Huayan and Lavin, Alex and Phoenix, D. Scott},
doi = {10.1126/science.aag2612},
file = {:Users/nijram13/Documents/Mendeley Desktop/George et al/2017/A generative vision model that trains with high data efficiency and breaks text-based CAPTCHAs/2017 - George et al. - A generative vision model that trains with high data efficiency and breaks text-based CAPTCHAs - Science.pdf:pdf},
issn = {0036-8075},
journal = {Science},
keywords = {Background Info,FormicID CNN},
mendeley-groups = {CNN internship/Background info},
mendeley-tags = {Background Info,FormicID CNN},
month = {oct},
pmid = {29074582},
title = {{A generative vision model that trains with high data efficiency and breaks text-based CAPTCHAs}},
url = {http://science.sciencemag.org/content/sci/early/2017/10/26/science.aag2612.full.pdf http://www.sciencemag.org/lookup/doi/10.1126/science.aag2612 http://www.ncbi.nlm.nih.gov/pubmed/29074582},
volume = {10},
year = {2017}
}
@article{Janocha2017,
abstract = {Deep neural networks are currently among the most commonly used classifiers. Despite easily achieving very good performance, one of the best selling points of these models is their modular design - one can conveniently adapt their architecture to specific needs, change connectivity patterns, attach specialised layers, experiment with a large amount of activation functions, normalisation schemes and many others. While one can find impressively wide spread of various configurations of almost every aspect of the deep nets, one element is, in authors' opinion, underrepresented - while solving classification problems, vast majority of papers and applications simply use log loss. In this paper we try to investigate how particular choices of loss functions affect deep models and their learning dynamics, as well as resulting classifiers robustness to various effects. We perform experiments on classical datasets, as well as provide some additional, theoretical insights into the problem. In particular we show that L1 and L2 losses are, quite surprisingly, justified classification objectives for deep nets, by providing probabilistic interpretation in terms of expected misclassification. We also introduce two losses which are not typically used as deep nets objectives and show that they are viable alternatives to the existing ones.},
archivePrefix = {arXiv},
arxivId = {1702.05659},
author = {Janocha, Katarzyna and Czarnecki, Wojciech Marian},
doi = {10.4467/20838476SI.16.004.6185},
eprint = {1702.05659},
file = {:Users/nijram13/Documents/Mendeley Desktop/Janocha, Czarnecki/2017/On Loss Functions for Deep Neural Networks in Classification/2017 - Janocha, Czarnecki - On Loss Functions for Deep Neural Networks in Classification - arXiv preprint ArXiv1702.05659.pdf:pdf},
issn = {20838476},
journal = {arXiv preprint ArXiv:1702.05659},
keywords = {FormicID CNN,Optimization},
mendeley-groups = {CNN internship/Optimization},
mendeley-tags = {FormicID CNN,Optimization},
month = {feb},
title = {{On Loss Functions for Deep Neural Networks in Classification}},
url = {http://arxiv.org/abs/1702.05659 http://www.ejournals.eu/Schedae-Informaticae/2016/Volume-25/art/9009/},
year = {2017}
}
@article{Huang2016,
abstract = {Very deep convolutional networks with hundreds of layers have led to significant reductions in error on competitive benchmarks. Although the unmatched expressiveness of the many layers can be highly desirable at test time, training very deep networks comes with its own set of challenges. The gradients can vanish, the forward flow often diminishes, and the training time can be painfully slow. To address these problems, we propose stochastic depth, a training procedure that enables the seemingly contradictory setup to train short networks and use deep networks at test time. We start with very deep networks but during training, for each mini-batch, randomly drop a subset of layers and bypass them with the identity function. This simple approach complements the recent success of residual networks. It reduces training time substantially and improves the test error significantly on almost all data sets that we used for evaluation. With stochastic depth we can increase the depth of residual networks even beyond 1200 layers and still yield meaningful improvements in test error (4.91{\%} on CIFAR-10).},
archivePrefix = {arXiv},
arxivId = {1603.09382},
author = {Huang, Gao and Sun, Yu and Liu, Zhuang and Sedra, Daniel and Weinberger, Kilian},
doi = {10.1007/978-3-319-46493-0_39},
eprint = {1603.09382},
file = {:Users/nijram13/Documents/Mendeley Desktop/Huang et al/2016/Deep Networks with Stochastic Depth/2016 - Huang et al. - Deep Networks with Stochastic Depth - arXiv preprint arXiv1603.09382v3.pdf:pdf},
isbn = {9783319464930},
issn = {0302-9743},
journal = {arXiv preprint arXiv:1603.09382v3},
keywords = {ConvNets,FormicID CNN},
mendeley-groups = {CNN internship/ConvNets},
mendeley-tags = {ConvNets,FormicID CNN},
month = {mar},
pmid = {4520227},
title = {{Deep Networks with Stochastic Depth}},
url = {http://arxiv.org/abs/1603.09382 http://link.springer.com/10.1007/978-3-319-46493-0{\_}39},
year = {2016}
}
@article{Donahue2013,
abstract = {We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms.},
archivePrefix = {arXiv},
arxivId = {1310.1531},
author = {Donahue, Jeff and Jia, Yangqing and Vinyals, Oriol and Hoffman, Judy and Zhang, Ning and Tzeng, Eric and Darrell, Trevor},
eprint = {1310.1531},
file = {:Users/nijram13/Documents/Mendeley Desktop/Donahue et al/2013/DeCAF A Deep Convolutional Activation Feature for Generic Visual Recognition/2013 - Donahue et al. - DeCAF A Deep Convolutional Activation Feature for Generic Visual Recognition - arXiv preprint arXiv1310.1531.pdf:pdf},
isbn = {9781634393973},
issn = {1938-7228},
journal = {arXiv preprint arXiv:1310.1531},
keywords = {Background Info,FormicID CNN},
mendeley-groups = {CNN internship/Background info},
mendeley-tags = {Background Info,FormicID CNN},
title = {{DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition}},
url = {http://arxiv.org/abs/1310.1531},
year = {2013}
}
@article{Loshchilov2016,
abstract = {Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14{\%} and 16.21{\%}, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR},
archivePrefix = {arXiv},
arxivId = {1608.03983v5},
author = {Loshchilov, Ilya and Hutter, Frank},
eprint = {1608.03983v5},
file = {:Users/nijram13/Documents/Mendeley Desktop/Loshchilov, Hutter/2016/SGDR Stochastic Gradient Descent with Warm Restarts/2016 - Loshchilov, Hutter - SGDR Stochastic Gradient Descent with Warm Restarts - arXiv preprint arXiv1608.03983v5.pdf:pdf},
journal = {arXiv preprint arXiv:1608.03983v5},
keywords = {FormicID CNN,Optimization},
mendeley-groups = {CNN internship/Optimization},
mendeley-tags = {FormicID CNN,Optimization},
month = {aug},
pages = {1--16},
title = {{SGDR: Stochastic Gradient Descent with Warm Restarts}},
url = {http://arxiv.org/abs/1608.03983v5},
year = {2016}
}
@article{Xu2015,
abstract = {In this paper we investigate the performance of different types of rectified activation functions in convolutional neural network: standard rectified linear unit (ReLU), leaky rectified linear unit (Leaky ReLU), parametric rectified linear unit (PReLU) and a new randomized leaky rectified linear units (RReLU). We evaluate these activation function on standard image classification task. Our experiments suggest that incorporating a non-zero slope for negative part in rectified activation units could consistently improve the results. Thus our findings are negative on the common belief that sparsity is the key of good performance in ReLU. Moreover, on small scale dataset, using deterministic negative slope or learning it are both prone to overfitting. They are not as effective as using their randomized counterpart. By using RReLU, we achieved 75.68$\backslash${\%} accuracy on CIFAR-100 test set without multiple test or ensemble.},
archivePrefix = {arXiv},
arxivId = {1505.00853},
author = {Xu, Bing and Wang, Naiyan and Chen, Tianqi and Li, Mu},
eprint = {1505.00853},
file = {:Users/nijram13/Documents/Mendeley Desktop/Xu et al/2015/Empirical Evaluation of Rectified Activations in Convolutional Network/2015 - Xu et al. - Empirical Evaluation of Rectified Activations in Convolutional Network - arXiv preprint arXiv1505.00853v2.pdf:pdf},
journal = {arXiv preprint arXiv:1505.00853v2},
keywords = {FormicID CNN,Optimization},
mendeley-groups = {CNN internship/Optimization},
mendeley-tags = {FormicID CNN,Optimization},
month = {may},
title = {{Empirical Evaluation of Rectified Activations in Convolutional Network}},
url = {http://arxiv.org/abs/1505.00853v2 http://arxiv.org/abs/1505.00853},
year = {2015}
}
@article{Held2015,
abstract = {Deep learning methods have typically been trained on large datasets in which many training examples are available. However, many real-world product datasets have only a small number of images available for each product. We explore the use of deep learning methods for recognizing object instances when we have only a single training example per class. We show that feedforward neural networks outperform state-of-the-art methods for recognizing objects from novel viewpoints even when trained from just a single image per object. To further improve our performance on this task, we propose to take advantage of a supplementary dataset in which we observe a separate set of objects from multiple viewpoints. We introduce a new approach for training deep learning methods for instance recognition with limited training data, in which we use an auxiliary multi-view dataset to train our network to be robust to viewpoint changes. We find that this approach leads to a more robust classifier for recognizing objects from novel viewpoints, outperforming previous state-of-the-art approaches including keypoint-matching, template-based techniques, and sparse coding.},
archivePrefix = {arXiv},
arxivId = {1507.08286},
author = {Held, David and Thrun, Sebastian and Savarese, Silvio},
eprint = {1507.08286},
file = {:Users/nijram13/Documents/Mendeley Desktop/Held, Thrun, Savarese/2015/Deep Learning for Single-View Instance Recognition/2015 - Held, Thrun, Savarese - Deep Learning for Single-View Instance Recognition - arXiv preprint arXiv1507.08286.pdf:pdf},
isbn = {9781467380256},
journal = {arXiv preprint arXiv:1507.08286},
keywords = {ConvNets,FormicID CNN},
mendeley-groups = {CNN internship/ConvNets},
mendeley-tags = {ConvNets,FormicID CNN},
month = {jul},
title = {{Deep Learning for Single-View Instance Recognition}},
url = {http://arxiv.org/abs/1507.08286},
year = {2015}
}
@article{Huang2016,
abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and models are available at https://github.com/liuzhuang13/DenseNet .},
archivePrefix = {arXiv},
arxivId = {1608.06993},
author = {Huang, Gao and Liu, Zhuang and Weinberger, Kilian Q. and van der Maaten, Laurens},
eprint = {1608.06993},
file = {:Users/nijram13/Documents/Mendeley Desktop/Huang et al/2016/Densely Connected Convolutional Networks/2016 - Huang et al. - Densely Connected Convolutional Networks - arXiv preprint arXiv608.06993.pdf:pdf},
issn = {0002-9645},
journal = {arXiv preprint arXiv:608.06993},
keywords = {ConvNets,FormicID CNN},
mendeley-groups = {CNN internship/ConvNets},
mendeley-tags = {ConvNets,FormicID CNN},
month = {aug},
title = {{Densely Connected Convolutional Networks}},
url = {http://arxiv.org/abs/1608.06993},
year = {2016}
}
@misc{Chollet2015,
author = {Chollet, Fran{\c{c}}ois},
booktitle = {Github},
keywords = {FormicID CNN,Websites},
mendeley-groups = {CNN internship/Websites},
mendeley-tags = {FormicID CNN,Websites},
title = {{Keras}},
url = {https://github.com/fchollet/keras},
year = {2015}
}
@article{Zeiler2014,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky $\backslash$etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
archivePrefix = {arXiv},
arxivId = {1311.2901v3},
author = {Zeiler, Matthew D and Fergus, Rob},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {1311.2901v3},
file = {:Users/nijram13/Documents/Mendeley Desktop/Zeiler, Fergus/2014/Visualizing and Understanding Convolutional Networks/2014 - Zeiler, Fergus - Visualizing and Understanding Convolutional Networks - arXiv preprint arXiv1311.2901v3.pdf:pdf},
isbn = {978-3-319-10589-5},
issn = {978-3-319-10589-5},
journal = {arXiv preprint arXiv:1311.2901v3},
keywords = {FormicID CNN,Visualizing},
mendeley-groups = {CNN internship/Visualizing},
mendeley-tags = {FormicID CNN,Visualizing},
month = {nov},
title = {{Visualizing and Understanding Convolutional Networks}},
url = {https://arxiv.org/abs/1311.2901v3 http://arxiv.org/abs/1311.2901 http://link.springer.com/10.1007/978-3-319-10590-1{\_}53},
year = {2014}
}
@article{Simonyan2013,
abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
archivePrefix = {arXiv},
arxivId = {1312.6034v2},
author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
eprint = {1312.6034v2},
file = {:Users/nijram13/Documents/Mendeley Desktop/Simonyan, Vedaldi, Zisserman/2013/Deep Inside Convolutional Networks Visualising Image Classification Models and Saliency Maps/2013 - Simonyan, Vedaldi, Zisserman - Deep Inside Convolutional Networks Visualising Image Classification Models and Saliency Maps - arX.pdf:pdf},
journal = {arXiv preprint arXiv:1312.6034v2},
keywords = {FormicID CNN,Visualizing},
mendeley-groups = {CNN internship/Visualizing},
mendeley-tags = {FormicID CNN,Visualizing},
month = {dec},
title = {{Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps}},
url = {http://arxiv.org/abs/1312.6034v2},
year = {2013}
}
@article{Howard2013,
abstract = {We investigate multiple techniques to improve upon the current state of the art deep convolutional neural network based image classification pipeline. The techiques include adding more image transformations to training data, adding more transformations to generate additional predictions at test time and using complementary models applied to higher resolution images. This paper summarizes our entry in the Imagenet Large Scale Visual Recognition Challenge 2013. Our system achieved a top 5 classification error rate of 13.55{\%} using no external data which is over a 20{\%} relative improvement on the previous year's winner.},
archivePrefix = {arXiv},
arxivId = {1312.5402},
author = {Howard, Andrew G.},
eprint = {1312.5402},
file = {:Users/nijram13/Documents/Mendeley Desktop/Howard/2013/Some Improvements on Deep Convolutional Neural Network Based Image Classification/2013 - Howard - Some Improvements on Deep Convolutional Neural Network Based Image Classification - arXiv preprint arXiv1312.5402.pdf:pdf},
journal = {arXiv preprint arXiv:1312.5402},
keywords = {ConvNets,FormicID CNN},
mendeley-groups = {CNN internship/ConvNets},
mendeley-tags = {ConvNets,FormicID CNN},
month = {dec},
title = {{Some Improvements on Deep Convolutional Neural Network Based Image Classification}},
url = {http://arxiv.org/abs/1312.5402},
year = {2013}
}
@article{Domingos2012,
author = {Domingos, Pedro},
file = {:Users/nijram13/Documents/Mendeley Desktop/Domingos/2012/A few useful things to know about machine learning/2012 - Domingos - A few useful things to know about machine learning - Communications of the ACM.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {FormicID CNN,Python {\&} Machine Learning},
mendeley-groups = {CNN internship/Python {\&} Machine learning},
mendeley-tags = {FormicID CNN,Python {\&} Machine Learning},
month = {oct},
number = {10},
pages = {78},
publisher = {ACM},
title = {{A few useful things to know about machine learning}},
url = {http://dl.acm.org/ft{\_}gateway.cfm?id=2347755{\&}type=html},
volume = {55},
year = {2012}
}
@article{Kang2012,
author = {Kang, Seung-Ho and Song, Su-Hee and Lee, Sang-Hee},
doi = {10.1016/j.aspen.2012.03.006},
file = {:Users/nijram13/Documents/Mendeley Desktop/Kang, Song, Lee/2012/Identification of butterfly species with a single neural network system/2012 - Kang, Song, Lee - Identification of butterfly species with a single neural network system - Journal of Asia-Pacific Entomology.pdf:pdf},
issn = {12268615},
journal = {Journal of Asia-Pacific Entomology},
keywords = {FormicID CNN,Other ID methods,automatic species identification},
mendeley-groups = {CNN internship/Other ID methods},
mendeley-tags = {FormicID CNN,Other ID methods},
month = {sep},
number = {3},
pages = {431--435},
title = {{Identification of butterfly species with a single neural network system}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1226861512000374},
volume = {15},
year = {2012}
}
@article{Weeks1997,
author = {Weeks, Paul J. D. and Gaston, Kevin J.},
doi = {10.1023/a:1018348204573},
file = {:Users/nijram13/Documents/Mendeley Desktop/Weeks, Gaston/1997/Image analysis, neural networks, and the taxonomic impediment to biodiversity studies/1997 - Weeks, Gaston - Image analysis, neural networks, and the taxonomic impediment to biodiversity studies - Biodiversity and Conserva.pdf:pdf},
journal = {Biodiversity and Conservation},
keywords = {Background Info,FormicID CNN,biodiversity,identification,image analysis,neural networks,taxonomy},
mendeley-groups = {CNN internship/Background info},
mendeley-tags = {Background Info,FormicID CNN},
number = {2},
pages = {263--274},
title = {{Image analysis, neural networks, and the taxonomic impediment to biodiversity studies}},
url = {http://dx.doi.org/10.1023/A:1018348204573},
volume = {6},
year = {1997}
}
@incollection{Kumar2012,
abstract = {We describe the first mobile app for identifying plant species using automatic visual recognition. The system – called Leafsnap – identifies tree species from photographs of their leaves. Key to this system are computer vision components for discarding non-leaf images, segmenting the leaf from an untextured background, extracting features representing the curvature of the leaf's contour over multiple scales, and identifying the species from a dataset of the 184 trees in the Northeastern United States. Our system obtains state-of-the-art performance on the real-world images from the new Leafsnap Dataset – the largest of its kind. Throughout the paper, we document many of the practical steps needed to produce a computer vision system such as ours, which currently has nearly a million users.},
author = {Kumar, Neeraj and Belhumeur, Peter N and Biswas, Arijit and Jacobs, David W},
booktitle = {Computer Vision – ECCV 2012},
doi = {10.1007/978-3-642-33709-3_36},
file = {:Users/nijram13/Documents/Mendeley Desktop/Kumar et al/2012/Leafsnap A Computer Vision System for Automatic Plant Species Identification/2012 - Kumar et al. - Leafsnap A Computer Vision System for Automatic Plant Species Identification - Computer Vision – ECCV 2012.pdf:pdf},
isbn = {978-3-642-33709-3},
keywords = {FormicID CNN,Other ID methods},
mendeley-groups = {CNN internship/Other ID methods},
mendeley-tags = {FormicID CNN,Other ID methods},
pages = {502--516},
title = {{Leafsnap: A Computer Vision System for Automatic Plant Species Identification}},
url = {http://dx.doi.org/10.1007/978-3-642-33709-3{\_}36},
year = {2012}
}
@article{Simonyan2014,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556v6},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {1409.1556v6},
file = {:Users/nijram13/Documents/Mendeley Desktop/Simonyan, Zisserman/2014/Very Deep Convolutional Networks for Large-Scale Image Recognition/2014 - Simonyan, Zisserman - Very Deep Convolutional Networks for Large-Scale Image Recognition - arXiv preprint arXiv1409.1556v6.pdf:pdf},
isbn = {0950-5849},
issn = {1095-9203},
journal = {arXiv preprint arXiv:1409.1556v6},
keywords = {ConvNets,FormicID CNN},
mendeley-groups = {CNN internship/ConvNets},
mendeley-tags = {ConvNets,FormicID CNN},
month = {sep},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556v6},
year = {2014}
}
@article{Hinton2012,
abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
archivePrefix = {arXiv},
arxivId = {1207.0580},
author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
eprint = {1207.0580},
file = {:Users/nijram13/Documents/Mendeley Desktop/Hinton et al/2012/Improving neural networks by preventing co-adaptation of feature detectors/2012 - Hinton et al. - Improving neural networks by preventing co-adaptation of feature detectors - arXiv preprint arXiv1207.0580.pdf:pdf},
isbn = {9781467394673},
issn = {9781467394673},
journal = {arXiv preprint arXiv:1207.0580},
keywords = {FormicID CNN,Optimization},
mendeley-groups = {CNN internship/Optimization},
mendeley-tags = {FormicID CNN,Optimization},
month = {jul},
title = {{Improving neural networks by preventing co-adaptation of feature detectors}},
url = {http://arxiv.org/abs/1207.0580},
year = {2012}
}
@article{Ba2016,
abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
archivePrefix = {arXiv},
arxivId = {1607.06450},
author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
eprint = {1607.06450},
file = {:Users/nijram13/Documents/Mendeley Desktop/Ba, Kiros, Hinton/2016/Layer Normalization/2016 - Ba, Kiros, Hinton - Layer Normalization - arXiv preprint arXiv1607.06450.pdf:pdf},
isbn = {978-3-642-04273-7},
issn = {1607.06450},
journal = {arXiv preprint arXiv:1607.06450},
keywords = {FormicID CNN,Optimization},
mendeley-groups = {CNN internship/Optimization},
mendeley-tags = {FormicID CNN,Optimization},
month = {jul},
title = {{Layer Normalization}},
url = {http://arxiv.org/abs/1607.06450},
year = {2016}
}
@article{Han2015,
abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce "deep compression", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.},
archivePrefix = {arXiv},
arxivId = {1510.00149},
author = {Han, Song and Mao, Huizi and Dally, William J.},
doi = {abs/1510.00149/1510.00149},
eprint = {1510.00149},
file = {:Users/nijram13/Documents/Mendeley Desktop/Han, Mao, Dally/2015/Deep Compression Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding/2015 - Han, Mao, Dally - Deep Compression Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding - arXiv.pdf:pdf},
journal = {arXiv preprint arXiv:1510.00149},
keywords = {FormicID CNN,Optimization},
mendeley-groups = {CNN internship/Optimization},
mendeley-tags = {FormicID CNN,Optimization},
month = {oct},
title = {{Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding}},
url = {http://arxiv.org/abs/1510.00149},
year = {2015}
}
@article{Hinton2006a,
abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such "autoencoder" networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
author = {Hinton, G. E.},
doi = {10.1126/science.1127647},
file = {:Users/nijram13/Documents/Mendeley Desktop/Hinton/2006/Reducing the Dimensionality of Data with Neural Networks/2006 - Hinton - Reducing the Dimensionality of Data with Neural Networks - Science.pdf:pdf},
issn = {0036-8075},
journal = {Science},
keywords = {FormicID CNN,Optimization},
mendeley-groups = {CNN internship/Optimization},
mendeley-tags = {FormicID CNN,Optimization},
month = {jul},
number = {5786},
pages = {504--507},
pmid = {16873662},
title = {{Reducing the Dimensionality of Data with Neural Networks}},
url = {http://www.sciencemag.org/cgi/doi/10.1126/science.1127647 http://www.ncbi.nlm.nih.gov/pubmed/16873662},
volume = {313},
year = {2006}
}
@article{LeCun2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
archivePrefix = {arXiv},
arxivId = {1603.05691},
author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
eprint = {1603.05691},
file = {:Users/nijram13/Documents/Mendeley Desktop/LeCun, Bengio, Hinton/2015/Deep learning/2015 - LeCun, Bengio, Hinton - Deep learning - Nature.pdf:pdf},
isbn = {3135786504},
issn = {0028-0836},
journal = {Nature},
keywords = {Background Info,FormicID CNN},
mendeley-groups = {CNN internship/Background info},
mendeley-tags = {Background Info,FormicID CNN},
month = {may},
number = {7553},
pages = {436--444},
pmid = {26017442},
title = {{Deep learning}},
url = {http://arxiv.org/abs/1603.05691 http://www.nature.com/doifinder/10.1038/nature14539 http://www.ncbi.nlm.nih.gov/pubmed/26017442},
volume = {521},
year = {2015}
}
@article{He2015a,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
eprint = {1512.03385},
file = {:Users/nijram13/Documents/Mendeley Desktop/He et al/2015/Deep Residual Learning for Image Recognition/2015 - He et al. - Deep Residual Learning for Image Recognition - arXiv preprint arXiv1512.03385.pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {15737721},
journal = {arXiv preprint arXiv:1512.03385},
keywords = {ConvNets,Convolutional neural networks,FormicID CNN,Image steganalysis,Residual learning},
mendeley-groups = {CNN internship/ConvNets},
mendeley-tags = {ConvNets,FormicID CNN},
month = {dec},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://arxiv.org/abs/1512.03385},
year = {2015}
}
@article{Chen2016,
abstract = {We learn recurrent neural network optimizers trained on simple synthetic functions by gradient descent. We show that these learned optimizers exhibit a remarkable degree of transfer in that they can be used to efficiently optimize a broad range of derivative-free black-box functions, including Gaussian process bandits, simple control objectives, global optimization benchmarks and hyper-parameter tuning tasks. Up to the training horizon, the learned optimizers learn to trade-off exploration and exploitation, and compare favourably with heavily engineered Bayesian optimization packages for hyper-parameter tuning.},
archivePrefix = {arXiv},
arxivId = {1611.03824},
author = {Chen, Yutian and Hoffman, Matthew W. and Colmenarejo, Sergio Gomez and Denil, Misha and Lillicrap, Timothy P. and Botvinick, Matt and de Freitas, Nando},
eprint = {1611.03824},
file = {:Users/nijram13/Documents/Mendeley Desktop/Chen et al/2016/Learning to Learn without Gradient Descent by Gradient Descent/2016 - Chen et al. - Learning to Learn without Gradient Descent by Gradient Descent - arXiv preprint arXiv1611.03824.pdf:pdf},
isbn = {1011500801515},
issn = {0219-1377},
journal = {arXiv preprint arXiv:1611.03824},
keywords = {FormicID CNN,Optimization},
mendeley-groups = {CNN internship/Optimization},
mendeley-tags = {FormicID CNN,Optimization},
title = {{Learning to Learn without Gradient Descent by Gradient Descent}},
url = {http://arxiv.org/abs/1611.03824},
year = {2016}
}
@article{Szegedy2014,
abstract = {The octavolateralis systems of fishes include the vestibular, auditory, lateral line and electrosensory systems. They are united by common developmental and neuro-computational features, including hair cell sensors and computations based on cross-neuron analyses of differential hair cell stimulation patterns. These systems also all use both spectral and temporal filters to separate signals from each other and from noise, and the distributed senses (lateral line and electroreception) add spatial filters as well. Like all sensory systems, these sensors must provide the animal with guidance for adaptive behavior within a sensory scene composed of multiple stimuli and varying levels of ambient noise, including that created by human activities. In the extreme, anthropogenic activities impact the octavolateralis systems by destroying or degrading the habitats that provide ecological resources and sensory inputs. At slightly lesser levels of effect, anthropogenic pollutants can be damaging to fish tissues, with sensory organs often the most vulnerable. The exposed sensory cells of the lateral line and electrosensory systems are especially sensitive to aquatic pollution. At still lesser levels of impact, anthropogenic activities can act as both acute and chronic stressors, activating hormonal changes that may affect behavioral and sensory function. Finally, human activities are now a nearly ubiquitous presence in aquatic habitats, often with no obvious effects on the animals exposed to them. Ship noise, indigenous and industrial fishing techniques, and all the ancillary noises of human civilization form a major part of the soundscape of fishes. How fish use these new sources of information about their habitat is a new and burgeoning field of study.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
doi = {10.1111/1749-4877.12092},
eprint = {1409.4842},
file = {:Users/nijram13/Documents/Mendeley Desktop/Szegedy et al/2014/Going Deeper with Convolutions/2014 - Szegedy et al. - Going Deeper with Convolutions - arXiv preprint arXiv1409.4842.pdf:pdf},
isbn = {9781467369640},
issn = {1749-4877},
journal = {arXiv preprint arXiv:1409.4842},
keywords = {ConvNets,FormicID CNN,audition,electrosense,lateral line,soundscape,underwater noise},
mendeley-groups = {CNN internship/ConvNets},
mendeley-tags = {ConvNets,FormicID CNN},
month = {sep},
title = {{Going Deeper with Convolutions}},
url = {http://arxiv.org/abs/1409.4842},
year = {2014}
}
@article{JamesBergstra2012,
abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimiza-tion. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a compar-ison with a large previous study that used grid search and manual search to configure neural net-works and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising con-figuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent " High Throughput " methods achieve surprising success—they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural base-line against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
author = {{James Bergstra} and Bengio, Yoshua},
file = {:Users/nijram13/Documents/Mendeley Desktop/James Bergstra, Bengio/2012/Random Search for Hyper-Parameter Optimization/2012 - James Bergstra, Bengio - Random Search for Hyper-Parameter Optimization - Journal of Machine Learning Research.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {FormicID CNN,Optimization,deep learning,global optimization,model selection,neural networks,response surface modeling},
mendeley-groups = {CNN internship/Optimization},
mendeley-tags = {FormicID CNN,Optimization},
number = {1},
pages = {281--305},
title = {{Random Search for Hyper-Parameter Optimization}},
volume = {13},
year = {2012}
}
@article{Jaderberg2016,
abstract = {Training directed neural networks typically requires forward-propagating data through a computation graph, followed by backpropagating error signal, to produce weight updates. All layers, or more generally, modules, of the network are therefore locked, in the sense that they must wait for the remainder of the network to execute forwards and propagate error backwards before they can be updated. In this work we break this constraint by decoupling modules by introducing a model of the future computation of the network graph. These models predict what the result of the modelled subgraph will produce using only local information. In particular we focus on modelling error gradients: by using the modelled synthetic gradient in place of true backpropagated error gradients we decouple subgraphs, and can update them independently and asynchronously i.e. we realise decoupled neural interfaces. We show results for feed-forward models, where every layer is trained asynchronously, recurrent neural networks (RNNs) where predicting one's future gradient extends the time over which the RNN can effectively model, and also a hierarchical RNN system with ticking at different timescales. Finally, we demonstrate that in addition to predicting gradients, the same framework can be used to predict inputs, resulting in models which are decoupled in both the forward and backwards pass -- amounting to independent networks which co-learn such that they can be composed into a single functioning corporation.},
archivePrefix = {arXiv},
arxivId = {1608.05343},
author = {Jaderberg, Max and Czarnecki, Wojciech Marian and Osindero, Simon and Vinyals, Oriol and Graves, Alex and Silver, David and Kavukcuoglu, Koray},
eprint = {1608.05343},
file = {:Users/nijram13/Documents/Mendeley Desktop/Jaderberg et al/2016/Decoupled Neural Interfaces using Synthetic Gradients/2016 - Jaderberg et al. - Decoupled Neural Interfaces using Synthetic Gradients - arxiv preprint arXiv1608.05343.pdf:pdf},
issn = {1938-7228},
journal = {arxiv preprint arXiv:1608.05343},
keywords = {FormicID CNN,Optimization},
mendeley-groups = {CNN internship/Optimization},
mendeley-tags = {FormicID CNN,Optimization},
month = {aug},
title = {{Decoupled Neural Interfaces using Synthetic Gradients}},
url = {http://arxiv.org/abs/1608.05343},
year = {2016}
}
@article{Kingma2014,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy},
eprint = {1412.6980},
file = {:Users/nijram13/Documents/Mendeley Desktop/Kingma, Ba/2014/Adam A Method for Stochastic Optimization/2014 - Kingma, Ba - Adam A Method for Stochastic Optimization - arXiv preprint arXiv1412.6980.pdf:pdf},
isbn = {9781450300728},
issn = {09252312},
journal = {arXiv preprint arXiv:1412.6980},
keywords = {FormicID CNN,Optimization},
mendeley-groups = {CNN internship/Optimization},
mendeley-tags = {FormicID CNN,Optimization},
month = {dec},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2014}
}
@article{Springenberg2014,
abstract = {Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the "deconvolution approach" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.},
archivePrefix = {arXiv},
arxivId = {1412.6806},
author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
eprint = {1412.6806},
file = {:Users/nijram13/Documents/Mendeley Desktop/Springenberg et al/2014/Striving for Simplicity The All Convolutional Net/2014 - Springenberg et al. - Striving for Simplicity The All Convolutional Net - arXiv preprint arXiv1412.6806.pdf:pdf},
isbn = {9781600066634},
issn = {02548704},
journal = {arXiv preprint arXiv:1412.6806},
keywords = {Computer Science - Computer Vision and Pattern Rec,Computer Science - Learning,Computer Science - Neural and Evolutionary Computi,ConvNets,FormicID CNN},
mendeley-groups = {CNN internship/ConvNets},
mendeley-tags = {ConvNets,FormicID CNN},
month = {dec},
title = {{Striving for Simplicity: The All Convolutional Net}},
url = {http://arxiv.org/abs/1412.6806},
year = {2014}
}
@article{Ioffe2015,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {1502.03167v3},
author = {Ioffe, Sergey and Szegedy, Christian},
eprint = {1502.03167v3},
file = {:Users/nijram13/Documents/Mendeley Desktop/Ioffe, Szegedy/2015/Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift/2015 - Ioffe, Szegedy - Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift - arXiv preprint arX.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
journal = {arXiv preprint arXiv:1502.03167v3},
keywords = {FormicID CNN,Optimization},
mendeley-groups = {CNN internship/Optimization},
mendeley-tags = {FormicID CNN,Optimization},
month = {feb},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://arxiv.org/abs/1502.03167v3},
year = {2015}
}
@incollection{LeCun2012,
abstract = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observedb y practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposedin serious technical publications. This paper gives some of those tricks, ando.ers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most “classical” second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.},
author = {LeCun, Yann A. and Bottou, L{\'{e}}on and Orr, Genevieve B. and M{\"{u}}ller, Klaus-Robert},
booktitle = {Neural networks: Tricks of the trade},
doi = {10.1007/978-3-642-35289-8_3},
file = {:Users/nijram13/Documents/Mendeley Desktop/LeCun et al/2012/Efficient BackProp/2012 - LeCun et al. - Efficient BackProp - Neural networks Tricks of the trade.pdf:pdf},
isbn = {9783642352881},
issn = {03029743},
keywords = {FormicID CNN,Optimization},
mendeley-groups = {CNN internship/Optimization},
mendeley-tags = {FormicID CNN,Optimization},
pages = {9--48},
publisher = {Springer},
title = {{Efficient BackProp}},
url = {http://link.springer.com/10.1007/978-3-642-35289-8{\_}3},
volume = {7700},
year = {2012}
}
@article{Karpathy2015,
abstract = {Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with finite horizon n-gram models traces the source of the LSTM improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study.},
archivePrefix = {arXiv},
arxivId = {1506.02078v2},
author = {Karpathy, Andrej and Johnson, Justin and Fei-Fei, Li},
eprint = {1506.02078v2},
file = {:Users/nijram13/Documents/Mendeley Desktop/Karpathy, Johnson, Fei-Fei/2015/Visualizing and Understanding Recurrent Networks/2015 - Karpathy, Johnson, Fei-Fei - Visualizing and Understanding Recurrent Networks - arXiv preprint arXiv1506.02078v2.pdf:pdf},
isbn = {978-3-319-10589-5},
issn = {978-3-319-10589-5},
journal = {arXiv preprint arXiv:1506.02078v2},
keywords = {FormicID CNN,Visualizing},
mendeley-groups = {CNN internship/Visualizing},
mendeley-tags = {FormicID CNN,Visualizing},
month = {jun},
title = {{Visualizing and Understanding Recurrent Networks}},
url = {http://arxiv.org/abs/1506.02078v2 http://link.springer.com/10.1007/978-3-319-10590-1{\_}53},
year = {2015}
}
@article{He2015,
abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94{\%} top-5 test error on the ImageNet 2012 classification dataset. This is a 26{\%} relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66{\%}). To our knowledge, our result is the first to surpass human-level performance (5.1{\%}, Russakovsky et al.) on this visual recognition challenge.},
archivePrefix = {arXiv},
arxivId = {1502.01852v1},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1109/ICCV.2015.123},
eprint = {1502.01852v1},
file = {:Users/nijram13/Documents/Mendeley Desktop/He et al/2015/Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classification/2015 - He et al. - Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classification - arXiv preprint arxiv1502.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {15505499},
journal = {arXiv preprint arxiv:1502.01852v1},
keywords = {FormicID CNN,Optimization},
mendeley-groups = {CNN internship/Optimization},
mendeley-tags = {FormicID CNN,Optimization},
month = {feb},
publisher = {IEEE},
title = {{Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}},
url = {http://ieeexplore.ieee.org/document/7410480/ http://arxiv.org/abs/1502.01852v1},
year = {2015}
}
@article{Dozat2016,
abstract = {This work aims to improve upon the recently proposed and rapidly popular-ized optimization algorithm Adam (Kingma {\&} Ba, 2014). Adam has two main components—a momentum component and an adaptive learning rate component. However, regular momentum can be shown conceptually and empirically to be in-ferior to a similar algorithm known as Nesterov's accelerated gradient (NAG). We show how to modify Adam's momentum component to take advantage of insights from NAG, and then we present preliminary evidence suggesting that making this substitution improves the speed of convergence and the quality of the learned mod-els.},
author = {Dozat, Timothy},
file = {:Users/nijram13/Documents/Mendeley Desktop/Dozat/2016/Incorporating Nesterov Momentum into Adam/2016 - Dozat - Incorporating Nesterov Momentum into Adam - Unknown.pdf:pdf},
keywords = {FormicID CNN,Optimization},
mendeley-groups = {CNN internship/Optimization},
mendeley-tags = {FormicID CNN,Optimization},
title = {{Incorporating Nesterov Momentum into Adam}},
year = {2016}
}
@article{Sutskever2013,
abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful mod- els that were considered to be almost impos- sible to train using stochastic gradient de- scent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum pa- rameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimiza- tion. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks per- form markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and re- current neural networks from random initial- izations have likely failed due to poor ini- tialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recur- rent network training objectives without the need for sophisticated second-order methods.},
author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
file = {:Users/nijram13/Documents/Mendeley Desktop/Sutskever et al/2013/On the importance of initizalization and momentum in deep learning/2013 - Sutskever et al. - On the importance of initizalization and momentum in deep learning - International conference on machine learn.pdf:pdf},
journal = {International conference on machine learning},
keywords = {FormicID CNN,Optimization},
mendeley-groups = {CNN internship/Optimization},
mendeley-tags = {FormicID CNN,Optimization},
pages = {1139--1147},
title = {{On the importance of initizalization and momentum in deep learning}},
year = {2013}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
file = {:Users/nijram13/Documents/Mendeley Desktop/Srivastava et al/2014/Dropout A Simple Way to Prevent Neural Networks from Overfitting/2014 - Srivastava et al. - Dropout A Simple Way to Prevent Neural Networks from Overfitting - Journal of Machine Learning Research.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {FormicID CNN,Optimization,deep learning,model combination,neural networks,regularization},
mendeley-groups = {CNN internship/Optimization},
mendeley-tags = {FormicID CNN,Optimization},
pages = {1929--1958},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@article{Bottou2012,
abstract = {The twenty last years have been marked by an increase in available$\backslash$ndata and computing power. In parallel to this trend, the focus of$\backslash$nneural network research and the practice of training neural networks$\backslash$nhas undergone a number of important changes, for example, use of$\backslash$ndeep learning machines.$\backslash$n$\backslash$nThe second edition of the book augments the first edition with more$\backslash$ntricks, which have resulted from 14 years of theory and experimentation$\backslash$nby some of the world's most prominent neural network researchers.$\backslash$nThese tricks can make a substantial difference (in terms of speed,$\backslash$nease of implementation, and accuracy) when it comes to putting algorithms$\backslash$nto work on real problems.},
archivePrefix = {arXiv},
arxivId = {arXiv:1206.5533v2},
author = {Bottou, L{\'{e}}on},
doi = {10.1007/978-3-642-35289-8},
eprint = {arXiv:1206.5533v2},
file = {:Users/nijram13/Documents/Mendeley Desktop/Bottou/2012/Stochastic Gradient Descent Tricks/2012 - Bottou - Stochastic Gradient Descent Tricks - Neural networks Tricks of the trade.pdf:pdf},
isbn = {978-3-642-35288-1},
issn = {03029743},
journal = {Neural networks: Tricks of the trade},
keywords = {FormicID CNN,Optimization},
mendeley-groups = {CNN internship/Optimization},
mendeley-tags = {FormicID CNN,Optimization},
pages = {421--436},
pmid = {17348934},
title = {{Stochastic Gradient Descent Tricks}},
url = {http://link.springer.com/10.1007/978-3-642-35289-8},
year = {2012}
}
@article{Bengio2012,
abstract = {Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.},
address = {Berlin, Heidelberg},
archivePrefix = {arXiv},
arxivId = {1206.5533v2},
author = {Bengio, Yoshua},
doi = {10.1007/978-3-642-35289-8_26},
edition = {2},
editor = {Montavon, Gr{\'{e}}goire and Orr, Genevi{\`{e}}ve B. and M{\"{u}}ller, Klaus-Robert},
eprint = {1206.5533v2},
file = {:Users/nijram13/Documents/Mendeley Desktop/Bengio/2012/Practical Recommendations for Gradient-Based Training of Deep Architectures/2012 - Bengio - Practical Recommendations for Gradient-Based Training of Deep Architectures - arxiv preprint arXiv1206.5533v2.pdf:pdf},
isbn = {978-3-642-35288-1},
issn = {03029743},
journal = {arxiv preprint arXiv:1206.5533v2},
keywords = {FormicID CNN,Optimization},
mendeley-groups = {CNN internship/Optimization},
mendeley-tags = {FormicID CNN,Optimization},
month = {jun},
publisher = {Springer Berlin Heidelberg},
title = {{Practical Recommendations for Gradient-Based Training of Deep Architectures}},
url = {http://arxiv.org/abs/1206.5533v2 http://link.springer.com/10.1007/978-3-642-35289-8{\_}26 https://doi.org/10.1007/978-3-642-35289-8{\_}26},
year = {2012}
}
@article{Hinton2006,
abstract = {We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
doi = {10.1162/neco.2006.18.7.1527},
eprint = {1111.6189v1},
file = {:Users/nijram13/Documents/Mendeley Desktop/Hinton, Osindero, Teh/2006/A fast learning algorithm for deep belief nets/2006 - Hinton, Osindero, Teh - A fast learning algorithm for deep belief nets. - Neural computation.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Algorithms,Animals,FormicID CNN,Humans,Learning,Learning: physiology,Neural Networks (Computer),Neurons,Neurons: physiology,Optimization},
mendeley-groups = {CNN internship/Optimization},
mendeley-tags = {FormicID CNN,Optimization},
month = {jul},
number = {7},
pages = {1527--54},
pmid = {16764513},
title = {{A fast learning algorithm for deep belief nets.}},
url = {http://www.mitpressjournals.org/doi/10.1162/neco.2006.18.7.1527 http://www.ncbi.nlm.nih.gov/pubmed/16764513},
volume = {18},
year = {2006}
}
@phdthesis{Pillay2014,
abstract = {Structural analysis is an important step in many document based recognition problem. Structural analysis is performed to associate elements in a document and assign meaning to their association. Handwritten mathematical expression recognition is one such problem which has been studied and researched for long. Many techniques have been researched to build a system that produce high performance mathematical expression recognition. We have presented a novel method to combine multiple structural recognition algorithms in which the combined result shows better performance than each individual recognition algo- rithms. In our experiment we have applied our method to combine multiple mathematical expression recognition parsers called DRACULAE. We have used Graph Transformation Network (GTN) which is a network of function based systems in which each system takes graphs as input, apply function and produces a graph as output. GTN is used to combine multiple DRACULAE parsers and its parameter are tuned using gradient based learning. It has been shown that such a combination method can be used to accentuate the strength of individual algorithms in combination to produce better combination result which higher recognition performance. In our experiment we were able to obtain a highest recogni- tion rate of 74{\%} as compared to best recognition result of 70{\%} from individual DRACULAE parsers. Our experiment also resulted into a maximum of 20{\%} reduction of parent recogni- tion errors and maximum 37{\%} reduction in relation recognition errors between symbols in expressions.},
author = {Pillay, Amit},
file = {:Users/nijram13/Documents/Mendeley Desktop/Pillay/2014/Intelligent Combination of Structural Analysis Algorithms Application to Mathematical Expression Recognition/2014 - Pillay - Intelligent Combination of Structural Analysis Algorithms Application to Mathematical Expression Recognition - Unknown.pdf:pdf},
keywords = {FormicID CNN,Others},
mendeley-groups = {CNN internship/Others},
mendeley-tags = {FormicID CNN,Others},
pages = {76},
school = {Rochester Institute of technology},
title = {{Intelligent Combination of Structural Analysis Algorithms: Application to Mathematical Expression Recognition}},
year = {2014}
}
@article{Qin2016,
abstract = {Underwater object recognition is in great demand, while the research is far from enough. The unrestricted natural environment makes it a challenging task. We propose a framework to recognize fish from videos captured by underwater cameras deployed in the ocean observation network. First, we extract the foreground via sparse and low-rank matrix decomposition. Then, a deep architecture is used to extract features of the foreground fish images. In this architecture, principal component analysis (PCA) is used in two convolutional layers, followed by binary hashing in the non-linear layer and block-wise histograms in the feature pooling layer. Then spatial pyramid pooling (SPP) is used to extract information invariant to large poses. Finally, a linear SVM classifier is used for the classification. This deep network model can be trained efficiently. On a real-world fish recognition dataset, we achieve the state-of-the-art accuracy of 98.64{\%}.},
author = {Qin, Hongwei and Li, Xiu and Liang, Jian and Peng, Yigang and Zhang, Changshui},
doi = {10.1016/j.neucom.2015.10.122},
file = {:Users/nijram13/Documents/Mendeley Desktop/Qin et al/2016/DeepFish Accurate underwater live fish recognition with a deep architecture/2016 - Qin et al. - DeepFish Accurate underwater live fish recognition with a deep architecture - Neurocomputing.pdf:pdf},
isbn = {0925-2312},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Cascaded network,ConvNets,Deep learning,FormicID CNN,Object recognition,Underwater},
mendeley-groups = {CNN internship/ConvNets},
mendeley-tags = {ConvNets,FormicID CNN},
month = {apr},
pages = {49--58},
publisher = {Elsevier},
title = {{DeepFish: Accurate underwater live fish recognition with a deep architecture}},
url = {http://dx.doi.org/10.1016/j.neucom.2015.10.122 http://linkinghub.elsevier.com/retrieve/pii/S0925231215017312},
volume = {187},
year = {2016}
}
@book{Swamynathan2017,
address = {Berkeley, CA},
author = {Swamynathan, Manohar},
doi = {10.1007/978-1-4842-2866-1},
file = {:Users/nijram13/Documents/Mendeley Desktop/Swamynathan/2017/Mastering Machine Learning with Python in Six Steps A Practical Implementation Guide to Predictive Data Analytics Using Python/2017 - Swamynathan - Mastering Machine Learning with Python in Six Steps A Practical Implementation Guide to Predictive Data Analytics U.pdf:pdf},
isbn = {978-1-4842-2865-4},
keywords = {FormicID CNN,Python {\&} Machine Learning},
mendeley-groups = {CNN internship/Python {\&} Machine learning},
mendeley-tags = {FormicID CNN,Python {\&} Machine Learning},
publisher = {Apress},
title = {{Mastering Machine Learning with Python in Six Steps: A Practical Implementation Guide to Predictive Data Analytics Using Python}},
url = {http://link.springer.com/10.1007/978-1-4842-2866-1},
year = {2017}
}
@book{Lutz2013,
author = {Lutz, Mark},
edition = {Fifth},
editor = {Roumeliotis, Rachel},
file = {:Users/nijram13/Documents/Mendeley Desktop/Lutz/2013/Learning Python/2013 - Lutz - Learning Python - Unknown.pdf:pdf},
isbn = {978-1-449-35573-9},
keywords = {FormicID CNN,Python {\&} Machine Learning},
mendeley-groups = {CNN internship/Python {\&} Machine learning},
mendeley-tags = {FormicID CNN,Python {\&} Machine Learning},
pages = {1648},
publisher = {O'Reilly Media, Inc.},
title = {{Learning Python}},
year = {2013}
}
@book{Coelho2013,
abstract = {This is a tutorial-driven and practical, but well-grounded book showcasing good Machine Learning practices. There will be an emphasis on using existing technologies instead of showing how to write your own implementations of algorithms. This book is a scenario-based, example-driven tutorial. By the end of the book you will have learnt critical aspects of Machine Learning Python projects and experienced the power of ML-based systems by actually working on them.This book primarily targets Python developers who want to learn about and build Machine Learning into their projects, or who want to provide Machine Learning support to their existing projects, and see them get implemented effectively .Computer science researchers, data scientists, Artificial Intelligence programmers, and statistical programmers would equally gain from this book and would learn about effective implementation through lots of the practical examples discussed.Readers need no prior experience with Machine Learning or statistical processing. Python development experience is assumed.},
author = {Coelho, Luis Pedro and Richert, Willi},
edition = {second},
file = {:Users/nijram13/Documents/Mendeley Desktop/Coelho, Richert/2013/Building Machine Learning Systems with Python/2013 - Coelho, Richert - Building Machine Learning Systems with Python - Unknown.pdf:pdf},
isbn = {978-1-78439-277-2},
keywords = {FormicID CNN,Python,Python {\&} Machine Learning},
mendeley-groups = {CNN internship,CNN internship/Python {\&} Machine learning},
mendeley-tags = {FormicID CNN,Python {\&} Machine Learning},
pages = {305},
publisher = {Packt Publishing Ltd.},
title = {{Building Machine Learning Systems with Python}},
year = {2013}
}
@article{Su2015,
abstract = {A longstanding question in computer vision concerns the representation of 3D shapes for recognition: should 3D shapes be represented with descriptors operating on their native 3D formats, such as voxel grid or polygon mesh, or can they be effectively represented with view-based descriptors? We address this question in the context of learning to recognize 3D shapes from a collection of their rendered views on 2D images. We first present a standard CNN architecture trained to recognize the shapes' rendered views independently of each other, and show that a 3D shape can be recognized even from a single view at an accuracy far higher than using state-of-the-art 3D shape descriptors. Recognition rates further increase when multiple views of the shapes are provided. In addition, we present a novel CNN architecture that combines information from multiple views of a 3D shape into a single and compact shape descriptor offering even better recognition performance. The same architecture can be applied to accurately recognize human hand-drawn sketches of shapes. We conclude that a collection of 2D views can be highly informative for 3D shape recognition and is amenable to emerging CNN architectures and their derivatives.},
archivePrefix = {arXiv},
arxivId = {1505.00880},
author = {Su, Hang and Maji, Subhransu and Kalogerakis, Evangelos and Learned-Miller, Erik},
doi = {10.1109/ICCV.2015.114},
eprint = {1505.00880},
file = {:Users/nijram13/Documents/Mendeley Desktop/Su et al/2015/Multi-view Convolutional Neural Networks for 3D Shape Recognition/2015 - Su et al. - Multi-view Convolutional Neural Networks for 3D Shape Recognition - arXiv preprint arXiv1505.00880.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {15505499},
journal = {arXiv preprint arXiv:1505.00880},
keywords = {FormicID CNN,Others},
mendeley-groups = {CNN internship/Others},
mendeley-tags = {FormicID CNN,Others},
month = {may},
publisher = {IEEE},
title = {{Multi-view Convolutional Neural Networks for 3D Shape Recognition}},
url = {http://vis-www.cs.umass.edu/mvcnn/docs/su15mvcnn.pdf http://arxiv.org/abs/1505.00880 http://ieeexplore.ieee.org/document/7410471/},
year = {2015}
}
@article{Qi2016,
abstract = {3D shape models are becoming widely available and easier to capture, making available 3D information crucial for progress in object classification. Current state-of-the-art methods rely on CNNs to address this problem. Recently, we witness two types of CNNs being developed: CNNs based upon volumetric representations versus CNNs based upon multi-view representations. Empirical results from these two types of CNNs exhibit a large gap, indicating that existing volumetric CNN architectures and approaches are unable to fully exploit the power of 3D representations. In this paper, we aim to improve both volumetric CNNs and multi-view CNNs according to extensive analysis of existing approaches. To this end, we introduce two distinct network architectures of volumetric CNNs. In addition, we examine multi-view CNNs, where we introduce multi-resolution filtering in 3D. Overall, we are able to outperform current state-of-the-art methods for both volumetric CNNs and multi-view CNNs. We provide extensive experiments designed to evaluate underlying design choices, thus providing a better understanding of the space of methods available for object classification on 3D data.},
archivePrefix = {arXiv},
arxivId = {1604.03265},
author = {Qi, Charles R. and Su, Hao and NieBner, Matthias and Dai, Angela and Yan, Mengyuan and Guibas, Leonidas J.},
doi = {10.1109/CVPR.2016.609},
eprint = {1604.03265},
file = {:Users/nijram13/Documents/Mendeley Desktop/Qi et al/2016/Volumetric and Multi-view CNNs for Object Classification on 3D Data/2016 - Qi et al. - Volumetric and Multi-view CNNs for Object Classification on 3D Data - arXiv preprint arXiv1604.03265.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {10636919},
journal = {arXiv preprint arXiv:1604.03265},
keywords = {FormicID CNN,Others},
mendeley-groups = {CNN internship/Others},
mendeley-tags = {FormicID CNN,Others},
month = {jun},
publisher = {IEEE},
title = {{Volumetric and Multi-view CNNs for Object Classification on 3D Data}},
url = {http://arxiv.org/abs/1604.03265 http://ieeexplore.ieee.org/document/7780978/},
year = {2016}
}
@article{Lee2015,
abstract = {This paper studies convolutional neural networks (CNN) to learn unsupervised feature representations for 44 different plant species, collected at the Royal Botanic Gardens, Kew, England. To gain intuition on the chosen features from the CNN model (opposed to a 'black box' solution), a visualisation technique based on the deconvolutional networks (DN) is utilized. It is found that venations of different order have been chosen to uniquely represent each of the plant species. Experimental results using these CNN features with different classifiers show consistency and superiority compared to the state-of-the art solutions which rely on hand-crafted features.},
archivePrefix = {arXiv},
arxivId = {1506.08425v1},
author = {Lee, Sue Han and Chan, Chee Seng and Wilkin, Paul and Remagnino, Paolo},
doi = {10.1109/ICIP.2015.7350839},
eprint = {1506.08425v1},
file = {:Users/nijram13/Documents/Mendeley Desktop/Lee et al/2015/Deep-Plant Plant Identification with convolutional neural networks/2015 - Lee et al. - Deep-Plant Plant Identification with convolutional neural networks - arXiv preprint arXiv1506.08425v1.pdf:pdf},
isbn = {9781479983391},
issn = {15224880},
journal = {arXiv preprint arXiv:1506.08425v1},
keywords = {ConvNets,FormicID CNN,deep learning,feature visualisation,plant classification},
mendeley-groups = {CNN internship/ConvNets},
mendeley-tags = {ConvNets,FormicID CNN},
month = {jun},
title = {{Deep-Plant: Plant Identification with convolutional neural networks}},
url = {http://arxiv.org/abs/1506.08425v1},
year = {2015}
}
@article{Krizhevsky2012,
abstract = {We present a fast, fully parameterizable GPU implementation of Convolutional Neural Network variants. Our feature extractors are neither carefully designed nor pre-wired, but rather learned in a supervised way. Our deep hierarchical architectures achieve the best published results on benchmarks for object classification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with error rates of 2.53{\%}, 19.51{\%}, 0.35{\%}, respectively. Deep nets trained by simple back-propagation perform better than more shallow ones. Learning is surprisingly rapid. NORB is completely trained within five epochs. Test error rates on MNIST drop to 2.42{\%}, 0.97{\%} and 0.48{\%} after 1, 3 and 17 epochs, respectively.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
doi = {10.1145/3065386},
file = {:Users/nijram13/Documents/Mendeley Desktop/Krizhevsky, Sutskever, Hinton/2012/ImageNet classification with deep convolutional neural networks/2012 - Krizhevsky, Sutskever, Hinton - ImageNet classification with deep convolutional neural networks - Advances in neural information.pdf:pdf},
isbn = {9781627480031},
issn = {00010782},
journal = {Advances in neural information processing systems},
keywords = {ConvNets,FormicID CNN},
mendeley-groups = {CNN internship/ConvNets},
mendeley-tags = {ConvNets,FormicID CNN},
month = {may},
pages = {1097--1105},
title = {{ImageNet classification with deep convolutional neural networks}},
url = {http://dl.acm.org/citation.cfm?doid=3098997.3065386},
year = {2012}
}
@article{Liew2016,
abstract = {An approach using a convolutional neural network (CNN) is proposed for real-time gender classification based on facial images. The proposed CNN architecture exhibits a much reduced design complexity when compared with other CNN solutions applied in pattern recognition. The number of processing layers in the CNN is reduced to only four by fusing the convolutional and subsampling layers. Unlike in conventional CNNs, we replace the convolution operation with cross-correlation, hence reducing the computational load. The network is trained using a second-order backpropagation learning algorithm with annealed global learning rates. Performance evaluation of the proposed CNN solution is conducted on two publicly available face databases of SUMS and AT{\&}T. We achieve classification accuracies of 98.75{\%} and 99.38{\%} on the SUMS and AT{\&}T databases, respectively. The neural network is able to process and classify a 32 × 32 pixel face image in less than 0.27 ms, which corresponds to a very high throughput of over 3700 images per second. Training converges within less than 20 epochs. These results correspond to a superior classification performance, verifying that the proposed CNN is an effective real-time solution for gender recognition. 1. Introduction Gender classification was first perceived as an issue in psychophysical studies; it focuses on the efforts of understanding human visual processing and identifying key features used to categorize between male and female individuals [1]. Research has shown that the disparity between facial masculinity and femininity can be utilized to improve performances of face recognition applications in biometrics, human–computer interactions, surveillance, and computer vision. However, in a real-world environment, the challenge is how to deal with the facial image being affected by the variance in factors such as illumination, pose, facial expression, occlusion, background information, and noise. This is then also the challenge in the development of a robust face-based gender classification system that has high classification accuracy and real-time performance. The conventional approach applied in face recognition, including face-based gender recognition, typically involves the stages of image acquisition and processing, dimensionality reduction, feature extraction, and classification, in that order. Prior knowledge of the application domain is required to determine the best feature extractor to design. In addition, the performance of the recognition system is highly dependent on the type of classifier chosen, which is in turn dependent on the feature extraction method applied. It is difficult},
author = {Liew, Shan Sung and Khallil-Hani, Mohamed and {Adhmad Radzi}, Syafeeza and Bakhteri, Rabia},
doi = {10.3906/elk-1311-58},
file = {:Users/nijram13/Documents/Mendeley Desktop/Liew et al/2016/Gender classification a convolutional neural network approach/2016 - Liew et al. - Gender classification a convolutional neural network approach - Turkish Journal of Electrical Engineering {\&} Compute.pdf:pdf},
isbn = {1300-0632},
issn = {13000632},
journal = {Turkish Journal of Electrical Engineering {\&} Computer Sciences},
keywords = {ConvNets,FormicID CNN,Gender classification,backprop-agation,convolutional neural network,fused convolutional and subsampling layers},
mendeley-groups = {CNN internship/ConvNets},
mendeley-tags = {ConvNets,FormicID CNN},
pages = {1248--1264},
title = {{Gender classification: a convolutional neural network approach}},
url = {http://online.journals.tubitak.gov.tr/openDoiPdf.htm?mKodu=elk-1311-58},
volume = {24},
year = {2016}
}
@article{Mohanty2016,
abstract = {Crop diseases are a major threat to food security, but their rapid identification remains difficult in many parts of the world due to the lack of the necessary infrastructure. The combination of increasing global smartphone penetration and recent advances in computer vision made possible by deep learning has paved the way for smartphone-assisted disease diagnosis. Using a public dataset of 54,306 images of diseased and healthy plant leaves collected under controlled conditions, we train a deep convolutional neural network to identify 14 crop species and 26 diseases (or absence thereof). The trained model achieves an accuracy of 99.35{\%} on a held-out test set, demonstrating the feasibility of this approach. Overall, the approach of training deep learning models on increasingly large and publicly available image datasets presents a clear path towards smartphone-assisted crop disease diagnosis on a massive global scale.},
archivePrefix = {arXiv},
arxivId = {1604.03169},
author = {Mohanty, Sharada P. and Hughes, David P. and Salath{\'{e}}, Marcel},
doi = {10.3389/fpls.2016.01419},
eprint = {1604.03169},
file = {:Users/nijram13/Documents/Mendeley Desktop/Mohanty, Hughes, Salath{\'{e}}/2016/Using Deep Learning for Image-Based Plant Disease Detection/2016 - Mohanty, Hughes, Salath{\'{e}} - Using Deep Learning for Image-Based Plant Disease Detection - Frontiers in Plant Science.pdf:pdf},
isbn = {1664-462X},
issn = {1664-462X},
journal = {Frontiers in Plant Science},
keywords = {FormicID CNN,Other ID methods,crop diseases,deep learning,digital epidemiology,machine learning},
mendeley-groups = {CNN internship/Other ID methods},
mendeley-tags = {FormicID CNN,Other ID methods},
month = {sep},
number = {September},
pages = {1419},
pmid = {27713752},
title = {{Using Deep Learning for Image-Based Plant Disease Detection}},
url = {http://journal.frontiersin.org/Article/10.3389/fpls.2016.01419/abstract},
volume = {7},
year = {2016}
}
@article{Weeks1997,
abstract = {In this paper we describe a semi-automated digital image analysis system which is capable of discriminating five closely related species of Ichneumonidae. Specimens were distinguished by differences in their wings. The system functions by (a) extracting the significant variation (principal components) among a training set of images of the same species, (b) using these principal components to efficiently represent the morphology of wings of that species, and (c) exploiting the fact that images of the same species will share characteristic principal components, while images of different species will not. Such an approach allows the construction of modular species classifiers, to which like species correlate strongly, while dissimilar species do not. A recognition accuracy of 94{\%} was achieved when the system was tested on 175 images of wings of the five ichneumonids. The wing images were caricatured to accentuate their venation and pigmentation patterns.},
author = {Weeks, P.J.D. J. D. and Gauld, I.D. Ian David and Gaston, Kevin J. K.J. and O'Neill, Mark A. M.a.},
doi = {10.1017/S000748530002736X},
file = {:Users/nijram13/Documents/Mendeley Desktop/Weeks et al/1997/Automating the identification of insects a new solution to an old problem/1997 - Weeks et al. - Automating the identification of insects a new solution to an old problem - Bulletin of Entomological Research.pdf:pdf},
isbn = {0007-4853},
issn = {0007-4853},
journal = {Bulletin of Entomological Research},
keywords = {Background Info,FormicID CNN},
mendeley-groups = {CNN internship/Background info},
mendeley-tags = {Background Info,FormicID CNN},
month = {apr},
number = {02},
pages = {203--211},
publisher = {Library African Studies Centre},
title = {{Automating the identification of insects: a new solution to an old problem}},
url = {http://www.journals.cambridge.org/abstract{\_}S000748530002736X},
volume = {87},
year = {1997}
}
@article{Affonso2017,
abstract = {A number of industries use human inspection to visually classify the quality of their products and the raw materials used in the production process, this process could be done automatically through digital image processing. The industries are not always interested in the most accurate technique for a given problem, but most appropriate for the expected results, there must be a balance between accuracy and computational cost. This paper investigates the classification of the quality of wood boards based on their images. For such, it compares the use of deep learning, particularly Convolutional Neural Networks, with the combination of texture-based feature extraction techniques and traditional techniques: Decision tree induction algorithms, Neural Networks, Nearest neighbors and Support vector machines. Reported studies show that Deep Learning techniques applied to image processing tasks have achieved predictive performance superior to traditional classification techniques, mainly in high complex scenarios. One of the reasons pointed out is their embedded feature extraction mechanism. Deep Learning techniques directly identify and extract features, considered by them to be relevant, in a given image dataset. However, empirical results for the image data set have shown that the texture descriptor method proposed, regardless of the strategy employed is very competitive when compared with Convolutional Neural Network for all the performed experiments. The best performance of the texture descriptor method could be caused by the nature of the image dataset. Finally are pointed out some perspectives of futures developments with the application of Active learning and Semi supervised methods.},
author = {Affonso, Carlos and Rossi, Andr{\'{e}} Luis Debiaso and Vieira, F{\'{a}}bio Henrique Antunes and de Carvalho, Andr{\'{e}} Carlos Ponce de Leon Ferreira},
doi = {10.1016/j.eswa.2017.05.039},
file = {:Users/nijram13/Documents/Mendeley Desktop/Affonso et al/2017/Deep learning for biological image classification/2017 - Affonso et al. - Deep learning for biological image classification - Expert Systems with Applications.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Background Info,Deep learning,FormicID CNN,Image classification,Machine learning,Wood classification},
mendeley-groups = {CNN internship/Background info},
mendeley-tags = {Background Info,FormicID CNN},
month = {nov},
pages = {114--122},
publisher = {Elsevier Ltd},
title = {{Deep learning for biological image classification}},
url = {http://dx.doi.org/10.1016/j.eswa.2017.05.039 http://linkinghub.elsevier.com/retrieve/pii/S0957417417303627},
volume = {85},
year = {2017}
}
@inproceedings{Apriyanti2013,
abstract = {In this paper, we developed the system for recognizing the orchid$\backslash$nspecies by using the images of flower. We used MSRM (Maximal Similarity$\backslash$nbased on Region Merging) method for segmenting the flower object$\backslash$nfrom the background and extracting the shape feature such as the$\backslash$ndistance from the edge to the centroid point of the flower, aspect$\backslash$nratio, roundness, moment invariant, fractal dimension and also extract$\backslash$ncolor feature. We used HSV color feature with ignoring the V value.$\backslash$nTo retrieve the image, we used Support Vector Machine (SVM) method.$\backslash$nOrchid is a unique flower. It has a part of flower called lip (labellum)$\backslash$nthat distinguishes it from other flowers even from other types of$\backslash$norchids. Thus, in this paper, we proposed to do feature extraction$\backslash$nnot only on flower region but also on lip (labellum) region. The$\backslash$nresult shows that our proposed method can increase the accuracy value$\backslash$nof content based flower image retrieval for orchid species up to$\backslash$n� 14{\%}. The most dominant feature is Centroid Contour Distance, Moment$\backslash$nInvariant and HSV Color. The system accuracy is 85,33{\%} in validation$\backslash$nphase and 79,33{\%} in testing phase.},
archivePrefix = {arXiv},
arxivId = {1406.2580},
author = {Apriyanti, Diah Harnoni and Arymurthy, Aniati Murni and Handoko, Laksana Tri},
booktitle = {2013 International Conference on Computer, Control, Informatics and Its Applications},
doi = {10.1109/IC3INA.2013.6819148},
eprint = {1406.2580},
file = {:Users/nijram13/Documents/Mendeley Desktop/Apriyanti, Arymurthy, Handoko/2013/Identification of orchid species using content-based flower image retrieval/2013 - Apriyanti, Arymurthy, Handoko - Identification of orchid species using content-based flower image retrieval - 2013 International.pdf:pdf},
isbn = {9781479910786},
keywords = {FormicID CNN,HSV color,Other ID methods,Support Vector Machine,centroid contour distance,flower image retrieval,orchid},
mendeley-groups = {CNN internship/Other ID methods},
mendeley-tags = {FormicID CNN,Other ID methods},
month = {nov},
pages = {53--57},
publisher = {IEEE},
title = {{Identification of orchid species using content-based flower image retrieval}},
url = {http://arxiv.org/abs/1406.2580 http://dx.doi.org/10.1109/IC3INA.2013.6819148 http://ieeexplore.ieee.org/document/6819148/},
year = {2013}
}
@article{Watson2004,
abstract = {Two hundred and thirty-seven species of Macrolepidoptera were light trapped at Treborth Botanical Garden, Gwynedd, UK. Live adults were digitally imaged using a simple, inexpensive method suitable for field use, then released. Inconsistent lighting, variation in resting posture and inclusion of worn individuals produced image sets high in intraspecific variation. Thirty-five common species were selected to provide training images for the Digital Automated Identification SYstem (DAISY). Twenty individuals per species were pre-processed to standardize size and posture and to enhance features. The right forewing of each was highlighted manually and the pattern rendered polar and greyscale for DAISY analysis. Despite poor quality of some images, 83{\%} of unknown species were identified correctly. The best species had 100{\%} correct identification and the worst 35{\%}. The most poorly identified images were those of moths that had lost scales or been unevenly illuminated. The precision with which the forewing was highlighted affected performance. When highlighted carefully, Laothoe populi was identified correctly twice as successfully as when the same image was highlighted poorly. Size of the training set was also important. Sets of 5, 10, 15 and 20 training images, plotted against performance produced a curve of diminishing returns. Colour images and inclusion of size should improve accuracy.},
author = {Watson, Anna T. and O'Neill, Mark A. and Kitching, Ian J.},
doi = {10.1017/S1477200003001208},
file = {:Users/nijram13/Documents/Mendeley Desktop/Watson, O'Neill, Kitching/2004/Automated identification of live moths (Macrolepidoptera) using digital automated identification System (DAISY)/2004 - Watson, O'Neill, Kitching - Automated identification of live moths (Macrolepidoptera) using digital automated identification Syst.pdf:pdf},
isbn = {1478-0933},
issn = {1477-2000},
journal = {Systematics and Biodiversity},
keywords = {FormicID CNN,Other ID methods,automated identification,biodiversity inventorying,computer-aided,image analysis,macrolepidoptera,taxonomy},
mendeley-groups = {CNN internship/Other ID methods},
mendeley-tags = {FormicID CNN,Other ID methods},
month = {feb},
number = {3},
pages = {287--300},
title = {{Automated identification of live moths (Macrolepidoptera) using digital automated identification System (DAISY)}},
url = {http://www.tandfonline.com/doi/abs/10.1017/S1477200003001208},
volume = {1},
year = {2004}
}
@article{Dyrmann2016,
abstract = {Information on which weed species are present within agricultural fields is important for site specific weed management. This paper presents a method that is capable of recognising plant species in colour images by using a convolutional neural network. The network is built from scratch trained and tested on a total of 10,413 images containing 22 weed and crop species at early growth stages. These images originate from six different data sets, which have variations with respect to lighting, resolution, and soil type. This includes images taken under controlled conditions with regard to camera stabilisation and illumination, and images shot with hand-held mobile phones in fields with changing lighting conditions and different soil types. For these 22 species, the network is able to achieve a classification accuracy of 86.2{\%}.},
author = {Dyrmann, Mads and Karstoft, Henrik and Midtiby, Henrik Skov},
doi = {10.1016/j.biosystemseng.2016.08.024},
file = {:Users/nijram13/Documents/Mendeley Desktop/Dyrmann, Karstoft, Midtiby/2016/Plant species classification using deep convolutional neural network/2016 - Dyrmann, Karstoft, Midtiby - Plant species classification using deep convolutional neural network - Biosystems Engineering.pdf:pdf},
issn = {15375110},
journal = {Biosystems Engineering},
keywords = {ConvNets,Convolutional Neural,Deep learning,FormicID CNN,Networks,Plant classification,Weed control},
mendeley-groups = {CNN internship/ConvNets},
mendeley-tags = {ConvNets,FormicID CNN},
month = {nov},
number = {2005},
pages = {72--80},
publisher = {Elsevier Ltd},
title = {{Plant species classification using deep convolutional neural network}},
url = {http://dx.doi.org/10.1016/j.biosystemseng.2016.08.024 http://linkinghub.elsevier.com/retrieve/pii/S1537511016301465},
volume = {151},
year = {2016}
}
@article{Jain2015,
abstract = {The purpose of this Paper is to describe our research on different feature extraction and matching techniques in designing a Content Based Image Retrieval/Processing system. Due to the enormous increase in image database sizes, as well as its vast deployment in various applications, the need for CBIR development arose. This paper outlines a description of primitive feature extraction techniques like: texture, color, and shape. Once these features are extracted and used as the basis for a similarity check between images, the various matching techniques are discussed. This paper proposes novel system architecture for CBIR system which combines techniques including content based image and color analysis, as well as data mining techniques. This is a study to propose segmentation module to build the CBIR system. It also includes concept of neighborhood color analysis module which also recognizes the side of every grids of image. The study also includes the implementation of 4 algorithms namely: -K-MEANS, SIFT, SURF and BRIEF algorithm.},
author = {Jain, Sahil and Pulaparthi, Kiranmai and Fulara, Chetan},
issn = {2309-4893},
journal = {International Journal of Advanced Engineering and Global Technology I},
keywords = {BRIEF,Content based images retrieval,Feature extraction,FormicID CNN,Image retrieval,K-Means Clustering,Others,SIFT,SURF},
mendeley-groups = {CNN internship/Others},
mendeley-tags = {FormicID CNN,Others},
number = {10},
title = {{Content based image retrieval}},
volume = {03},
year = {2015}
}
@inproceedings{Wang2009,
abstract = {Image classification and annotation are important prob- lems in computer vision, but rarely considered together. In- tuitively, annotations provide evidence for the class label, and the class label provides evidence for annotations. For example, an image of class highway is more likely anno- tated with words “road,” “car,” and “traffic” than words “fish,” “boat,” and “scuba.” In this paper, we develop a new probabilistic model for jointly modeling the image, its class label, and its annotations. Our model treats the class label as a global description of the image, and treats an- notation terms as local descriptions of parts of the image. Its underlying probabilistic assumptions naturally integrate these two sources of information. We derive an approximate inference and estimation algorithms based on variational methods, as well as efficient approximations for classifying and annotating new images. We examine the performance of our model on two real-world image data sets, illustrating that a single model provides competitive annotation perfor- mance, and superior classification performance.},
author = {Wang, Chong and Blei, David and Fei-Fei, Li},
booktitle = {2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2009},
doi = {10.1109/CVPRW.2009.5206800},
isbn = {9781424439935},
issn = {1063-6919},
keywords = {FormicID CNN,Others},
mendeley-groups = {CNN internship/Others},
mendeley-tags = {FormicID CNN,Others},
pages = {1903--1910},
pmid = {1000198810},
title = {{Simultaneous image classification and annotation}},
year = {2009}
}
@article{Francoy2008,
abstract = {Currently available morphometric and genetic techniques that can accurately identify African- ized honey bees are both costly and time consuming. We tested two new morphometric techniques (ABIS Automatic Bee Identification System and geometric morphometrics analysis) on samples consisting of digital images of five worker forewings per colony. These were collected from 394 colonies of Africanized bees from all over Brazil and from colonies of African bees, Apis mellifera scutellata (n = 14), and Euro- pean bees, A. m. ligustica (n = 10), A. m. mellifera (n = 15), and A. m. carnica (n=15) from the Ruttner collection in Oberursel, Germany (preserved specimens). Both methods required less than five minutes per sample, giving more than 99{\%} correct identifications. There was just one misidentification (based on ge- ometric morphometrics analysis) of Africanized bees compared with European subspecies, which would be the principal concern in newly-colonized areas, such as the southern USA. These new techniques are inexpensive, fast and precise.},
author = {Francoy, Tiago Mauricio and Wittmann, Dieter and Drauschke, Martin and M{\"{u}}ller, Stefan and Steinhage, Volker and Bezerra-Laure, Marcela A. F. and {De Jong}, David and Gon{\c{c}}alves, Lionel Segui},
doi = {10.1051/apido:2008028},
file = {:Users/nijram13/Documents/Mendeley Desktop/Tiago Mauricio et al/2008/Original article Identification of Africanized honey bees through wing morphometrics two fast and efficient procedures M u/2008 - Tiago Mauricio et al. - Original article Identification of Africanized honey bees through wing morphometrics two fast and effici.pdf:pdf},
isbn = {0044-8435$\backslash$r1297-9678},
issn = {0044-8435},
journal = {Apidologie},
keywords = {FormicID CNN,Other ID methods},
mendeley-groups = {CNN internship/Other ID methods},
mendeley-tags = {FormicID CNN,Other ID methods},
month = {sep},
number = {5},
pages = {488--494},
title = {{Identification of Africanized honey bees through wing morphometrics: two fast and efficient procedures}},
url = {http://www.apidologie.org http://link.springer.com/10.1051/apido:2008028},
volume = {39},
year = {2008}
}
@misc{,
keywords = {FormicID CNN,Other ID methods},
mendeley-groups = {CNN internship/Other ID methods},
mendeley-tags = {FormicID CNN,Other ID methods},
title = {{idBee - Automatic Bee Identification from Wing Venation}},
url = {https://www.engr.wisc.edu/},
urldate = {2017-09-14}
}
@book{MacLeod2007,
abstract = {The automated identification of biological objects or groups has been a dream among taxonomists and systematists for centuries. However, progress in designing and implementing practical systems for fully automated taxon identification has been frustratingly slow. Regardless, the dream has never died. Recent developments in computer architectures and innovations in software design have placed the tools needed to realize this vision in the hands of the systematics community, not several years hence, but now. And not just for DNA barcodes or other molecular data, but for digital images of organisms, digital sounds, digitized chemical data - essentially any type of digital data.Based on evidence accumulated over the last decade and written by applied researchers, Automated Taxon Identification in Systematics explores contemporary applications of quantitative approaches to the problem of taxon recognition. The book begins by reviewing the current state of systematics and placing automated taxon identification in the context of contemporary trends, needs, and opportunities. The chapters present and evaluate different aspects of current automated system designs. They then provide descriptions of case studies in which different theoretical and practical aspects of the overall group-identification problem are identified, analyzed, and discussed.A recurring theme through the chapters is the relationship between taxonomic identification, automated group identification, and morphometrics. This collection provides a bridge between these communities and between them and the wider world of applied taxonomy. The only book-length treatment that explores automated group identification in systematic context, this text also includes introductions to basic aspects of the fields of contemporary artificial intelligence and mathematical group recognition for the entire biological community.},
author = {MacLeod, Norman},
booktitle = {Systematics Association},
file = {:Users/nijram13/Documents/Mendeley Desktop/MacLeod/2007/Automated Taxon Identification in Systematics Theory, Approaches and Applications/2007 - MacLeod - Automated Taxon Identification in Systematics Theory, Approaches and Applications - Systematics Association.pdf:pdf},
isbn = {9780849382055},
issn = {0033-5770},
keywords = {Background Info,FormicID CNN},
mendeley-groups = {CNN internship/Background info},
mendeley-tags = {Background Info,FormicID CNN},
pages = {339},
title = {{Automated Taxon Identification in Systematics: Theory, Approaches and Applications}},
url = {http://books.google.com/books?id=Fz6v8YPyQQgC{\&}pgis=1},
year = {2007}
}
@article{Wang2012a,
abstract = {A new automatic identification system has been designed to identify insect specimen images at the order level. Several relative features were designed according to the methods of digital image progressing, pattern recognition and the theory of taxonomy. Artificial neural networks (ANNs) and a support vector machine (SVM) are used as pattern recognition methods for the identification tests. During tests on nine common orders and sub-orders with an artificial neural network, the system performed with good stability and accuracy reached 93{\%}. Results from tests using the support vector machine further improved accuracy. We also did tests on eight- and nine-orders with different features and based on these results we compare the advantages and disadvantages of our system and provide some advice for future research on insect image recognition. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
author = {Wang, Jiangning and Lin, Congtian and Ji, Liqiang and Liang, Aiping},
doi = {10.1016/j.knosys.2012.03.014},
file = {:Users/nijram13/Documents/Mendeley Desktop/Wang et al/2012/A new automatic identification system of insect images at the order level/2012 - Wang et al. - A new automatic identification system of insect images at the order level - Knowledge-Based Systems.pdf:pdf},
isbn = {09507051},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {ANN,Feature,FormicID CNN,Insect,Order,Other ID methods,SVM},
mendeley-groups = {CNN internship/Other ID methods},
mendeley-tags = {FormicID CNN,Other ID methods},
month = {sep},
pages = {102--110},
publisher = {Elsevier B.V.},
title = {{A new automatic identification system of insect images at the order level}},
url = {http://dx.doi.org/10.1016/j.knosys.2012.03.014 http://linkinghub.elsevier.com/retrieve/pii/S0950705112000822},
volume = {33},
year = {2012}
}
@article{Wang2012,
abstract = {There is increasing interest in the automatic identification of insect species from images. Here content-based image retrieval (CBIR) is applied because of its capacity for mass processing and operability. A series of shape, colour and texture features was developed that draw on CBIR and allow the identification of butterfly images to the taxonomic scale of family. In our test the accuracy of Papilionidae reached 84{\%} indicating that CBIR is suitable for the identification of butterflies at the family level. Furthermore, experiments with different features, feature weights and similarity matching algorithms were compared. Testing revealed that data attributes such as species diversity, image quality and resolution affected system success the most, followed by features and match algorithms; shape features are more important than colour or texture features in the identification of butterfly families. These findings are important to future improvements in this technology and its applicability. {\textcopyright} 2011 IAgrE.},
author = {Wang, Jiangning and Ji, Liqiang and Liang, Aiping and Yuan, Decheng},
doi = {10.1016/j.biosystemseng.2011.10.003},
file = {:Users/nijram13/Documents/Mendeley Desktop/Wang et al/2012/The identification of butterfly families using content-based image retrieval/2012 - Wang et al. - The identification of butterfly families using content-based image retrieval - Biosystems Engineering.pdf:pdf},
issn = {15375110},
journal = {Biosystems Engineering},
keywords = {FormicID CNN},
mendeley-groups = {CNN internship},
mendeley-tags = {FormicID CNN},
month = {jan},
number = {1},
pages = {24--32},
publisher = {IAgrE},
title = {{The identification of butterfly families using content-based image retrieval}},
url = {http://dx.doi.org/10.1016/j.biosystemseng.2011.10.003 http://linkinghub.elsevier.com/retrieve/pii/S1537511011001784},
volume = {111},
year = {2012}
}
@article{Wen2012,
abstract = {Insect identification and classification is time-consuming work requiring expert knowledge for integrated pest management in orchards. An image-based automated insect identification and classification method is described in the paper. The complete method includes three models. An invariant local feature model was built for insect identification and classification using affine invariant local features; a global feature model was built for insect identification and classification using 54 global features; and a hierarchical combination model was proposed based on local feature and global feature models to combine advantages of the two models and increase performance. The three models were applied and tested for insect classification on eight insect species from pest colonies and orchards. The hierarchical combination model yielded better performance over global and local models. Moreover, to study the pose change of insects on traps and the hypothesis that an optimal time to acquire and image after landing exists, advanced analysis on time-dependent pose change of insects on traps is included in this study. The experimental results on field insect image classification with field-based images for training achieved the classification rate of 86.6{\%} when testing with the combination model. This demonstrates the image-based insect identification and classification method could be a potential way for automated insect classification in integrated pest management. ?? 2012 Elsevier B.V.},
author = {Wen, Chenglu and Guyer, Daniel},
doi = {10.1016/j.compag.2012.08.008},
file = {:Users/nijram13/Documents/Mendeley Desktop/Wen, Guyer/2012/Image-based orchard insect automated identification and classification method/2012 - Wen, Guyer - Image-based orchard insect automated identification and classification method - Computers and Electronics in Agricul.pdf:pdf},
issn = {01681699},
journal = {Computers and Electronics in Agriculture},
keywords = {FormicID CNN,Global feature,Image processing,Insect classification,Integrated pest management,Local feature,Other ID methods},
mendeley-groups = {CNN internship/Other ID methods},
mendeley-tags = {FormicID CNN,Other ID methods},
month = {nov},
pages = {110--115},
publisher = {Elsevier B.V.},
title = {{Image-based orchard insect automated identification and classification method}},
url = {http://dx.doi.org/10.1016/j.compag.2012.08.008 http://linkinghub.elsevier.com/retrieve/pii/S0168169912002190},
volume = {89},
year = {2012}
}
@article{Zhou2013,
abstract = {BACKGROUND Pattern recognition algorithms are useful in bioimage informatics applications such as quantifying cellular and subcellular objects, annotating gene expressions, and classifying phenotypes. To provide effective and efficient image classification and annotation for the ever-increasing microscopic images, it is desirable to have tools that can combine and compare various algorithms, and build customizable solution for different biological problems. However, current tools often offer a limited solution in generating user-friendly and extensible tools for annotating higher dimensional images that correspond to multiple complicated categories. RESULTS We develop the BIOimage Classification and Annotation Tool (BIOCAT). It is able to apply pattern recognition algorithms to two- and three-dimensional biological image sets as well as regions of interest (ROIs) in individual images for automatic classification and annotation. We also propose a 3D anisotropic wavelet feature extractor for extracting textural features from 3D images with xy-z resolution disparity. The extractor is one of the about 20 built-in algorithms of feature extractors, selectors and classifiers in BIOCAT. The algorithms are modularized so that they can be "chained" in a customizable way to form adaptive solution for various problems, and the plugin-based extensibility gives the tool an open architecture to incorporate future algorithms. We have applied BIOCAT to classification and annotation of images and ROIs of different properties with applications in cell biology and neuroscience. CONCLUSIONS BIOCAT provides a user-friendly, portable platform for pattern recognition based biological image classification of two- and three- dimensional images and ROIs. We show, via diverse case studies, that different algorithms and their combinations have different suitability for various problems. The customizability of BIOCAT is thus expected to be useful for providing effective and efficient solutions for a variety of biological problems involving image classification and annotation. We also demonstrate the effectiveness of 3D anisotropic wavelet in classifying both 3D image sets and ROIs.},
author = {Zhou, Jie and Lamichhane, Santosh and Sterne, Gabriella and Ye, Bing and Peng, Hanchuan},
doi = {10.1186/1471-2105-14-291},
file = {:Users/nijram13/Documents/Mendeley Desktop/Zhou et al/2013/BIOCAT a pattern recognition platform for customizable biological image classification and annotation/2013 - Zhou et al. - BIOCAT a pattern recognition platform for customizable biological image classification and annotation. - BMC bioinf.pdf:pdf},
isbn = {1471-2105 (Electronic)},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {FormicID CNN,Other ID methods},
mendeley-groups = {CNN internship/Other ID methods},
mendeley-tags = {FormicID CNN,Other ID methods},
month = {oct},
number = {1},
pages = {291},
pmid = {24090164},
title = {{BIOCAT: a pattern recognition platform for customizable biological image classification and annotation.}},
url = {http://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-291 http://www.ncbi.nlm.nih.gov/pubmed/24090164 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3854450},
volume = {14},
year = {2013}
}
@article{Yang2015,
abstract = {For some insect groups, wing outline is an important character for species identification. We have constructed a program as the integral part of an automated system to identify insects based on wing outlines (DAIIS). This program includes two main functions: (1) outline digitization and Elliptic Fourier transformation and (2) classifier model training by pattern recognition of support vector machines and model validation. To demonstrate the utility of this program, a sample of 120 owlflies (Neuroptera: Ascalaphidae) was split into training and validation sets. After training, the sample was sorted into seven species using this tool. In five repeated experiments, the mean accuracy for identification of each species ranged from 90{\%} to 98{\%}. The accuracy increased to 99{\%} when the samples were first divided into two groups based on features of their compound eyes. DAIIS can therefore be a useful tool for developing a system of automated insect identification.},
author = {Yang, He-Ping and Ma, Chun-Sen and Wen, Hui and Zhan, Qing-Bin and Wang, Xin-Li},
doi = {10.1038/srep12786},
file = {:Users/nijram13/Documents/Mendeley Desktop/Yang et al/2015/A tool for developing an automatic insect identification system based on wing outlines/2015 - Yang et al. - A tool for developing an automatic insect identification system based on wing outlines. - Scientific reports.pdf:pdf},
issn = {2045-2322},
journal = {Scientific reports},
keywords = {FormicID CNN,Other ID methods},
mendeley-groups = {CNN internship/Other ID methods},
mendeley-tags = {FormicID CNN,Other ID methods},
month = {aug},
number = {1},
pages = {12786},
pmid = {26251292},
publisher = {Nature Publishing Group},
title = {{A tool for developing an automatic insect identification system based on wing outlines.}},
url = {http://www.nature.com/srep/2015/150807/srep12786/full/srep12786.html http://www.nature.com/articles/srep12786 http://www.ncbi.nlm.nih.gov/pubmed/26251292 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4528224},
volume = {5},
year = {2015}
}
