@article{Reyes2015,
abstract = {This paper describes the participation of the ECOUAN team in the LifeCLEF 2015 challenge. We used a deep learning approach in which the complete system was learned without hand-engineered compo-nents. We pre-trained a convolutional neural network using 1.8 million images and used a fine-tuning strategy to transfer learned recognition capabilities from general domains to the specific challenge of Plant Iden-tification task. The classification accuracy obtained by our method out-performed the best result obtained in 2014. Our group obtained the 4th position among all teams and the 10th position among 18 runs.},
author = {Reyes, Angie K. and Caicedo, Juan C. and Camargo, Jorge E.},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Reyes, Caicedo, Camargo/2015/Reyes, Caicedo, Camargo - 2015 - Fine-tuning deep convolutional networks for plant recognition.pdf:pdf},
issn = {16130073},
journal = {preprint},
keywords = {Convolutional neural networks,Deep learning,Image retrieval,ImageCLEF,Plant recognition},
title = {{Fine-tuning deep convolutional networks for plant recognition}},
volume = {1391},
year = {2015}
}
@article{Lu2015,
abstract = {Transfer learning aims to provide a framework to utilize previously-acquired knowledge to solve new but similar problems much more quickly and effectively. In contrast to classical machine learning methods, transfer learning methods exploit the knowledge accumulated from data in auxiliary domains to facilitate predictive modeling consisting of different data patterns in the current domain. To improve the performance of existing transfer learning methods and handle the knowledge transfer process in real-world systems, computational intelligence has recently been applied in transfer learning. This paper systematically examines computational intelligence-based transfer learning techniques and clusters related technique developments into four main categories: (a) neural network-based transfer learning; (b) Bayes-based transfer learning; (c) fuzzy transfer learning, and (d) applications of computational intelligence-based transfer learning. By providing state-of-the-art knowledge, this survey will directly support researchers and practice-based professionals to understand the developments in computational intelligence-based transfer learning research and applications.},
archivePrefix = {arXiv},
arxivId = {1511.05641v4},
author = {Lu, Jie and Behbood, Vahid and Hao, Peng and Zuo, Hua and Xue, Shan and Zhang, Guangquan},
doi = {10.1016/j.knosys.2015.01.010},
eprint = {1511.05641v4},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Lu et al/2015/Lu et al. - 2015 - Transfer learning using computational intelligence A survey.pdf:pdf},
isbn = {0950-7051},
issn = {09507051},
journal = {arXiv preprint arXiv:1511.05641v4},
keywords = {Bayes,Computational intelligence,Fuzzy sets and systems,Genetic algorithm,Neural network,Transfer learning},
month = {may},
pages = {14--23},
title = {{Transfer learning using computational intelligence: A survey}},
url = {http://arxiv.org/abs/1511.05641 http://linkinghub.elsevier.com/retrieve/pii/S0950705115000179},
volume = {80},
year = {2015}
}
@article{Sun2017,
abstract = {Plant image identification has become an interdisciplinary focus in both botanical taxonomy and computer vision. The first plant image dataset collected by mobile phone in natural scene is presented, which contains 10,000 images of 100 ornamental plant species in Beijing Forestry University campus. A 26-layer deep learning model consisting of 8 residual building blocks is designed for large-scale plant classification in natural environment. The proposed model achieves a recognition rate of 91.78{\%} on the BJFU100 dataset, demonstrating that deep learning is a promising technology for smart forestry.},
author = {Sun, Yu and Liu, Yuan and Wang, Guan and Zhang, Haiyan},
doi = {10.1155/2017/7361042},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Sun et al/2017/Sun et al. - 2017 - Deep Learning for Plant Identification in Natural Environment.pdf:pdf},
issn = {1687-5265},
journal = {Computational Intelligence and Neuroscience},
pages = {1--6},
pmid = {28611840},
title = {{Deep Learning for Plant Identification in Natural Environment}},
url = {https://www.hindawi.com/journals/cin/2017/7361042/ http://www.ncbi.nlm.nih.gov/pubmed/28611840 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC5458433},
volume = {2017},
year = {2017}
}
@article{Yao2007,
abstract = {In this paper we study a family of gradient descent algorithms to approx- imate the regression function from reproducing kernel Hilbert spaces (RKHSs), the family being characterized by a polynomial decreasing rate of step sizes (or learning rate). By solving a bias-variance trade-off we obtain an early stopping rule and some probabilistic upper bounds for the convergence of the algorithms.We also discuss the implication of these results in the context of classification where some fast conver- gence rates can be achieved for plug-in classifiers. Some connections are addressed with Boosting, Landweber iterations, and the online learning algorithms as stochastic approximations of the gradient descent method.},
author = {Yao, Yuan and Rosasco, Lorenzo and Caponnetto, Andrea},
doi = {10.1007/s00365-006-0663-2},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Yao, Rosasco, Caponnetto/2007/Yao, Rosasco, Caponnetto - 2007 - On Early Stopping in Gradient Descent Learning.pdf:pdf},
issn = {0176-4276},
journal = {Constructive Approximation},
keywords = {Boosting,Early stopping,Gradient descent method,Landweber iter- ation,Regularization,Reproducing kernel Hilbert space},
month = {aug},
number = {2},
pages = {289--315},
title = {{On Early Stopping in Gradient Descent Learning}},
url = {http://link.springer.com/10.1007/s00365-006-0663-2},
volume = {26},
year = {2007}
}
@article{Waldchen2017,
abstract = {Species knowledge is essential for protecting biodiversity. The identification of plants by conventional keys is complex, time consuming, and due to the use of specific botanical terms frustrating for non-experts. This creates a hard to overcome hurdle for novices interested in acquiring species knowledge. Today, there is an increasing interest in automating the process of species identification. The availability and ubiquity of relevant technologies, such as, digital cameras and mobile devices, the remote access to databases, new techniques in image processing and pattern recognition let the idea of automated species identification become reality. This paper is the first systematic literature review with the aim of a thorough analysis and comparison of primary studies on computer vision approaches for plant species identification. We identified 120 peer-reviewed studies, selected through a multi-stage process, published in the last 10 years (2005–2015). After a careful analysis of these studies, we describe the applied methods categorized according to the studied plant organ, and the studied features, i.e., shape, texture, color, margin, and vein structure. Furthermore, we compare methods based on classification accuracy achieved on publicly available datasets. Our results are relevant to researches in ecology as well as computer vision for their ongoing research. The systematic and concise overview will also be helpful for beginners in those research fields, as they can use the comparable analyses of applied methods as a guide in this complex activity.},
author = {W{\"{a}}ldchen, Jana and M{\"{a}}der, Patrick},
doi = {10.1007/s11831-016-9206-z},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/W{\"{a}}ldchen, M{\"{a}}der/2017/W{\"{a}}ldchen, M{\"{a}}der - 2017 - Plant Species Identification Using Computer Vision Techniques A Systematic Literature Review.pdf:pdf},
isbn = {0123456789},
issn = {1134-3060},
journal = {Archives of Computational Methods in Engineering},
month = {jan},
number = {0},
pages = {1--37},
publisher = {Springer Netherlands},
title = {{Plant Species Identification Using Computer Vision Techniques: A Systematic Literature Review}},
url = {http://link.springer.com/10.1007/s11831-016-9206-z},
volume = {0},
year = {2017}
}
@article{Peeters1991,
abstract = {The term "ergatogyne" is used in ants to describe permanently-wingless female adults which are "morphologically intermediate" between workers and winged queens. This definition is ambiguous because there are two distinct categories of "ergatogynes": ergatoid queens and intercastes. Both have an external appearance (ocelli and alitrunk structure) which combines traditional queen and worker characters, and thus can be confused if they both function as reproductives - however intercastes in most species cannot reproduce. Ergatoid queens have replaced winged queens in a substantial number of species. They are sometimes externally similar to conspecific workers, especially in various ponerine species which exhibit limited size dimorphism between castes. Ergatoid queens retain the specialized attributes of a reproductive caste, including larger ovaries, and they are always the functional egg-layers in a colony. In contrast, conspecific intercastes represent various graded stages in a series connecting workers and winged queens, and they occur together with the queens. These hybrid phenotypes result from deviations from the normal pattern of caste differentiation during larval development. Intercastes generally lack a spermatheca and have no reproductive function; however they can mate in a few leptothoracine ants, and then reproduce instead of winged queens in a proportion of colonies.},
author = {Peeters, C. P.},
doi = {10.1007/BF01242708},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Peeters/1991/Peeters - 1991 - Ergatoid queens and intercastes in ants Two distinct adult forms which look morphologically intermediate between worker.pdf:pdf},
issn = {0020-1812},
journal = {Insectes Sociaux},
keywords = {Leptothoracini,Ponerinae,Reproduction,caste,morphology},
month = {mar},
number = {1},
pages = {1--15},
title = {{Ergatoid queens and intercastes in ants: Two distinct adult forms which look morphologically intermediate between workers and winged queens}},
url = {http://link.springer.com/10.1007/BF01242708},
volume = {38},
year = {1991}
}
@article{MehdipourGhazi2017,
abstract = {We use deep convolutional neural networks to identify the plant species captured in a photograph and evaluate different factors affecting the performance of these networks. Three powerful and popular deep learning architectures, namely GoogLeNet, AlexNet, and VGGNet, are used for this purpose. Transfer learning is used to fine-tune the pre-trained models, using the plant task datasets of LifeCLEF 2015. To decrease the chance of overfitting, data augmentation techniques are applied based on image transforms such as rotation, translation, reflection, and scaling. Furthermore, the networks' parameters are adjusted and different classifiers are fused to improve overall performance. Our best combined system has achieved an overall accuracy of 80{\%} on the validation set and an overall inverse rank score of 0.752 on the official test set. A comparison of our results against the results of the LifeCLEF 2015 plant identification campaign shows that we have improved the overall validation accuracy of the top system by 15{\%} points and its overall inverse rank score on the test set by 0.1 while outperforming the top three competition participants in all categories. The system recently obtained a very close second place in the PlantCLEF 2016.},
author = {{Mehdipour Ghazi}, Mostafa and Yanikoglu, Berrin and Aptoula, Erchan},
doi = {10.1016/j.neucom.2017.01.018},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Mehdipour Ghazi, Yanikoglu, Aptoula/2017/Mehdipour Ghazi, Yanikoglu, Aptoula - 2017 - Plant identification using deep neural networks via optimization of transfer learning param.pdf:pdf},
isbn = {0925-2312},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Convolutional neural networks,Deep learning,Inverse rank score,Plant identification,Transfer learning},
month = {apr},
number = {August 2016},
pages = {228--235},
publisher = {Elsevier B.V.},
title = {{Plant identification using deep neural networks via optimization of transfer learning parameters}},
url = {http://dx.doi.org/10.1016/j.neucom.2017.01.018 http://linkinghub.elsevier.com/retrieve/pii/S0925231217300498},
volume = {235},
year = {2017}
}
@article{Kearns1997,
abstract = {We give a theoretical and experimental analysis of the generalization error of cross validation using two natural measures of the problem under consideration. The approximation rate measures the accuracy to which the target function can be ideally approximated as a function of the number of parameters, and thus captures the complexity of the target function with respect to the hypothesis model. The estimation rate measures the deviation between the training and generalization errors as a function of the number of parameters, and thus captures the extent to which the hypothesis model suffers from overfitting. Using these two measures, we give a rigorous and general bound on the error of the simplest form of cross validation. The bound clearly shows the dangers of making gamma-the fraction of data saved for testing-too large or too small. By optimizing the bound with respect to gamma, we then argue that the following qualitative properties of cross-validation behavior should be quite robust to significant changes in the underlying model selection problem: When the target function complexity is small compared to the sample size, the performance of cross validation is relatively insensitive to the choice of gamma. The importance of choosing gamma optimally increases, and the optimal value for gamma decreases, as the target function becomes more complex relative to the sample size. There is nevertheless a single fixed value for gamma that works nearly optimally for a wide range of target function complexity.},
author = {Kearns, Michael},
doi = {10.1162/neco.1997.9.5.1143},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Kearns/1997/Kearns - 1997 - A Bound on the Error of Cross Validation Using the Approximation and Estimation Rates, with Consequences for the Trainin.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural Computation},
month = {jul},
number = {5},
pages = {1143--1161},
title = {{A Bound on the Error of Cross Validation Using the Approximation and Estimation Rates, with Consequences for the Training-Test Split}},
url = {http://www.mitpressjournals.org/doi/10.1162/neco.1997.9.5.1143},
volume = {9},
year = {1997}
}
@misc{Guyon1996,
abstract = {We address the problem of determining what fraction of the training set should be reserved as development test set or validation set. We determine that the ratio of the validation set size over the training set size scales like the square root of two complexity parameters: the complexity of the second level of inference (minimizing the validation error) over the complexity of the first level of inference (minimizing the error rate on the training set).},
author = {Guyon, Isabelle},
booktitle = {preprint},
doi = {10.1.1.33.1337},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Guyon/1996/Guyon - 1996 - A scaling law for the validation-set training-set size ratio.pdf:pdf},
keywords = {Cross-validation,Experiment Design,Learning Theory,Machine Learning,Pattern Recognition,Statistics,Test Set,Training Set,Validation Set},
title = {{A scaling law for the validation-set training-set size ratio}},
year = {1996}
}
@article{Schmidhuber2015,
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short programs encoding deep and large networks.},
archivePrefix = {arXiv},
arxivId = {1404.7828},
author = {Schmidhuber, J{\"{u}}rgen},
doi = {10.1016/j.neunet.2014.09.003},
eprint = {1404.7828},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Schmidhuber/2015/Schmidhuber - 2015 - Deep learning in neural networks An overview.pdf:pdf},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Deep learning,Evolutionary computation,Reinforcement learning,Supervised learning,Unsupervised learning},
month = {jan},
pages = {85--117},
pmid = {25462637},
publisher = {Elsevier Ltd},
title = {{Deep learning in neural networks: An overview}},
url = {http://dx.doi.org/10.1016/j.neunet.2014.09.003 http://linkinghub.elsevier.com/retrieve/pii/S0893608014002135 http://www.ncbi.nlm.nih.gov/pubmed/25462637 http://arxiv.org/abs/1404.7828},
volume = {61},
year = {2015}
}
@article{Bengio2012c,
abstract = {The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, auto-encoders, manifold learning, and deep networks. This motivates longer-term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation and manifold learning.},
archivePrefix = {arXiv},
arxivId = {1206.5538},
author = {Bengio, Yoshua and Courville, Aaron and Vincent, Pascal},
doi = {10.1109/TPAMI.2013.50},
eprint = {1206.5538},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Bengio, Courville, Vincent/2012/Bengio, Courville, Vincent - 2012 - Representation Learning A Review and New Perspectives(2).pdf:pdf},
issn = {0162-8828},
journal = {arXiv preprint arXiv:1206.5538},
keywords = {Boltzmann machine,Deep learning,autoencoder,feature learning,neural nets,representation learning,unsupervised learning},
month = {jun},
number = {8},
title = {{Representation Learning: A Review and New Perspectives}},
url = {http://arxiv.org/abs/1206.5538 http://www.ncbi.nlm.nih.gov/pubmed/23787338 http://ieeexplore.ieee.org/document/6472238/},
volume = {35},
year = {2012}
}
@article{Wetterer2010c,
abstract = {The pharaoh ant, Monomorium pharaonis (LINNAEUS, 1758), has long been considered the most ubiquitous house ant in the world. Monomorium pharaonis is particularly notorious as a pest in hospitals, where it is known as a vector for disease. I compiled and mapped specimen records of M. pharaonis from {\textgreater} 1200 sites to document its known worldwide distribution and evaluate hypotheses concerning its geographic origin. I documented the earliest known M. pharaonis records for 225 geographic areas (countries, island groups, major islands, US states, Canadian provinces, and Russian federal districts), including many for which I found no previously published records: Austral Islands, Central African Republic, Delaware, Dominican Republic, Gabon, Guatemala, Hainan Island, Haiti, Iraq, Ivory Coast, Maine, Missouri, Montana, Nebraska, New Hampshire, Pakistan, Palmyra Atoll, Rhode Island, Tunisia, and West Virginia. In tropical areas, M. pharaonis occurs both indoors and out, but in temperate areas, it is found almost exclusively indoors. It is by far the most common tropical ant found in heated buildings of Europe and North America. Monomorium pharaonis appears to have originated in tropical Asia, where widespread outdoor records have been re- ported. Also, Monomorium longi FOREL, 1902 and Monomorium wroughtoni FOREL, 1902, the two species thought to be most closely related to M. pharaonis, are endemic to tropical Asia. Although M. pharaonis was first described from Egypt, I found no evidence supporting the popular, but apparently mistaken, idea that M. pharaonis is native to Africa. Numerous authors contend that M. pharaonis populations are rapidly expanding. My analyses, however, suggest that M. pharaonis had already spread over much of the world {\textgreater} 100 years ago. Much of the purportedly recent population increases of M. pharaonis may be an artifact of greater sampling and dissemination of information.},
author = {Wetterer, James K.},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Wetterer/2010/Wetterer - 2010 - Worldwide spread of the pharaoh ant, Monomorium pharaonis (Hymenoptera Formicidae).pdf:pdf},
isbn = {1994-4136},
issn = {19944136},
journal = {Myrmecological News},
keywords = {Disease vector,Exotic species,Hospital pest,House pest,Pest ant,Urban pest},
number = {April 2010},
pages = {115--129},
title = {{Worldwide spread of the pharaoh ant, Monomorium pharaonis (Hymenoptera: Formicidae)}},
volume = {13},
year = {2010}
}
@article{Joly2014,
abstract = {Speeding up the collection and integration of raw botanical observation data is a crucial step towards a sustainable development of agriculture and the conservation of biodiversity. Initiated in the context of a citizen sciences project, the main contribution of this paper is an innovative collaborative workflow focused on image-based plant identification as a mean to enlist new contributors and facilitate access to botanical data. Since 2010, hundreds of thousands of geo-tagged and dated plant photographs were collected and revised by hundreds of novice, amateur and expert botanists of a specialized social network. An image-based identification tool - available as both a web and a mobile application - is synchronized with that growing data and allows any user to query or enrich the system with new observations. An important originality is that it works with up to five different organs contrarily to previous approaches that mainly relied on the leaf. This allows querying the system at any period of the year and with complementary images composing a plant observation. Extensive experiments of the visual search engine as well as system-oriented and user-oriented evaluations of the application show that it is already very helpful to determine a plant among hundreds or thousands of species. At the time of writing, the whole framework covers about half of the plant species living in France (2200 species), which already makes it the widest existing automated identification tool (with its imperfections). {\textcopyright} 2013 Elsevier B.V.},
author = {Joly, Alexis and Go{\"{e}}au, Herv{\'{e}} and Bonnet, Pierre and Baki{\'{c}}, Vera and Barbe, Julien and Selmi, Souheil and Yahiaoui, Itheri and Carr{\'{e}}, Jennifer and Mouysset, Elise and Molino, Jean-Fran{\c{c}}ois and Boujemaa, Nozha and Barth{\'{e}}l{\'{e}}my, Daniel},
doi = {10.1016/j.ecoinf.2013.07.006},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Joly et al/2014/Joly et al. - 2014 - Interactive plant identification based on social image data.pdf:pdf},
isbn = {1574-9541},
issn = {15749541},
journal = {Ecological Informatics},
keywords = {Bark,Botanist,Citizen science,Collaborative,Computer vision,Crowdsourcing,Ecology,Flower,Fruit,Identification,Images,Leaf,Monitoring,Multi-organ,Multimedia,Plant,Retrieval,Social network,Surveillance,Visual},
month = {sep},
pages = {22--34},
title = {{Interactive plant identification based on social image data}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S157495411300071X},
volume = {23},
year = {2014}
}
@article{Barre2017,
abstract = {Aims Taxon identification is an important step in many plant ecological studies. Its efficiency and reproducibility might greatly benefit from partly automating this task. Image-based identification systems exist, but mostly rely on hand-crafted algorithms to extract sets of features chosen a priori to identify species of selected taxa. In consequence, such systems are restricted to these taxa and additionally require involving experts that provide taxonomical knowledge for developing such customized systems. The aim of this study was to develop a deep learning system to learn discriminative features from leaf images along with a classifier for species identification of plants. By comparing our results with customized systems like LeafSnap we can show that learning the features by a convolutional neural network (CNN) can provide better feature representation for leaf images compared to hand-crafted features. Methods We developed LeafNet, a CNN-based plant identification system. For evaluation, we utilized the publicly available LeafSnap, Flavia and Foliage datasets. Results Evaluating the recognition accuracies of LeafNet on the LeafSnap, Flavia and Foliage datasets reveals a better performance of LeafNet compared to hand-crafted customized systems. Conclusions Given the overall species diversity of plants, the goal of a complete automatisation of visual plant species identification is unlikely to be met solely by continually gathering assemblies of customized, specialized and hand-crafted (and therefore expensive) identification systems. Deep Learning CNN approaches offer a self-learning state-of-the-art alternative that allows adaption to different taxa just by presenting new training data instead of developing new software systems.},
author = {Barr{\'{e}}, Pierre and St{\"{o}}ver, Ben C. and M{\"{u}}ller, Kai F. and Steinhage, Volker},
doi = {10.1016/j.ecoinf.2017.05.005},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Barr{\'{e}} et al/2017/Barr{\'{e}} et al. - 2017 - LeafNet A computer vision system for automatic plant species identification.pdf:pdf},
issn = {15749541},
journal = {Ecological Informatics},
keywords = {Convolutional layers,Convolutional neural network,Deep learning,Feature maps,Feature representation,Plant classification},
month = {jul},
number = {December 2016},
pages = {50--56},
title = {{LeafNet: A computer vision system for automatic plant species identification}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1574954116302515},
volume = {40},
year = {2017}
}
@article{Gerard2015,
abstract = {Presence of diploid males in wild bees reflects inbreeding and provides information about the health status of a colony or population. Detection of diploid males, and discrimination from haploid males and workers, has, however, been limited to molecular diagnostics. Here we present a novel method based on differences in wing shape, e.g., venation patterns in wings. Themethod is easy to apply and results, for Bombus terrestris ,inveryhigh discrimination success. Possible applications of the method are discussed.},
author = {Gerard, Maxence and Michez, Denis and Fournier, Denis and Maebe, Kevin and Smagghe, Guy and Biesmeijer, Jacobus C. and {De Meulemeester}, Thibaut},
doi = {10.1007/s13592-015-0352-3},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Gerard et al/2015/Gerard et al. - 2015 - Discrimination of haploid and diploid males of Bombus terrestris (Hymenoptera Apidae) based on wing shape.pdf:pdf},
issn = {12979678},
journal = {Apidologie},
keywords = {Bombus,decline,diploid males,geometric morphometrics,wing shape},
month = {sep},
number = {5},
pages = {644--653},
title = {{Discrimination of haploid and diploid males of Bombus terrestris (Hymenoptera; Apidae) based on wing shape}},
url = {http://link.springer.com/10.1007/s13592-015-0352-3},
volume = {46},
year = {2015}
}
@misc{Boer2016,
abstract = {1. There is no evidence for the presence of native Holarctic Lasius and Serviformica-species in North America. It concerns in particular: Formica fusca, Lasius flavus, Lasius umbratus and Lasius alienus. 2. Up to now as Lasius umbratus identified North American specimens are not identical with the (Palearctic) Lasius umbratus. The North American species can relate to Lasius aphidicola, but given the unclear status of this taxon it is here called provisionally L. (Chthonolasius) sp. USA01. 3. The Nearctic workers and images on AntWeb I saw from ants identified as L. flavus, were not identical with this species. 4. Up to now as Lasius alienus identified North American specimens, are not identical with the (Palearctic) type of L. alienus. The North American species is probably identical to L. americanus (revived from synonymy). It is not impossible that this is a complex of species. 5. Lasius niger is a Nearctic species, and perhaps Holarctic, it depends on which status this species include: native or exotic. 6. Lasius (Lasius) sp USA01 and Lasius (Lasius) sp USA02 are undescribed species, or there may be a previously described species or subspecies, or variety, which will be raised to species level.},
author = {Boer, Peter},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Boer/2016/Boer - 2016 - Are their native Holarctic Lasius and Serviformica ant species in the USA, other than exotic ones With a key of the North.pdf:pdf},
language = {English},
month = {mar},
pages = {7},
title = {{Are their native Holarctic Lasius and Serviformica ant species in the USA, other than exotic ones? With a key of the North American Lasius s.str. and Chthonolasius subgenera.}},
url = {http://www.nlmieren.nl/IMAGES/Nearctic Lasius{\_}species.pdf},
year = {2016}
}
@article{Andersen1997,
abstract = {Ecological patterns and processes are characteristically scale dependent, and research findings often cannot be translated easily from one scale to another. Conservation biology is challenged by a lack of congruence between the spatial scales of ecological research (typically involving small plots) and land management (typically involving whole landscapes). Here, I discuss spatial scaling issues as they relate to an understanding of ant communities and, consequently, their use as bioindicators in land management. Our perceptions of fundamental patterns and processes in ant communities depend on scale: taxa that are behaviorally dominant at one scale are not necessarily so at others, functional groups recognized at one scale are often inappropriate for others, and the role of competition in community structure depends on the scale of analysis. Patterns of species richness and composition, and the ability of total richness to be estimated by surrogates, are all also scale dependent. Ant community ecology has a tradition of detailed studies in small plots, but the use of ants as bioindicators requires a predictive understanding of community structure and dynamics at a range of spatial scales. Such an appreciation of ant communities and their most effective use as bioindicators is best served by studies integrating results from plot-scale research with the broad-scale paradigms of biogeography, systematics, and evolutionary biology.},
author = {Andersen, Alan N.},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Andersen/1997/Andersen - 1997 - Using ants as bioindicators Multiple issues in ant community ecology.pdf:pdf},
issn = {17083087},
journal = {Conservation Ecology},
keywords = {Ants,Biodiversity surrogacy,Bioindicators,Community,Diversity patterns,Functional groups,Multiscale,Spatial scale},
number = {1},
pages = {1--17},
title = {{Using ants as bioindicators: Multiple issues in ant community ecology}},
volume = {1},
year = {1997}
}
@article{Pik1999,
abstract = {The concept of taxonomic sufficiency (identifying organisms only to a level of taxonomic resolution sufficient to satisfy the objectives of a study) has received little attention in ecological studies of terrestrial invertebrate assemblages. Here we critically evaluate three approaches to taxonopnic sufficiency: the use of morphospecies, genera and functional groups. The objective was to compare estimates of richness (alpha diversity) and turnover (beta diversity) of ant. assemblages generated by these data with estimates produced using data for ant species. Ground-active ants were sampled using pitfall trapping within three habitat types: a eucalypt plantation, woodland regrowth patches and the surrounding grassland at a study site in the upper Hunter Valley, New South Wales. Comparisons of assemblage richness and turnover among taxonomic data sets and habitats and after different data transformations used univariate (simple correlation and ANOVA) and multivariate (Mantel tests, ANOSIM and SSHMDS) techniques. Our study found: (i) morphospecies and genus richness was highly correlated with species richness a er the study area; (ii) ordination scatterplots using species, morphospecies and genus data revealed similar patterns of site separation for the three habitats; (iii) the results were very similar using untransformed, log transformed and binary data; (iv) functional group ordinations separated all three habitat types for untransformed abundance data; and (v) estimates of species turnover were highly correlated with estimates of morphospecies and genus turnover. These results are discussed in relation to future monitoring of ant community structure},
author = {Pik, Anthony J. and Oliver, Ian and Beattie, Andrew J.},
doi = {10.1046/j.1442-9993.1999.01003.x},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Pik, Oliver, Beattie/1999/Pik, Oliver, Beattie - 1999 - Taxonomic sufficiency in ecological studies of terrestrial invertebrates.pdf:pdf},
isbn = {0307-692X},
issn = {14429985},
journal = {Austral Ecology},
keywords = {Ants,Formicidae,Functional groups,Morphospecies,Taxonomic sufficiency,Terrestrial biodiversity surveys},
number = {5},
pages = {555--562},
title = {{Taxonomic sufficiency in ecological studies of terrestrial invertebrates}},
volume = {24},
year = {1999}
}
@article{Andersen2002,
abstract = {1. The indicator qualities of terrestrial invertebrates are widely recognized in thecontext of detecting ecological change associated with human land-use. However, the useof terrestrial invertebrates as bioindicators remains more a topic of scientific discoursethan a part of land-management practice, largely because their inordinate numbers,taxonomic challenges and general unfamiliarity make invertebrates too intimidatingfor most land-management agencies. Terrestrial invertebrates will not be widely adoptedas bioindicators in land management until simple and efficient protocols have beendeveloped that meet the needs of land managers.2. In Australia, ants are one group of terrestrial insects that has been commonlyadopted as bioindicators in land management, and this study examined the reliability ofa simplified ant assessment protocol designed to be within the capacity of a wide rangeof land managers.3. Ants had previously been surveyed intensively as part of a comprehensive assessmentof biodiversity responses to SO2 emissions from a large copper and lead smelter at MtIsa in the Australian semi-arid tropics. This intensive ant survey yielded 174 species from24 genera, and revealed seven key patterns of ant community structure and compositionin relation to habitat and SO2 levels.4. We tested the extent to which a greatly simplified ant assessment was able toreproduce these results. Our simplified assessment was based on ant ‘bycatch' frombucket-sized (20-litre) pitfall traps used to sample vertebrates as part of the broaderbiodiversity survey. We also greatly simplified the sorting of ant morphospecies byconsidering only large (using a threshold of 4 mm) species, and we reduced sorting time byconsidering only the presence or absence of species at each site. In this manner, the inclusionof ants in the assessment process required less than 10{\%} of the effort demanded by theintensive ant survey.5. Our simplified protocol reproduced virtually all the key findings of the intensivesurvey. This puts effective ant monitoring within the capacity of a wide range of landmanagers.},
author = {Andersen, Alan N. and Hoffmann, Benjamin D. and Muller, Warren J. and Griffiths, Anthony D.},
doi = {10.1046/j.1365-2664.2002.00704.x},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Andersen et al/2002/Andersen et al. - 2002 - Using ants as bioindicators in land management simplifying assessment of ant community responses.pdf:pdf},
isbn = {1365-2664},
issn = {0021-8901},
journal = {Journal of Applied Ecology},
keywords = {Environmental assessment,Land-use impacts,Monitoring,SO2,Sampling protocols},
month = {feb},
number = {1},
pages = {8--17},
title = {{Using ants as bioindicators in land management: simplifying assessment of ant community responses}},
url = {http://doi.wiley.com/10.1046/j.1365-2664.2002.00704.x},
volume = {39},
year = {2002}
}
@inproceedings{Chollet2017,
abstract = {We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters.},
archivePrefix = {arXiv},
arxivId = {1610.02357},
author = {Chollet, Francois},
booktitle = {arXiv preprint arXiv:1610.02357},
doi = {10.1109/CVPR.2017.195},
eprint = {1610.02357},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Chollet/2017/Chollet - 2017 - Xception Deep Learning with Depthwise Separable Convolutions.pdf:pdf},
isbn = {978-1-5386-0457-1},
issn = {1063-6919},
month = {jul},
publisher = {IEEE},
title = {{Xception: Deep Learning with Depthwise Separable Convolutions}},
url = {http://arxiv.org/abs/1610.02357 http://ieeexplore.ieee.org/document/8099678/},
year = {2017}
}
@article{Abadi2016,
abstract = {TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.},
archivePrefix = {arXiv},
arxivId = {1603.04467},
author = {Abadi, Mart{\'{i}}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Mane, Dan and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Viegas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
eprint = {1603.04467},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Abadi et al/2016/Abadi et al. - 2016 - TensorFlow Large-Scale Machine Learning on Heterogeneous Distributed Systems.pdf:pdf},
isbn = {0010-0277},
issn = {0270-6474},
journal = {arxiv preprint arXiv:1603.04467},
month = {mar},
title = {{TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems}},
url = {http://arxiv.org/abs/1603.04467},
year = {2016}
}
@incollection{He2016,
abstract = {Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62{\%} error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers},
archivePrefix = {arXiv},
arxivId = {1603.05027},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {arXiv preprint arXiv:1603.05027},
doi = {10.1007/978-3-319-46493-0_38},
eprint = {1603.05027},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/He et al/2016/He et al. - 2016 - Identity Mappings in Deep Residual Networks.pdf:pdf},
isbn = {9783319464923},
issn = {16113349},
month = {mar},
title = {{Identity Mappings in Deep Residual Networks}},
url = {http://link.springer.com/10.1007/978-3-319-46493-0{\_}38},
year = {2016}
}
@article{Peeters1988,
author = {Peeters, Christian and Crozier, Ross H},
doi = {10.1155/1988/52368},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Peeters, Crozier/1988/Peeters, Crozier - 1988 - Caste and Reproduction in Ants Not All Mated Egg-Layers are “Queens”.pdf:pdf},
issn = {0033-2615},
journal = {Psyche: A Journal of Entomology},
number = {3-4},
pages = {283--288},
title = {{Caste and Reproduction in Ants: Not All Mated Egg-Layers are “Queens”}},
url = {http://www.hindawi.com/journals/psyche/1988/052368/abs/},
volume = {95},
year = {1988}
}
@article{Ribeiro2016,
abstract = {Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally around the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.},
archivePrefix = {arXiv},
arxivId = {1602.04938},
author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
eprint = {1602.04938},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Ribeiro, Singh, Guestrin/2016/Ribeiro, Singh, Guestrin - 2016 - Why Should I Trust You Explaining the Predictions of Any Classifier.pdf:pdf},
isbn = {9781450321389},
issn = {9781450321389},
journal = {arXiv preprint arXiv:1602.04938},
month = {feb},
title = {{"Why Should I Trust You?": Explaining the Predictions of Any Classifier}},
url = {http://arxiv.org/abs/1602.04938},
year = {2016}
}
@misc{AntWebV3,
author = {AntWeb.org},
title = {{AntWeb API (version 3)}},
url = {https://www.antweb.org/api.do},
urldate = {2017-01-22}
}
@misc{AntWebV2,
author = {AntWeb.org},
title = {{AntWeb API (version 2)}},
url = {http://www.antweb.org/api/v2/},
urldate = {2017-01-22}
}
@inproceedings{Han2016,
abstract = {State-of-the-art deep neural networks (DNNs) have hundreds of millions of connections and are both computationally and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources and power budgets. While custom hardware helps the computation, fetching weights from DRAM is two orders of magnitude more expensive than ALU operations, and dominates the required power. Previously proposed 'Deep Compression' makes it possible to fit large DNNs (AlexNet and VGGNet) fully in on-chip SRAM. This compression is achieved by pruning the redundant connections and having multiple connections share the same weight. We propose an energy efficient inference engine (EIE) that performs inference on this compressed network model and accelerates the resulting sparse matrix-vector multiplication with weight sharing. Going from DRAM to SRAM gives EIE 120x energy saving; Exploiting sparsity saves 10x; Weight sharing gives 8x; Skipping zero activations from ReLU saves another 3x. Evaluated on nine DNN benchmarks, EIE is 189x and 13x faster when compared to CPU and GPU implementations of the same DNN without compression. EIE has a processing power of 102GOPS/s working directly on a compressed network, corresponding to 3TOPS/s on an uncompressed network, and processes FC layers of AlexNet at 1.88x10{\^{}}4 frames/sec with a power dissipation of only 600mW. It is 24,000x and 3,400x more energy efficient than a CPU and GPU respectively. Compared with DaDianNao, EIE has 2.9x, 19x and 3x better throughput, energy efficiency and area efficiency.},
archivePrefix = {arXiv},
arxivId = {1602.01528},
author = {Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A. and Dally, William J.},
booktitle = {arXiv preprint arXiv:1507.06228},
doi = {10.1109/ISCA.2016.30},
eprint = {1602.01528},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Han et al/2016/Han et al. - 2016 - EIE Efficient Inference Engine on Compressed Deep Neural Network.pdf:pdf},
isbn = {978-1-4673-8947-1},
keywords = {-deep learning,FormicID CNN,Optimization,acceleration,algorithm-hardware co-design,asic,hardware,model compression},
mendeley-tags = {FormicID CNN,Optimization},
month = {jun},
pages = {243--254},
publisher = {IEEE},
title = {{EIE: Efficient Inference Engine on Compressed Deep Neural Network}},
url = {http://arxiv.org/abs/1602.01528 http://ieeexplore.ieee.org/document/7551397/},
volume = {16},
year = {2016}
}
@article{Szegedy2016,
abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question of whether there are any benefit in combining the Inception architecture with residual connections. Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4, we achieve 3.08 percent top-5 error on the test set of the ImageNet classification (CLS) challenge},
archivePrefix = {arXiv},
arxivId = {1602.07261},
author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alex},
eprint = {1602.07261},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Szegedy et al/2016/Szegedy et al. - 2016 - Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning.pdf:pdf},
journal = {arXiv preprint arXiv:1602.07261},
keywords = {ConvNets,FormicID CNN},
mendeley-tags = {ConvNets,FormicID CNN},
month = {feb},
title = {{Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning}},
url = {http://arxiv.org/abs/1602.07261},
year = {2016}
}
@article{Srivastava2015,
abstract = {Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success. However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information flow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures.},
archivePrefix = {arXiv},
arxivId = {1507.06228},
author = {Srivastava, Rupesh Kumar and Greff, Klaus and Schmidhuber, J{\"{u}}rgen},
eprint = {1507.06228},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Srivastava, Greff, Schmidhuber/2015/Srivastava, Greff, Schmidhuber - 2015 - Training Very Deep Networks.pdf:pdf},
journal = {arXiv preprint arXiv:1507.06228},
keywords = {FormicID CNN,Optimization},
mendeley-tags = {FormicID CNN,Optimization},
month = {jul},
pages = {1--11},
title = {{Training Very Deep Networks}},
url = {http://arxiv.org/abs/1507.06228},
year = {2015}
}
@article{Chatfield2014,
abstract = {The latest generation of Convolutional Neural Networks (CNN) have achieved impressive results in challenging benchmarks on image recognition and object detection, significantly raising the interest of the community in these methods. Nevertheless, it is still unclear how different CNN methods compare with each other and with previous state-of-the-art shallow representations such as the Bag-of-Visual-Words and the Improved Fisher Vector. This paper conducts a rigorous evaluation of these new techniques, exploring different deep architectures and comparing them on a common ground, identifying and disclosing important implementation details. We identify several useful properties of CNN-based representations, including the fact that the dimensionality of the CNN output layer can be reduced significantly without having an adverse effect on performance. We also identify aspects of deep and shallow methods that can be successfully shared. In particular, we show that the data augmentation techniques commonly applied to CNN-based methods can also be applied to shallow methods, and result in an analogous performance boost. Source code and models to reproduce the experiments in the paper is made publicly available.},
archivePrefix = {arXiv},
arxivId = {1405.3531},
author = {Chatfield, Ken and Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
eprint = {1405.3531},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Chatfield et al/2014/Chatfield et al. - 2014 - Return of the Devil in the Details Delving Deep into Convolutional Nets.pdf:pdf},
journal = {arXiv preprint arXiv:1405.3531},
keywords = {Background Info,FormicID CNN},
mendeley-tags = {Background Info,FormicID CNN},
month = {may},
pages = {1--11},
title = {{Return of the Devil in the Details: Delving Deep into Convolutional Nets}},
url = {http://arxiv.org/abs/1405.3531},
year = {2014}
}
@article{Goodfellow2013,
abstract = {We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN.},
archivePrefix = {arXiv},
arxivId = {1302.4389},
author = {Goodfellow, Ian J. and Warde-Farley, David and Mirza, Mehdi and Courville, Aaron and Bengio, Yoshua},
eprint = {1302.4389},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Goodfellow et al/2013/Goodfellow et al. - 2013 - Maxout Networks.pdf:pdf},
journal = {arXiv preprint arXiv:1302.4389},
keywords = {FormicID CNN,Optimization},
mendeley-tags = {FormicID CNN,Optimization},
month = {feb},
title = {{Maxout Networks}},
url = {http://arxiv.org/abs/1302.4389},
year = {2013}
}
@article{Iandola2016,
abstract = {Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet). The SqueezeNet architecture is available for download here: https://github.com/DeepScale/SqueezeNet},
archivePrefix = {arXiv},
arxivId = {1602.07360},
author = {Iandola, Forrest N. and Han, Song and Moskewicz, Matthew W. and Ashraf, Khalid and Dally, William J. and Keutzer, Kurt},
eprint = {1602.07360},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Iandola et al/2016/Iandola et al. - 2016 - SqueezeNet AlexNet-level accuracy with 50x fewer parameters and 0.5MB model size.pdf:pdf},
journal = {arXiv preprint arXiv:1602.07360},
keywords = {ConvNets,FormicID CNN},
mendeley-tags = {ConvNets,FormicID CNN},
month = {feb},
pages = {1--13},
title = {{SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and {\textless}0.5MB model size}},
url = {http://arxiv.org/abs/1602.07360},
year = {2016}
}
@article{Stathakis2009,
abstract = {The question of how many hidden layers and how many hidden nodes should there be always comes up in any classification task of remotely sensed data using neural networks. Until today there has been no exact solution. A method of shedding some light to this question is presented in this paper. A near-optimal solution is discovered after searching with a genetic algorithm. A novel fitness function is introduced that concurrently seeks for the most accurate and compact solution. The proposed method is thoroughly compared to many other methods currently in use, including several heuristics and pruning algorithms. The results are encouraging, indicating that it is time to shift our focus from suboptimal practices to efficient search methods, to tune the parameters of neural networks.},
author = {Stathakis, D.},
doi = {10.1080/01431160802549278},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Stathakis/2009/Stathakis - 2009 - How many hidden layers and nodes.pdf:pdf},
isbn = {0143-1161},
issn = {0143-1161},
journal = {International Journal of Remote Sensing},
keywords = {FormicID CNN,Optimization},
mendeley-tags = {FormicID CNN,Optimization},
month = {apr},
number = {8},
pages = {2133--2147},
title = {{How many hidden layers and nodes?}},
url = {http://www.tandfonline.com/doi/abs/10.1080/01431160802549278},
volume = {30},
year = {2009}
}
@article{Baker2016,
abstract = {At present, designing convolutional neural network (CNN) architectures requires both human expertise and labor. New architectures are handcrafted by careful experimentation or modified from a handful of existing networks. We introduce MetaQNN, a meta-modeling algorithm based on reinforcement learning to automatically generate high-performing CNN architectures for a given learning task. The learning agent is trained to sequentially choose CNN layers using {\$}Q{\$}-learning with an {\$}\backslashepsilon{\$}-greedy exploration strategy and experience replay. The agent explores a large but finite space of possible architectures and iteratively discovers designs with improved performance on the learning task. On image classification benchmarks, the agent-designed networks (consisting of only standard convolution, pooling, and fully-connected layers) beat existing networks designed with the same layer types and are competitive against the state-of-the-art methods that use more complex layer types. We also outperform existing meta-modeling approaches for network design on image classification tasks.},
archivePrefix = {arXiv},
arxivId = {1611.02167},
author = {Baker, Bowen and Gupta, Otkrist and Naik, Nikhil and Raskar, Ramesh},
eprint = {1611.02167},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Baker et al/2016/Baker et al. - 2016 - Designing Neural Network Architectures using Reinforcement Learning.pdf:pdf},
journal = {arXiv preprint arXiv:1611.02167},
keywords = {ConvNets,FormicID CNN},
mendeley-tags = {ConvNets,FormicID CNN},
month = {nov},
pages = {1--18},
title = {{Designing Neural Network Architectures using Reinforcement Learning}},
url = {http://arxiv.org/abs/1611.02167},
year = {2016}
}
@article{Sabour2017,
abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
archivePrefix = {arXiv},
arxivId = {1710.09829},
author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E},
eprint = {1710.09829},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Sabour, Frosst, Hinton/2017/Sabour, Frosst, Hinton - 2017 - Dynamic Routing Between Capsules.pdf:pdf},
journal = {arXiv preprint arXiv:1710.09829},
keywords = {FormicID CNN,Optimization},
mendeley-tags = {FormicID CNN,Optimization},
month = {oct},
number = {Nips},
title = {{Dynamic Routing Between Capsules}},
url = {http://arxiv.org/abs/1710.09829},
year = {2017}
}
@article{Thoma2017,
abstract = {Convolutional Neural Networks (CNNs) dominate various computer vision tasks since Alex Krizhevsky showed that they can be trained effectively and reduced the top-5 error from 26.2 {\%} to 15.3 {\%} on the ImageNet large scale visual recognition challenge. Many aspects of CNNs are examined in various publications, but literature about the analysis and construction of neural network architectures is rare. This work is one step to close this gap. A comprehensive overview over existing techniques for CNN analysis and topology construction is provided. A novel way to visualize classification errors with confusion matrices was developed. Based on this method, hierarchical classifiers are described and evaluated. Additionally, some results are confirmed and quantified for CIFAR-100. For example, the positive impact of smaller batch sizes, averaging ensembles, data augmentation and test-time transformations on the accuracy. Other results, such as the positive impact of learned color transformation on the test accuracy could not be confirmed. A model which has only one million learned parameters for an input size of 32x32x3 and 100 classes and which beats the state of the art on the benchmark dataset Asirra, GTSRB, HASYv2 and STL-10 was developed.},
archivePrefix = {arXiv},
arxivId = {1707.09725},
author = {Thoma, Martin},
eprint = {1707.09725},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Thoma/2017/Thoma - 2017 - Analysis and Optimization of Convolutional Neural Network Architectures.pdf:pdf},
journal = {arXiv preprint arXiv:1707.09725},
keywords = {FormicID CNN,Optimization},
mendeley-tags = {FormicID CNN,Optimization},
month = {jul},
number = {August},
title = {{Analysis and Optimization of Convolutional Neural Network Architectures}},
url = {http://arxiv.org/abs/1707.09725},
year = {2017}
}
@article{Szegedy2015,
abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2{\%} top-1 and 5.6{\%} top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5{\%} top-5 error on the validation set (3.6{\%} error on the test set) and 17.3{\%} top-1 error on the validation set.},
archivePrefix = {arXiv},
arxivId = {1512.00567},
author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
doi = {10.1109/CVPR.2016.308},
eprint = {1512.00567},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Szegedy et al/2015/Szegedy et al. - 2015 - Rethinking the Inception Architecture for Computer Vision.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {08866236},
journal = {arXiv preprint arXiv:1512.00567},
keywords = {ConvNets,FormicID CNN},
mendeley-tags = {ConvNets,FormicID CNN},
month = {dec},
pages = {2818--2826},
pmid = {8190083},
publisher = {IEEE},
title = {{Rethinking the Inception Architecture for Computer Vision}},
url = {http://arxiv.org/abs/1512.00567 http://ieeexplore.ieee.org/document/7780677/},
year = {2015}
}
@article{Marcus2018,
abstract = {Although deep learning has historical roots going back decades, neither the term "deep learning" nor the approach was popular just over five years ago, when the field was reignited by papers such as Krizhevsky, Sutskever and Hinton's now classic (2012) deep network model of Imagenet. What has the field discovered in the five subsequent years? Against a background of considerable progress in areas such as speech recognition, image recognition, and game playing, and considerable enthusiasm in the popular press, I present ten concerns for deep learning, and suggest that deep learning must be supplemented by other techniques if we are to reach artificial general intelligence.},
archivePrefix = {arXiv},
arxivId = {1801.00631},
author = {Marcus, Gary},
eprint = {1801.00631},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Marcus/2018/Marcus - 2018 - Deep Learning A Critical Appraisal.pdf:pdf},
journal = {arXiv preprint arXiv:1801.00631},
keywords = {Background Info,FormicID CNN},
mendeley-tags = {Background Info,FormicID CNN},
month = {jan},
pages = {1--27},
title = {{Deep Learning: A Critical Appraisal}},
url = {http://arxiv.org/abs/1801.00631},
year = {2018}
}
@inproceedings{Dodge2016,
abstract = {Image quality is an important practical challenge that is often overlooked in the design of machine vision systems. Commonly, machine vision systems are trained and tested on high quality image datasets, yet in practical applications the input images can not be assumed to be of high quality. Recently, deep neural networks have obtained state-of-the-art performance on many machine vision tasks. In this paper we provide an evaluation of 4 state-of-the-art deep neural network models for image classification under quality distortions. We consider five types of quality distortions: blur, noise, contrast, JPEG, and JPEG2000 compression. We show that the existing networks are susceptible to these quality distortions, particularly to blur and noise. These results enable future work in developing deep neural networks that are more invariant to quality distortions.},
archivePrefix = {arXiv},
arxivId = {1604.04004},
author = {Dodge, Samuel and Karam, Lina},
booktitle = {arXiv preprint arXiv:1604.04004},
doi = {10.1109/QoMEX.2016.7498955},
eprint = {1604.04004},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Dodge, Karam/2016/Dodge, Karam - 2016 - Understanding how image quality affects deep neural networks.pdf:pdf},
keywords = {Background Info,FormicID CNN},
mendeley-tags = {Background Info,FormicID CNN},
month = {jun},
pages = {1--6},
publisher = {IEEE},
title = {{Understanding how image quality affects deep neural networks}},
url = {http://ieeexplore.ieee.org/document/7498955/},
year = {2016}
}
@article{Ba2013,
abstract = {Currently, deep neural networks are the state of the art on problems such as speech recognition and computer vision. In this extended abstract, we show that shallow feed-forward networks can learn the complex functions previously learned by deep nets and achieve accuracies previously only achievable with deep models. Moreover, in some cases the shallow neural nets can learn these deep functions using a total number of parameters similar to the original deep model. We evaluate our method on the TIMIT phoneme recognition task and are able to train shallow fully-connected nets that perform similarly to complex, well-engineered, deep convolutional architectures. Our success in training shallow neural nets to mimic deeper models suggests that there probably exist better algorithms for training shallow feed-forward nets than those currently available.},
archivePrefix = {arXiv},
arxivId = {1312.6184},
author = {Ba, Lei Jimmy and Caruana, Rich},
doi = {10.1038/nature14539},
eprint = {1312.6184},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Ba, Caruana/2013/Ba, Caruana - 2013 - Do Deep Nets Really Need to be Deep.pdf:pdf},
isbn = {3135786504},
issn = {0028-0836},
journal = {arXiv preprint arXiv:1312.6184},
keywords = {Background Info,FormicID CNN},
mendeley-tags = {Background Info,FormicID CNN},
title = {{Do Deep Nets Really Need to be Deep?}},
url = {http://arxiv.org/abs/1312.6184 http://www.nature.com/articles/nature14539},
year = {2013}
}
@article{Sethi2017,
abstract = {With an abundance of research papers in deep learning, reproducibility or adoption of the existing works becomes a challenge. This is due to the lack of open source implementations provided by the authors. Further, re-implementing research papers in a different library is a daunting task. To address these challenges, we propose a novel extensible approach, DLPaper2Code, to extract and understand deep learning design flow diagrams and tables available in a research paper and convert them to an abstract computational graph. The extracted computational graph is then converted into execution ready source code in both Keras and Caffe, in real-time. An arXiv-like website is created where the automatically generated designs is made publicly available for 5,000 research papers. The generated designs could be rated and edited using an intuitive drag-and-drop UI framework in a crowdsourced manner. To evaluate our approach, we create a simulated dataset with over 216,000 valid design visualizations using a manually defined grammar. Experiments on the simulated dataset show that the proposed framework provide more than {\$}93\backslash{\%}{\$} accuracy in flow diagram content extraction.},
archivePrefix = {arXiv},
arxivId = {1711.03543},
author = {Sethi, Akshay and Sankaran, Anush and Panwar, Naveen and Khare, Shreya and Mani, Senthil},
eprint = {1711.03543},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Sethi et al/2017/Sethi et al. - 2017 - DLPaper2Code Auto-generation of Code from Deep Learning Research Papers.pdf:pdf},
journal = {arXiv preprint arXiv:1711.03543},
keywords = {FormicID CNN,Others},
mendeley-tags = {FormicID CNN,Others},
month = {nov},
title = {{DLPaper2Code: Auto-generation of Code from Deep Learning Research Papers}},
url = {http://arxiv.org/abs/1711.03543},
year = {2017}
}
@article{Schulman2017,
abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
archivePrefix = {arXiv},
arxivId = {1707.06347},
author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
eprint = {1707.06347},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Schulman et al/2017/Schulman et al. - 2017 - Proximal Policy Optimization Algorithms.pdf:pdf},
journal = {arXiv preprint arXiv:1707.06347},
keywords = {FormicID CNN,Optimization},
mendeley-tags = {FormicID CNN,Optimization},
month = {jul},
pages = {1--12},
title = {{Proximal Policy Optimization Algorithms}},
url = {http://arxiv.org/abs/1707.06347},
year = {2017}
}
@article{Mishkin2017,
abstract = {The paper systematically studies the impact of a range of recent advances in CNN architectures and learning methods on the object categorization (ILSVRC) problem. The evalution tests the influence of the following choices of the architecture: non-linearity (ReLU, ELU, maxout, compatibility with batch normalization), pooling variants (stochastic, max, average, mixed), network width, classifier design (convolutional, fully-connected, SPP), image pre-processing, and of learning parameters: learning rate, batch size, cleanliness of the data, etc. The performance gains of the proposed modifications are first tested individually and then in combination. The sum of individual gains is bigger than the observed improvement when all modifications are introduced, but the "deficit" is small suggesting independence of their benefits. We show that the use of 128x128 pixel images is sufficient to make qualitative conclusions about optimal network structure that hold for the full size Caffe and VGG nets. The results are obtained an order of magnitude faster than with the standard 224 pixel images.},
archivePrefix = {arXiv},
arxivId = {1606.02228},
author = {Mishkin, Dmytro and Sergievskiy, Nikolay and Matas, Jiri},
doi = {10.1016/j.cviu.2017.05.007},
eprint = {1606.02228},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Mishkin, Sergievskiy, Matas/2017/Mishkin, Sergievskiy, Matas - 2017 - Systematic evaluation of CNN advances on the Imagenet.pdf:pdf},
journal = {arXiv preprint arXiv:1606.02228},
keywords = {Background Info,FormicID CNN,benchmark,cnn,imagenet,non-linearity,pooling},
mendeley-tags = {Background Info,FormicID CNN},
month = {aug},
pages = {11--19},
title = {{Systematic evaluation of CNN advances on the Imagenet}},
url = {http://arxiv.org/abs/1606.02228{\%}0Ahttp://dx.doi.org/10.1016/j.cviu.2017.05.007 http://linkinghub.elsevier.com/retrieve/pii/S1077314217300814},
volume = {161},
year = {2017}
}
@article{Schuettpelz2017,
author = {Schuettpelz, Eric and Frandsen, Paul and Dikow, Rebecca and Brown, Abel and Orli, Sylvia and Peters, Melinda and Metallo, Adam and Funk, Vicki and Dorr, Laurence},
doi = {10.3897/BDJ.5.e21139},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Schuettpelz et al/2017/Schuettpelz et al. - 2017 - Applications of deep convolutional neural networks to digitized natural history collections.pdf:pdf},
issn = {1314-2828},
journal = {Biodiversity Data Journal},
keywords = {Background Info,FormicID CNN},
mendeley-tags = {Background Info,FormicID CNN},
month = {nov},
number = {e21139},
title = {{Applications of deep convolutional neural networks to digitized natural history collections}},
url = {https://bdj.pensoft.net/articles.php?id=21139},
volume = {5},
year = {2017}
}
@article{Koushik2016,
abstract = {In this paper we propose a simple and efficient method for improving stochastic gradient descent methods by using feedback from the objective function. The method tracks the relative changes in the objective function with a running average, and uses it to adaptively tune the learning rate in stochastic gradient descent. We specifically apply this idea to modify Adam, a popular algorithm for training deep neural networks. We conduct experiments to compare the resulting algorithm, which we call Eve, with state of the art methods used for training deep learning models. We train CNNs for image classification, and RNNs for language modeling and question answering. Our experiments show that Eve outperforms all other algorithms on these benchmark tasks. We also analyze the behavior of the feedback mechanism during the training process.},
archivePrefix = {arXiv},
arxivId = {1611.01505},
author = {Koushik, Jayanth and Hayashi, Hiroaki},
eprint = {1611.01505},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Koushik, Hayashi/2016/Koushik, Hayashi - 2016 - Improving Stochastic Gradient Descent with Feedback.pdf:pdf},
journal = {arXiv preprint arXiv:1611.01505v1},
keywords = {FormicID CNN,Optimization},
mendeley-tags = {FormicID CNN,Optimization},
month = {nov},
title = {{Improving Stochastic Gradient Descent with Feedback}},
url = {http://arxiv.org/abs/1611.01505},
year = {2016}
}
@article{George2017,
abstract = {Learning from few examples and generalizing to dramatically different situations are capabilities of human visual intelligence that are yet to be matched by leading machine learning models. By drawing inspiration from systems neuroscience, we introduce a probabilistic generative model for vision in which message-passing based inference handles recognition, segmentation and reasoning in a unified way. The model demonstrates excellent generalization and occlusion-reasoning capabilities, and outperforms deep neural networks on a challenging scene text recognition benchmark while being 300-fold more data efficient. In addition, the model fundamentally breaks the defense of modern text-based CAPTCHAs by generatively segmenting characters without CAPTCHA-specific heuristics. Our model emphasizes aspects like data efficiency and compositionality that may be important in the path toward general artificial intelligence.},
author = {George, Dileep and Lehrach, Wolfgang and Kansky, Ken and L{\'{a}}zaro-Gredilla, M. and Laan, Christopher and Marthi, Bhaskara and Lou, Xinghua and Meng, Zaoshi and Liu, Yi and Wang, Huayan and Lavin, Alex and Phoenix, D. Scott},
doi = {10.1126/science.aag2612},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/George et al/2017/George et al. - 2017 - A generative vision model that trains with high data efficiency and breaks text-based CAPTCHAs.pdf:pdf},
issn = {0036-8075},
journal = {Science},
keywords = {Background Info,FormicID CNN},
mendeley-tags = {Background Info,FormicID CNN},
month = {oct},
pmid = {29074582},
title = {{A generative vision model that trains with high data efficiency and breaks text-based CAPTCHAs}},
url = {http://science.sciencemag.org/content/sci/early/2017/10/26/science.aag2612.full.pdf http://www.sciencemag.org/lookup/doi/10.1126/science.aag2612 http://www.ncbi.nlm.nih.gov/pubmed/29074582},
volume = {10},
year = {2017}
}
@article{Janocha2017,
abstract = {Deep neural networks are currently among the most commonly used classifiers. Despite easily achieving very good performance, one of the best selling points of these models is their modular design - one can conveniently adapt their architecture to specific needs, change connectivity patterns, attach specialised layers, experiment with a large amount of activation functions, normalisation schemes and many others. While one can find impressively wide spread of various configurations of almost every aspect of the deep nets, one element is, in authors' opinion, underrepresented - while solving classification problems, vast majority of papers and applications simply use log loss. In this paper we try to investigate how particular choices of loss functions affect deep models and their learning dynamics, as well as resulting classifiers robustness to various effects. We perform experiments on classical datasets, as well as provide some additional, theoretical insights into the problem. In particular we show that L1 and L2 losses are, quite surprisingly, justified classification objectives for deep nets, by providing probabilistic interpretation in terms of expected misclassification. We also introduce two losses which are not typically used as deep nets objectives and show that they are viable alternatives to the existing ones.},
archivePrefix = {arXiv},
arxivId = {1702.05659},
author = {Janocha, Katarzyna and Czarnecki, Wojciech Marian},
doi = {10.4467/20838476SI.16.004.6185},
eprint = {1702.05659},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Janocha, Czarnecki/2017/Janocha, Czarnecki - 2017 - On Loss Functions for Deep Neural Networks in Classification.pdf:pdf},
issn = {20838476},
journal = {arXiv preprint ArXiv:1702.05659},
keywords = {FormicID CNN,Optimization},
mendeley-tags = {FormicID CNN,Optimization},
month = {feb},
title = {{On Loss Functions for Deep Neural Networks in Classification}},
url = {http://arxiv.org/abs/1702.05659 http://www.ejournals.eu/Schedae-Informaticae/2016/Volume-25/art/9009/},
year = {2017}
}
@article{Huang2016a,
abstract = {Very deep convolutional networks with hundreds of layers have led to significant reductions in error on competitive benchmarks. Although the unmatched expressiveness of the many layers can be highly desirable at test time, training very deep networks comes with its own set of challenges. The gradients can vanish, the forward flow often diminishes, and the training time can be painfully slow. To address these problems, we propose stochastic depth, a training procedure that enables the seemingly contradictory setup to train short networks and use deep networks at test time. We start with very deep networks but during training, for each mini-batch, randomly drop a subset of layers and bypass them with the identity function. This simple approach complements the recent success of residual networks. It reduces training time substantially and improves the test error significantly on almost all data sets that we used for evaluation. With stochastic depth we can increase the depth of residual networks even beyond 1200 layers and still yield meaningful improvements in test error (4.91{\%} on CIFAR-10).},
archivePrefix = {arXiv},
arxivId = {1603.09382},
author = {Huang, Gao and Sun, Yu and Liu, Zhuang and Sedra, Daniel and Weinberger, Kilian},
doi = {10.1007/978-3-319-46493-0_39},
eprint = {1603.09382},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Huang et al/2016/Huang et al. - 2016 - Deep Networks with Stochastic Depth.pdf:pdf},
isbn = {9783319464930},
issn = {0302-9743},
journal = {arXiv preprint arXiv:1603.09382v3},
keywords = {ConvNets,FormicID CNN},
mendeley-tags = {ConvNets,FormicID CNN},
month = {mar},
pmid = {4520227},
title = {{Deep Networks with Stochastic Depth}},
url = {http://arxiv.org/abs/1603.09382 http://link.springer.com/10.1007/978-3-319-46493-0{\_}39},
year = {2016}
}
@article{Loshchilov2016,
abstract = {Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14{\%} and 16.21{\%}, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR},
archivePrefix = {arXiv},
arxivId = {1608.03983v5},
author = {Loshchilov, Ilya and Hutter, Frank},
eprint = {1608.03983v5},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Loshchilov, Hutter/2016/Loshchilov, Hutter - 2016 - SGDR Stochastic Gradient Descent with Warm Restarts.pdf:pdf},
journal = {arXiv preprint arXiv:1608.03983v5},
keywords = {FormicID CNN,Optimization},
mendeley-tags = {FormicID CNN,Optimization},
month = {aug},
pages = {1--16},
title = {{SGDR: Stochastic Gradient Descent with Warm Restarts}},
url = {http://arxiv.org/abs/1608.03983v5},
year = {2016}
}
@article{Donahue2013,
abstract = {We evaluate whether features extracted from the activation of a deep convolutional network trained in a fully supervised fashion on a large, fixed set of object recognition tasks can be re-purposed to novel generic tasks. Our generic tasks may differ significantly from the originally trained tasks and there may be insufficient labeled or unlabeled data to conventionally train or adapt a deep architecture to the new tasks. We investigate and visualize the semantic clustering of deep convolutional features with respect to a variety of such tasks, including scene recognition, domain adaptation, and fine-grained recognition challenges. We compare the efficacy of relying on various network levels to define a fixed feature, and report novel results that significantly outperform the state-of-the-art on several important vision challenges. We are releasing DeCAF, an open-source implementation of these deep convolutional activation features, along with all associated network parameters to enable vision researchers to be able to conduct experimentation with deep representations across a range of visual concept learning paradigms.},
archivePrefix = {arXiv},
arxivId = {1310.1531},
author = {Donahue, Jeff and Jia, Yangqing and Vinyals, Oriol and Hoffman, Judy and Zhang, Ning and Tzeng, Eric and Darrell, Trevor},
eprint = {1310.1531},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Donahue et al/2013/Donahue et al. - 2013 - DeCAF A Deep Convolutional Activation Feature for Generic Visual Recognition.pdf:pdf},
isbn = {9781634393973},
issn = {1938-7228},
journal = {arXiv preprint arXiv:1310.1531},
keywords = {Background Info,FormicID CNN},
mendeley-tags = {Background Info,FormicID CNN},
title = {{DeCAF: A Deep Convolutional Activation Feature for Generic Visual Recognition}},
url = {http://arxiv.org/abs/1310.1531},
year = {2013}
}
@article{Xu2015,
abstract = {In this paper we investigate the performance of different types of rectified activation functions in convolutional neural network: standard rectified linear unit (ReLU), leaky rectified linear unit (Leaky ReLU), parametric rectified linear unit (PReLU) and a new randomized leaky rectified linear units (RReLU). We evaluate these activation function on standard image classification task. Our experiments suggest that incorporating a non-zero slope for negative part in rectified activation units could consistently improve the results. Thus our findings are negative on the common belief that sparsity is the key of good performance in ReLU. Moreover, on small scale dataset, using deterministic negative slope or learning it are both prone to overfitting. They are not as effective as using their randomized counterpart. By using RReLU, we achieved 75.68$\backslash${\%} accuracy on CIFAR-100 test set without multiple test or ensemble.},
archivePrefix = {arXiv},
arxivId = {1505.00853},
author = {Xu, Bing and Wang, Naiyan and Chen, Tianqi and Li, Mu},
eprint = {1505.00853},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Xu et al/2015/Xu et al. - 2015 - Empirical Evaluation of Rectified Activations in Convolutional Network.pdf:pdf},
journal = {arXiv preprint arXiv:1505.00853v2},
keywords = {FormicID CNN,Optimization},
mendeley-tags = {FormicID CNN,Optimization},
month = {may},
title = {{Empirical Evaluation of Rectified Activations in Convolutional Network}},
url = {http://arxiv.org/abs/1505.00853v2 http://arxiv.org/abs/1505.00853},
year = {2015}
}
@article{Held2015,
abstract = {Deep learning methods have typically been trained on large datasets in which many training examples are available. However, many real-world product datasets have only a small number of images available for each product. We explore the use of deep learning methods for recognizing object instances when we have only a single training example per class. We show that feedforward neural networks outperform state-of-the-art methods for recognizing objects from novel viewpoints even when trained from just a single image per object. To further improve our performance on this task, we propose to take advantage of a supplementary dataset in which we observe a separate set of objects from multiple viewpoints. We introduce a new approach for training deep learning methods for instance recognition with limited training data, in which we use an auxiliary multi-view dataset to train our network to be robust to viewpoint changes. We find that this approach leads to a more robust classifier for recognizing objects from novel viewpoints, outperforming previous state-of-the-art approaches including keypoint-matching, template-based techniques, and sparse coding.},
archivePrefix = {arXiv},
arxivId = {1507.08286},
author = {Held, David and Thrun, Sebastian and Savarese, Silvio},
eprint = {1507.08286},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Held, Thrun, Savarese/2015/Held, Thrun, Savarese - 2015 - Deep Learning for Single-View Instance Recognition.pdf:pdf},
isbn = {9781467380256},
journal = {arXiv preprint arXiv:1507.08286},
keywords = {ConvNets,FormicID CNN},
mendeley-tags = {ConvNets,FormicID CNN},
month = {jul},
title = {{Deep Learning for Single-View Instance Recognition}},
url = {http://arxiv.org/abs/1507.08286},
year = {2015}
}
@article{Huang2016,
abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and models are available at https://github.com/liuzhuang13/DenseNet .},
archivePrefix = {arXiv},
arxivId = {1608.06993},
author = {Huang, Gao and Liu, Zhuang and Weinberger, Kilian Q. and van der Maaten, Laurens},
eprint = {1608.06993},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Huang et al/2016/Huang et al. - 2016 - Densely Connected Convolutional Networks.pdf:pdf},
issn = {0002-9645},
journal = {arXiv preprint arXiv:608.06993},
keywords = {ConvNets,FormicID CNN},
mendeley-tags = {ConvNets,FormicID CNN},
month = {aug},
title = {{Densely Connected Convolutional Networks}},
url = {http://arxiv.org/abs/1608.06993},
year = {2016}
}
@misc{Chollet2015,
author = {Chollet, Fran{\c{c}}ois},
booktitle = {Github},
keywords = {FormicID CNN,Websites},
mendeley-tags = {FormicID CNN,Websites},
title = {{Keras}},
url = {https://github.com/fchollet/keras},
year = {2015}
}
@article{Zeiler2014,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky $\backslash$etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
archivePrefix = {arXiv},
arxivId = {1311.2901v3},
author = {Zeiler, Matthew D and Fergus, Rob},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {1311.2901v3},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Zeiler, Fergus/2014/Zeiler, Fergus - 2014 - Visualizing and Understanding Convolutional Networks.pdf:pdf},
isbn = {978-3-319-10589-5},
issn = {978-3-319-10589-5},
journal = {arXiv preprint arXiv:1311.2901v3},
keywords = {FormicID CNN,Visualizing},
mendeley-tags = {FormicID CNN,Visualizing},
month = {nov},
title = {{Visualizing and Understanding Convolutional Networks}},
url = {https://arxiv.org/abs/1311.2901v3 http://arxiv.org/abs/1311.2901 http://link.springer.com/10.1007/978-3-319-10590-1{\_}53},
year = {2014}
}
@article{Simonyan2013,
abstract = {This paper addresses the visualisation of image classification models, learnt using deep Convolutional Networks (ConvNets). We consider two visualisation techniques, based on computing the gradient of the class score with respect to the input image. The first one generates an image, which maximises the class score [Erhan et al., 2009], thus visualising the notion of the class, captured by a ConvNet. The second technique computes a class saliency map, specific to a given image and class. We show that such maps can be employed for weakly supervised object segmentation using classification ConvNets. Finally, we establish the connection between the gradient-based ConvNet visualisation methods and deconvolutional networks [Zeiler et al., 2013].},
archivePrefix = {arXiv},
arxivId = {1312.6034v2},
author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
eprint = {1312.6034v2},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Simonyan, Vedaldi, Zisserman/2013/Simonyan, Vedaldi, Zisserman - 2013 - Deep Inside Convolutional Networks Visualising Image Classification Models and Saliency Maps.pdf:pdf},
journal = {arXiv preprint arXiv:1312.6034v2},
keywords = {FormicID CNN,Visualizing},
mendeley-tags = {FormicID CNN,Visualizing},
month = {dec},
title = {{Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps}},
url = {http://arxiv.org/abs/1312.6034v2},
year = {2013}
}
@article{Howard2013,
abstract = {We investigate multiple techniques to improve upon the current state of the art deep convolutional neural network based image classification pipeline. The techiques include adding more image transformations to training data, adding more transformations to generate additional predictions at test time and using complementary models applied to higher resolution images. This paper summarizes our entry in the Imagenet Large Scale Visual Recognition Challenge 2013. Our system achieved a top 5 classification error rate of 13.55{\%} using no external data which is over a 20{\%} relative improvement on the previous year's winner.},
archivePrefix = {arXiv},
arxivId = {1312.5402},
author = {Howard, Andrew G.},
eprint = {1312.5402},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Howard/2013/Howard - 2013 - Some Improvements on Deep Convolutional Neural Network Based Image Classification.pdf:pdf},
journal = {arXiv preprint arXiv:1312.5402},
keywords = {ConvNets,FormicID CNN},
mendeley-tags = {ConvNets,FormicID CNN},
month = {dec},
title = {{Some Improvements on Deep Convolutional Neural Network Based Image Classification}},
url = {http://arxiv.org/abs/1312.5402},
year = {2013}
}
@article{Domingos2012,
author = {Domingos, Pedro},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Domingos/2012/Domingos - 2012 - A few useful things to know about machine learning.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
keywords = {FormicID CNN,Python {\&} Machine Learning},
mendeley-tags = {FormicID CNN,Python {\&} Machine Learning},
month = {oct},
number = {10},
pages = {78},
publisher = {ACM},
title = {{A few useful things to know about machine learning}},
url = {http://dl.acm.org/ft{\_}gateway.cfm?id=2347755{\&}type=html},
volume = {55},
year = {2012}
}
@article{Kang2012,
author = {Kang, Seung-Ho and Song, Su-Hee and Lee, Sang-Hee},
doi = {10.1016/j.aspen.2012.03.006},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Kang, Song, Lee/2012/Kang, Song, Lee - 2012 - Identification of butterfly species with a single neural network system.pdf:pdf},
issn = {12268615},
journal = {Journal of Asia-Pacific Entomology},
keywords = {FormicID CNN,Other ID methods,automatic species identification},
mendeley-tags = {FormicID CNN,Other ID methods},
month = {sep},
number = {3},
pages = {431--435},
title = {{Identification of butterfly species with a single neural network system}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1226861512000374},
volume = {15},
year = {2012}
}
@article{Weeks1997,
author = {Weeks, Paul J. D. and Gaston, Kevin J.},
doi = {10.1023/a:1018348204573},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Weeks, Gaston/1997/Weeks, Gaston - 1997 - Image analysis, neural networks, and the taxonomic impediment to biodiversity studies.pdf:pdf},
journal = {Biodiversity and Conservation},
keywords = {Background Info,FormicID CNN,biodiversity,identification,image analysis,neural networks,taxonomy},
mendeley-tags = {Background Info,FormicID CNN},
number = {2},
pages = {263--274},
title = {{Image analysis, neural networks, and the taxonomic impediment to biodiversity studies}},
url = {http://dx.doi.org/10.1023/A:1018348204573},
volume = {6},
year = {1997}
}
@incollection{Kumar2012,
abstract = {We describe the first mobile app for identifying plant species using automatic visual recognition. The system – called Leafsnap – identifies tree species from photographs of their leaves. Key to this system are computer vision components for discarding non-leaf images, segmenting the leaf from an untextured background, extracting features representing the curvature of the leaf's contour over multiple scales, and identifying the species from a dataset of the 184 trees in the Northeastern United States. Our system obtains state-of-the-art performance on the real-world images from the new Leafsnap Dataset – the largest of its kind. Throughout the paper, we document many of the practical steps needed to produce a computer vision system such as ours, which currently has nearly a million users.},
author = {Kumar, Neeraj and Belhumeur, Peter N and Biswas, Arijit and Jacobs, David W},
booktitle = {Computer Vision – ECCV 2012},
doi = {10.1007/978-3-642-33709-3_36},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Kumar et al/2012/Kumar et al. - 2012 - Leafsnap A Computer Vision System for Automatic Plant Species Identification.pdf:pdf},
isbn = {978-3-642-33709-3},
keywords = {FormicID CNN,Other ID methods},
mendeley-tags = {FormicID CNN,Other ID methods},
pages = {502--516},
title = {{Leafsnap: A Computer Vision System for Automatic Plant Species Identification}},
url = {http://dx.doi.org/10.1007/978-3-642-33709-3{\_}36},
year = {2012}
}
@article{Simonyan2014,
abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
archivePrefix = {arXiv},
arxivId = {1409.1556v6},
author = {Simonyan, Karen and Zisserman, Andrew},
eprint = {1409.1556v6},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Simonyan, Zisserman/2014/Simonyan, Zisserman - 2014 - Very Deep Convolutional Networks for Large-Scale Image Recognition.pdf:pdf},
isbn = {0950-5849},
issn = {1095-9203},
journal = {arXiv preprint arXiv:1409.1556v6},
keywords = {ConvNets,FormicID CNN},
mendeley-tags = {ConvNets,FormicID CNN},
month = {sep},
title = {{Very Deep Convolutional Networks for Large-Scale Image Recognition}},
url = {http://arxiv.org/abs/1409.1556v6},
year = {2014}
}
@article{Hinton2012,
abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
archivePrefix = {arXiv},
arxivId = {1207.0580},
author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
eprint = {1207.0580},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Hinton et al/2012/Hinton et al. - 2012 - Improving neural networks by preventing co-adaptation of feature detectors.pdf:pdf},
isbn = {9781467394673},
issn = {9781467394673},
journal = {arXiv preprint arXiv:1207.0580},
keywords = {FormicID CNN,Optimization},
mendeley-tags = {FormicID CNN,Optimization},
month = {jul},
title = {{Improving neural networks by preventing co-adaptation of feature detectors}},
url = {http://arxiv.org/abs/1207.0580},
year = {2012}
}
@article{Ba2016,
abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
archivePrefix = {arXiv},
arxivId = {1607.06450},
author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
eprint = {1607.06450},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Ba, Kiros, Hinton/2016/Ba, Kiros, Hinton - 2016 - Layer Normalization.pdf:pdf},
isbn = {978-3-642-04273-7},
issn = {1607.06450},
journal = {arXiv preprint arXiv:1607.06450},
keywords = {FormicID CNN,Optimization},
mendeley-tags = {FormicID CNN,Optimization},
month = {jul},
title = {{Layer Normalization}},
url = {http://arxiv.org/abs/1607.06450},
year = {2016}
}
@article{Han2015,
abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce "deep compression", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.},
archivePrefix = {arXiv},
arxivId = {1510.00149},
author = {Han, Song and Mao, Huizi and Dally, William J.},
doi = {abs/1510.00149/1510.00149},
eprint = {1510.00149},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Han, Mao, Dally/2015/Han, Mao, Dally - 2015 - Deep Compression Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding.pdf:pdf},
journal = {arXiv preprint arXiv:1510.00149},
keywords = {FormicID CNN,Optimization},
mendeley-tags = {FormicID CNN,Optimization},
month = {oct},
title = {{Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding}},
url = {http://arxiv.org/abs/1510.00149},
year = {2015}
}
@article{Hinton2006a,
abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such "autoencoder" networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
author = {Hinton, G. E.},
doi = {10.1126/science.1127647},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Hinton/2006/Hinton - 2006 - Reducing the Dimensionality of Data with Neural Networks.pdf:pdf},
issn = {0036-8075},
journal = {Science},
keywords = {FormicID CNN,Optimization},
mendeley-tags = {FormicID CNN,Optimization},
month = {jul},
number = {5786},
pages = {504--507},
pmid = {16873662},
title = {{Reducing the Dimensionality of Data with Neural Networks}},
url = {http://www.sciencemag.org/cgi/doi/10.1126/science.1127647 http://www.ncbi.nlm.nih.gov/pubmed/16873662},
volume = {313},
year = {2006}
}
@article{LeCun2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
archivePrefix = {arXiv},
arxivId = {1603.05691},
author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
eprint = {1603.05691},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/LeCun, Bengio, Hinton/2015/LeCun, Bengio, Hinton - 2015 - Deep learning.pdf:pdf},
isbn = {3135786504},
issn = {0028-0836},
journal = {Nature},
keywords = {Background Info,FormicID CNN},
mendeley-tags = {Background Info,FormicID CNN},
month = {may},
number = {7553},
pages = {436--444},
pmid = {26017442},
title = {{Deep learning}},
url = {http://arxiv.org/abs/1603.05691 http://www.nature.com/doifinder/10.1038/nature14539 http://www.ncbi.nlm.nih.gov/pubmed/26017442},
volume = {521},
year = {2015}
}
@article{He2015a,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
eprint = {1512.03385},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/He et al/2015/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:pdf},
isbn = {978-1-4673-6964-0},
issn = {15737721},
journal = {arXiv preprint arXiv:1512.03385},
keywords = {ConvNets,Convolutional neural networks,FormicID CNN,Image steganalysis,Residual learning},
mendeley-tags = {ConvNets,FormicID CNN},
month = {dec},
title = {{Deep Residual Learning for Image Recognition}},
url = {http://arxiv.org/abs/1512.03385},
year = {2015}
}
@article{Chen2016,
abstract = {We learn recurrent neural network optimizers trained on simple synthetic functions by gradient descent. We show that these learned optimizers exhibit a remarkable degree of transfer in that they can be used to efficiently optimize a broad range of derivative-free black-box functions, including Gaussian process bandits, simple control objectives, global optimization benchmarks and hyper-parameter tuning tasks. Up to the training horizon, the learned optimizers learn to trade-off exploration and exploitation, and compare favourably with heavily engineered Bayesian optimization packages for hyper-parameter tuning.},
archivePrefix = {arXiv},
arxivId = {1611.03824},
author = {Chen, Yutian and Hoffman, Matthew W. and Colmenarejo, Sergio Gomez and Denil, Misha and Lillicrap, Timothy P. and Botvinick, Matt and de Freitas, Nando},
eprint = {1611.03824},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Chen et al/2016/Chen et al. - 2016 - Learning to Learn without Gradient Descent by Gradient Descent.pdf:pdf},
isbn = {1011500801515},
issn = {0219-1377},
journal = {arXiv preprint arXiv:1611.03824},
keywords = {FormicID CNN,Optimization},
mendeley-tags = {FormicID CNN,Optimization},
title = {{Learning to Learn without Gradient Descent by Gradient Descent}},
url = {http://arxiv.org/abs/1611.03824},
year = {2016}
}
@article{Szegedy2014,
abstract = {The octavolateralis systems of fishes include the vestibular, auditory, lateral line and electrosensory systems. They are united by common developmental and neuro-computational features, including hair cell sensors and computations based on cross-neuron analyses of differential hair cell stimulation patterns. These systems also all use both spectral and temporal filters to separate signals from each other and from noise, and the distributed senses (lateral line and electroreception) add spatial filters as well. Like all sensory systems, these sensors must provide the animal with guidance for adaptive behavior within a sensory scene composed of multiple stimuli and varying levels of ambient noise, including that created by human activities. In the extreme, anthropogenic activities impact the octavolateralis systems by destroying or degrading the habitats that provide ecological resources and sensory inputs. At slightly lesser levels of effect, anthropogenic pollutants can be damaging to fish tissues, with sensory organs often the most vulnerable. The exposed sensory cells of the lateral line and electrosensory systems are especially sensitive to aquatic pollution. At still lesser levels of impact, anthropogenic activities can act as both acute and chronic stressors, activating hormonal changes that may affect behavioral and sensory function. Finally, human activities are now a nearly ubiquitous presence in aquatic habitats, often with no obvious effects on the animals exposed to them. Ship noise, indigenous and industrial fishing techniques, and all the ancillary noises of human civilization form a major part of the soundscape of fishes. How fish use these new sources of information about their habitat is a new and burgeoning field of study.},
archivePrefix = {arXiv},
arxivId = {1409.4842},
author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
doi = {10.1111/1749-4877.12092},
eprint = {1409.4842},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Szegedy et al/2014/Szegedy et al. - 2014 - Going Deeper with Convolutions.pdf:pdf},
isbn = {9781467369640},
issn = {1749-4877},
journal = {arXiv preprint arXiv:1409.4842},
keywords = {ConvNets,FormicID CNN,audition,electrosense,lateral line,soundscape,underwater noise},
mendeley-tags = {ConvNets,FormicID CNN},
month = {sep},
title = {{Going Deeper with Convolutions}},
url = {http://arxiv.org/abs/1409.4842},
year = {2014}
}
@article{JamesBergstra2012,
abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimiza-tion. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a compar-ison with a large previous study that used grid search and manual search to configure neural net-works and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising con-figuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent " High Throughput " methods achieve surprising success—they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural base-line against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
author = {{James Bergstra} and Bengio, Yoshua},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/James Bergstra, Bengio/2012/James Bergstra, Bengio - 2012 - Random Search for Hyper-Parameter Optimization.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {FormicID CNN,Optimization,deep learning,global optimization,model selection,neural networks,response surface modeling},
mendeley-tags = {FormicID CNN,Optimization},
number = {1},
pages = {281--305},
title = {{Random Search for Hyper-Parameter Optimization}},
volume = {13},
year = {2012}
}
@article{Jaderberg2016,
abstract = {Training directed neural networks typically requires forward-propagating data through a computation graph, followed by backpropagating error signal, to produce weight updates. All layers, or more generally, modules, of the network are therefore locked, in the sense that they must wait for the remainder of the network to execute forwards and propagate error backwards before they can be updated. In this work we break this constraint by decoupling modules by introducing a model of the future computation of the network graph. These models predict what the result of the modelled subgraph will produce using only local information. In particular we focus on modelling error gradients: by using the modelled synthetic gradient in place of true backpropagated error gradients we decouple subgraphs, and can update them independently and asynchronously i.e. we realise decoupled neural interfaces. We show results for feed-forward models, where every layer is trained asynchronously, recurrent neural networks (RNNs) where predicting one's future gradient extends the time over which the RNN can effectively model, and also a hierarchical RNN system with ticking at different timescales. Finally, we demonstrate that in addition to predicting gradients, the same framework can be used to predict inputs, resulting in models which are decoupled in both the forward and backwards pass -- amounting to independent networks which co-learn such that they can be composed into a single functioning corporation.},
archivePrefix = {arXiv},
arxivId = {1608.05343},
author = {Jaderberg, Max and Czarnecki, Wojciech Marian and Osindero, Simon and Vinyals, Oriol and Graves, Alex and Silver, David and Kavukcuoglu, Koray},
eprint = {1608.05343},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Jaderberg et al/2016/Jaderberg et al. - 2016 - Decoupled Neural Interfaces using Synthetic Gradients.pdf:pdf},
issn = {1938-7228},
journal = {arxiv preprint arXiv:1608.05343},
keywords = {FormicID CNN,Optimization},
mendeley-tags = {FormicID CNN,Optimization},
month = {aug},
title = {{Decoupled Neural Interfaces using Synthetic Gradients}},
url = {http://arxiv.org/abs/1608.05343},
year = {2016}
}
@article{Kingma2014,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy},
eprint = {1412.6980},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Kingma, Ba/2014/Kingma, Ba - 2014 - Adam A Method for Stochastic Optimization.pdf:pdf},
isbn = {9781450300728},
issn = {09252312},
journal = {arXiv preprint arXiv:1412.6980},
keywords = {FormicID CNN,Optimization},
mendeley-tags = {FormicID CNN,Optimization},
month = {dec},
title = {{Adam: A Method for Stochastic Optimization}},
url = {http://arxiv.org/abs/1412.6980},
year = {2014}
}
@article{Springenberg2014,
abstract = {Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the "deconvolution approach" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.},
archivePrefix = {arXiv},
arxivId = {1412.6806},
author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
eprint = {1412.6806},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Springenberg et al/2014/Springenberg et al. - 2014 - Striving for Simplicity The All Convolutional Net.pdf:pdf},
isbn = {9781600066634},
issn = {02548704},
journal = {arXiv preprint arXiv:1412.6806},
keywords = {Computer Science - Computer Vision and Pattern Rec,Computer Science - Learning,Computer Science - Neural and Evolutionary Computi,ConvNets,FormicID CNN},
mendeley-tags = {ConvNets,FormicID CNN},
month = {dec},
title = {{Striving for Simplicity: The All Convolutional Net}},
url = {http://arxiv.org/abs/1412.6806},
year = {2014}
}
@article{Ioffe2015,
abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9{\%} top-5 validation error (and 4.8{\%} test error), exceeding the accuracy of human raters.},
archivePrefix = {arXiv},
arxivId = {1502.03167v3},
author = {Ioffe, Sergey and Szegedy, Christian},
eprint = {1502.03167v3},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Ioffe, Szegedy/2015/Ioffe, Szegedy - 2015 - Batch Normalization Accelerating Deep Network Training by Reducing Internal Covariate Shift.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
journal = {arXiv preprint arXiv:1502.03167v3},
keywords = {FormicID CNN,Optimization},
mendeley-tags = {FormicID CNN,Optimization},
month = {feb},
title = {{Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift}},
url = {http://arxiv.org/abs/1502.03167v3},
year = {2015}
}
@incollection{LeCun2012,
abstract = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observedb y practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposedin serious technical publications. This paper gives some of those tricks, ando.ers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most “classical” second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.},
author = {LeCun, Yann A. and Bottou, L{\'{e}}on and Orr, Genevieve B. and M{\"{u}}ller, Klaus-Robert},
booktitle = {Neural networks: Tricks of the trade},
doi = {10.1007/978-3-642-35289-8_3},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/LeCun et al/2012/LeCun et al. - 2012 - Efficient BackProp.pdf:pdf},
isbn = {9783642352881},
issn = {03029743},
keywords = {FormicID CNN,Optimization},
mendeley-tags = {FormicID CNN,Optimization},
pages = {9--48},
publisher = {Springer},
title = {{Efficient BackProp}},
url = {http://link.springer.com/10.1007/978-3-642-35289-8{\_}3},
volume = {7700},
year = {2012}
}
@article{Karpathy2015,
abstract = {Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with finite horizon n-gram models traces the source of the LSTM improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study.},
archivePrefix = {arXiv},
arxivId = {1506.02078v2},
author = {Karpathy, Andrej and Johnson, Justin and Fei-Fei, Li},
eprint = {1506.02078v2},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Karpathy, Johnson, Fei-Fei/2015/Karpathy, Johnson, Fei-Fei - 2015 - Visualizing and Understanding Recurrent Networks.pdf:pdf},
isbn = {978-3-319-10589-5},
issn = {978-3-319-10589-5},
journal = {arXiv preprint arXiv:1506.02078v2},
keywords = {FormicID CNN,Visualizing},
mendeley-tags = {FormicID CNN,Visualizing},
month = {jun},
title = {{Visualizing and Understanding Recurrent Networks}},
url = {http://arxiv.org/abs/1506.02078v2 http://link.springer.com/10.1007/978-3-319-10590-1{\_}53},
year = {2015}
}
@article{He2015,
abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94{\%} top-5 test error on the ImageNet 2012 classification dataset. This is a 26{\%} relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66{\%}). To our knowledge, our result is the first to surpass human-level performance (5.1{\%}, Russakovsky et al.) on this visual recognition challenge.},
archivePrefix = {arXiv},
arxivId = {1502.01852v1},
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
doi = {10.1109/ICCV.2015.123},
eprint = {1502.01852v1},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/He et al/2015/He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Level Performance on ImageNet Classification.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {15505499},
journal = {arXiv preprint arxiv:1502.01852v1},
keywords = {FormicID CNN,Optimization},
mendeley-tags = {FormicID CNN,Optimization},
month = {feb},
publisher = {IEEE},
title = {{Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification}},
url = {http://ieeexplore.ieee.org/document/7410480/ http://arxiv.org/abs/1502.01852v1},
year = {2015}
}
@article{Dozat2016,
abstract = {This work aims to improve upon the recently proposed and rapidly popular-ized optimization algorithm Adam (Kingma {\&} Ba, 2014). Adam has two main components—a momentum component and an adaptive learning rate component. However, regular momentum can be shown conceptually and empirically to be in-ferior to a similar algorithm known as Nesterov's accelerated gradient (NAG). We show how to modify Adam's momentum component to take advantage of insights from NAG, and then we present preliminary evidence suggesting that making this substitution improves the speed of convergence and the quality of the learned mod-els.},
author = {Dozat, Timothy},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Dozat/2016/Dozat - 2016 - Incorporating Nesterov Momentum into Adam.pdf:pdf},
keywords = {FormicID CNN,Optimization},
mendeley-tags = {FormicID CNN,Optimization},
title = {{Incorporating Nesterov Momentum into Adam}},
year = {2016}
}
@article{Sutskever2013,
abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful mod- els that were considered to be almost impos- sible to train using stochastic gradient de- scent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum pa- rameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimiza- tion. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks per- form markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and re- current neural networks from random initial- izations have likely failed due to poor ini- tialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recur- rent network training objectives without the need for sophisticated second-order methods.},
author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Sutskever et al/2013/Sutskever et al. - 2013 - On the importance of initizalization and momentum in deep learning.pdf:pdf},
journal = {International conference on machine learning},
keywords = {FormicID CNN,Optimization},
mendeley-tags = {FormicID CNN,Optimization},
pages = {1139--1147},
title = {{On the importance of initizalization and momentum in deep learning}},
year = {2013}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Srivastava et al/2014/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {FormicID CNN,Optimization,deep learning,model combination,neural networks,regularization},
mendeley-tags = {FormicID CNN,Optimization},
pages = {1929--1958},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@article{Bottou2012,
abstract = {The twenty last years have been marked by an increase in available$\backslash$ndata and computing power. In parallel to this trend, the focus of$\backslash$nneural network research and the practice of training neural networks$\backslash$nhas undergone a number of important changes, for example, use of$\backslash$ndeep learning machines.$\backslash$n$\backslash$nThe second edition of the book augments the first edition with more$\backslash$ntricks, which have resulted from 14 years of theory and experimentation$\backslash$nby some of the world's most prominent neural network researchers.$\backslash$nThese tricks can make a substantial difference (in terms of speed,$\backslash$nease of implementation, and accuracy) when it comes to putting algorithms$\backslash$nto work on real problems.},
archivePrefix = {arXiv},
arxivId = {arXiv:1206.5533v2},
author = {Bottou, L{\'{e}}on},
doi = {10.1007/978-3-642-35289-8},
eprint = {arXiv:1206.5533v2},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Bottou/2012/Bottou - 2012 - Stochastic Gradient Descent Tricks.pdf:pdf},
isbn = {978-3-642-35288-1},
issn = {03029743},
journal = {Neural networks: Tricks of the trade},
keywords = {FormicID CNN,Optimization},
mendeley-tags = {FormicID CNN,Optimization},
pages = {421--436},
pmid = {17348934},
title = {{Stochastic Gradient Descent Tricks}},
url = {http://link.springer.com/10.1007/978-3-642-35289-8},
year = {2012}
}
@article{Bengio2012,
abstract = {Learning algorithms related to artificial neural networks and in particular for Deep Learning may seem to involve many bells and whistles, called hyper-parameters. This chapter is meant as a practical guide with recommendations for some of the most commonly used hyper-parameters, in particular in the context of learning algorithms based on back-propagated gradient and gradient-based optimization. It also discusses how to deal with the fact that more interesting results can be obtained when allowing one to adjust many hyper-parameters. Overall, it describes elements of the practice used to successfully and efficiently train and debug large-scale and often deep multi-layer neural networks. It closes with open questions about the training difficulties observed with deeper architectures.},
address = {Berlin, Heidelberg},
archivePrefix = {arXiv},
arxivId = {1206.5533v2},
author = {Bengio, Yoshua},
doi = {10.1007/978-3-642-35289-8_26},
edition = {2},
editor = {Montavon, Gr{\'{e}}goire and Orr, Genevi{\`{e}}ve B. and M{\"{u}}ller, Klaus-Robert},
eprint = {1206.5533v2},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Bengio/2012/Bengio - 2012 - Practical Recommendations for Gradient-Based Training of Deep Architectures.pdf:pdf},
isbn = {978-3-642-35288-1},
issn = {03029743},
journal = {arxiv preprint arXiv:1206.5533v2},
keywords = {FormicID CNN,Optimization},
mendeley-tags = {FormicID CNN,Optimization},
month = {jun},
publisher = {Springer Berlin Heidelberg},
title = {{Practical Recommendations for Gradient-Based Training of Deep Architectures}},
url = {http://arxiv.org/abs/1206.5533v2 http://link.springer.com/10.1007/978-3-642-35289-8{\_}26 https://doi.org/10.1007/978-3-642-35289-8{\_}26},
year = {2012}
}
@article{Hinton2006,
abstract = {We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
doi = {10.1162/neco.2006.18.7.1527},
eprint = {1111.6189v1},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Hinton, Osindero, Teh/2006/Hinton, Osindero, Teh - 2006 - A fast learning algorithm for deep belief nets.pdf:pdf},
isbn = {0899-7667},
issn = {0899-7667},
journal = {Neural computation},
keywords = {Algorithms,Animals,FormicID CNN,Humans,Learning,Learning: physiology,Neural Networks (Computer),Neurons,Neurons: physiology,Optimization},
mendeley-tags = {FormicID CNN,Optimization},
month = {jul},
number = {7},
pages = {1527--54},
pmid = {16764513},
title = {{A fast learning algorithm for deep belief nets.}},
url = {http://www.mitpressjournals.org/doi/10.1162/neco.2006.18.7.1527 http://www.ncbi.nlm.nih.gov/pubmed/16764513},
volume = {18},
year = {2006}
}
@phdthesis{Pillay2014,
abstract = {Structural analysis is an important step in many document based recognition problem. Structural analysis is performed to associate elements in a document and assign meaning to their association. Handwritten mathematical expression recognition is one such problem which has been studied and researched for long. Many techniques have been researched to build a system that produce high performance mathematical expression recognition. We have presented a novel method to combine multiple structural recognition algorithms in which the combined result shows better performance than each individual recognition algo- rithms. In our experiment we have applied our method to combine multiple mathematical expression recognition parsers called DRACULAE. We have used Graph Transformation Network (GTN) which is a network of function based systems in which each system takes graphs as input, apply function and produces a graph as output. GTN is used to combine multiple DRACULAE parsers and its parameter are tuned using gradient based learning. It has been shown that such a combination method can be used to accentuate the strength of individual algorithms in combination to produce better combination result which higher recognition performance. In our experiment we were able to obtain a highest recogni- tion rate of 74{\%} as compared to best recognition result of 70{\%} from individual DRACULAE parsers. Our experiment also resulted into a maximum of 20{\%} reduction of parent recogni- tion errors and maximum 37{\%} reduction in relation recognition errors between symbols in expressions.},
author = {Pillay, Amit},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Pillay/2014/Pillay - 2014 - Intelligent Combination of Structural Analysis Algorithms Application to Mathematical Expression Recognition.pdf:pdf},
keywords = {FormicID CNN,Others},
mendeley-tags = {FormicID CNN,Others},
pages = {76},
school = {Rochester Institute of technology},
title = {{Intelligent Combination of Structural Analysis Algorithms: Application to Mathematical Expression Recognition}},
year = {2014}
}
@article{Qin2016,
abstract = {Underwater object recognition is in great demand, while the research is far from enough. The unrestricted natural environment makes it a challenging task. We propose a framework to recognize fish from videos captured by underwater cameras deployed in the ocean observation network. First, we extract the foreground via sparse and low-rank matrix decomposition. Then, a deep architecture is used to extract features of the foreground fish images. In this architecture, principal component analysis (PCA) is used in two convolutional layers, followed by binary hashing in the non-linear layer and block-wise histograms in the feature pooling layer. Then spatial pyramid pooling (SPP) is used to extract information invariant to large poses. Finally, a linear SVM classifier is used for the classification. This deep network model can be trained efficiently. On a real-world fish recognition dataset, we achieve the state-of-the-art accuracy of 98.64{\%}.},
author = {Qin, Hongwei and Li, Xiu and Liang, Jian and Peng, Yigang and Zhang, Changshui},
doi = {10.1016/j.neucom.2015.10.122},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Qin et al/2016/Qin et al. - 2016 - DeepFish Accurate underwater live fish recognition with a deep architecture.pdf:pdf},
isbn = {0925-2312},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Cascaded network,ConvNets,Deep learning,FormicID CNN,Object recognition,Underwater},
mendeley-tags = {ConvNets,FormicID CNN},
month = {apr},
pages = {49--58},
publisher = {Elsevier},
title = {{DeepFish: Accurate underwater live fish recognition with a deep architecture}},
url = {http://dx.doi.org/10.1016/j.neucom.2015.10.122 http://linkinghub.elsevier.com/retrieve/pii/S0925231215017312},
volume = {187},
year = {2016}
}
@book{Swamynathan2017,
address = {Berkeley, CA},
author = {Swamynathan, Manohar},
doi = {10.1007/978-1-4842-2866-1},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Swamynathan/2017/Swamynathan - 2017 - Mastering Machine Learning with Python in Six Steps A Practical Implementation Guide to Predictive Data Analytics U.pdf:pdf},
isbn = {978-1-4842-2865-4},
keywords = {FormicID CNN,Python {\&} Machine Learning},
mendeley-tags = {FormicID CNN,Python {\&} Machine Learning},
publisher = {Apress},
title = {{Mastering Machine Learning with Python in Six Steps: A Practical Implementation Guide to Predictive Data Analytics Using Python}},
url = {http://link.springer.com/10.1007/978-1-4842-2866-1},
year = {2017}
}
@book{Coelho2013,
abstract = {This is a tutorial-driven and practical, but well-grounded book showcasing good Machine Learning practices. There will be an emphasis on using existing technologies instead of showing how to write your own implementations of algorithms. This book is a scenario-based, example-driven tutorial. By the end of the book you will have learnt critical aspects of Machine Learning Python projects and experienced the power of ML-based systems by actually working on them.This book primarily targets Python developers who want to learn about and build Machine Learning into their projects, or who want to provide Machine Learning support to their existing projects, and see them get implemented effectively .Computer science researchers, data scientists, Artificial Intelligence programmers, and statistical programmers would equally gain from this book and would learn about effective implementation through lots of the practical examples discussed.Readers need no prior experience with Machine Learning or statistical processing. Python development experience is assumed.},
author = {Coelho, Luis Pedro and Richert, Willi},
edition = {second},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Coelho, Richert/2013/Coelho, Richert - 2013 - Building Machine Learning Systems with Python.pdf:pdf},
isbn = {978-1-78439-277-2},
keywords = {FormicID CNN,Python,Python {\&} Machine Learning},
mendeley-tags = {FormicID CNN,Python {\&} Machine Learning},
pages = {305},
publisher = {Packt Publishing Ltd.},
title = {{Building Machine Learning Systems with Python}},
year = {2013}
}
@article{Su2015,
abstract = {A longstanding question in computer vision concerns the representation of 3D shapes for recognition: should 3D shapes be represented with descriptors operating on their native 3D formats, such as voxel grid or polygon mesh, or can they be effectively represented with view-based descriptors? We address this question in the context of learning to recognize 3D shapes from a collection of their rendered views on 2D images. We first present a standard CNN architecture trained to recognize the shapes' rendered views independently of each other, and show that a 3D shape can be recognized even from a single view at an accuracy far higher than using state-of-the-art 3D shape descriptors. Recognition rates further increase when multiple views of the shapes are provided. In addition, we present a novel CNN architecture that combines information from multiple views of a 3D shape into a single and compact shape descriptor offering even better recognition performance. The same architecture can be applied to accurately recognize human hand-drawn sketches of shapes. We conclude that a collection of 2D views can be highly informative for 3D shape recognition and is amenable to emerging CNN architectures and their derivatives.},
archivePrefix = {arXiv},
arxivId = {1505.00880},
author = {Su, Hang and Maji, Subhransu and Kalogerakis, Evangelos and Learned-Miller, Erik},
doi = {10.1109/ICCV.2015.114},
eprint = {1505.00880},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Su et al/2015/Su et al. - 2015 - Multi-view Convolutional Neural Networks for 3D Shape Recognition.pdf:pdf},
isbn = {978-1-4673-8391-2},
issn = {15505499},
journal = {arXiv preprint arXiv:1505.00880},
keywords = {FormicID CNN,Others},
mendeley-tags = {FormicID CNN,Others},
month = {may},
publisher = {IEEE},
title = {{Multi-view Convolutional Neural Networks for 3D Shape Recognition}},
url = {http://vis-www.cs.umass.edu/mvcnn/docs/su15mvcnn.pdf http://arxiv.org/abs/1505.00880 http://ieeexplore.ieee.org/document/7410471/},
year = {2015}
}
@article{Qi2016,
abstract = {3D shape models are becoming widely available and easier to capture, making available 3D information crucial for progress in object classification. Current state-of-the-art methods rely on CNNs to address this problem. Recently, we witness two types of CNNs being developed: CNNs based upon volumetric representations versus CNNs based upon multi-view representations. Empirical results from these two types of CNNs exhibit a large gap, indicating that existing volumetric CNN architectures and approaches are unable to fully exploit the power of 3D representations. In this paper, we aim to improve both volumetric CNNs and multi-view CNNs according to extensive analysis of existing approaches. To this end, we introduce two distinct network architectures of volumetric CNNs. In addition, we examine multi-view CNNs, where we introduce multi-resolution filtering in 3D. Overall, we are able to outperform current state-of-the-art methods for both volumetric CNNs and multi-view CNNs. We provide extensive experiments designed to evaluate underlying design choices, thus providing a better understanding of the space of methods available for object classification on 3D data.},
archivePrefix = {arXiv},
arxivId = {1604.03265},
author = {Qi, Charles R. and Su, Hao and NieBner, Matthias and Dai, Angela and Yan, Mengyuan and Guibas, Leonidas J.},
doi = {10.1109/CVPR.2016.609},
eprint = {1604.03265},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Qi et al/2016/Qi et al. - 2016 - Volumetric and Multi-view CNNs for Object Classification on 3D Data.pdf:pdf},
isbn = {978-1-4673-8851-1},
issn = {10636919},
journal = {arXiv preprint arXiv:1604.03265},
keywords = {FormicID CNN,Others},
mendeley-tags = {FormicID CNN,Others},
month = {jun},
publisher = {IEEE},
title = {{Volumetric and Multi-view CNNs for Object Classification on 3D Data}},
url = {http://arxiv.org/abs/1604.03265 http://ieeexplore.ieee.org/document/7780978/},
year = {2016}
}
@article{Lee2015,
abstract = {This paper studies convolutional neural networks (CNN) to learn unsupervised feature representations for 44 different plant species, collected at the Royal Botanic Gardens, Kew, England. To gain intuition on the chosen features from the CNN model (opposed to a 'black box' solution), a visualisation technique based on the deconvolutional networks (DN) is utilized. It is found that venations of different order have been chosen to uniquely represent each of the plant species. Experimental results using these CNN features with different classifiers show consistency and superiority compared to the state-of-the art solutions which rely on hand-crafted features.},
archivePrefix = {arXiv},
arxivId = {1506.08425v1},
author = {Lee, Sue Han and Chan, Chee Seng and Wilkin, Paul and Remagnino, Paolo},
doi = {10.1109/ICIP.2015.7350839},
eprint = {1506.08425v1},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Lee et al/2015/Lee et al. - 2015 - Deep-Plant Plant Identification with convolutional neural networks.pdf:pdf},
isbn = {9781479983391},
issn = {15224880},
journal = {arXiv preprint arXiv:1506.08425v1},
keywords = {ConvNets,FormicID CNN,deep learning,feature visualisation,plant classification},
mendeley-tags = {ConvNets,FormicID CNN},
month = {jun},
title = {{Deep-Plant: Plant Identification with convolutional neural networks}},
url = {http://arxiv.org/abs/1506.08425v1},
year = {2015}
}
@article{Krizhevsky2012,
abstract = {We present a fast, fully parameterizable GPU implementation of Convolutional Neural Network variants. Our feature extractors are neither carefully designed nor pre-wired, but rather learned in a supervised way. Our deep hierarchical architectures achieve the best published results on benchmarks for object classification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with error rates of 2.53{\%}, 19.51{\%}, 0.35{\%}, respectively. Deep nets trained by simple back-propagation perform better than more shallow ones. Learning is surprisingly rapid. NORB is completely trained within five epochs. Test error rates on MNIST drop to 2.42{\%}, 0.97{\%} and 0.48{\%} after 1, 3 and 17 epochs, respectively.},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
doi = {10.1145/3065386},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Krizhevsky, Sutskever, Hinton/2012/Krizhevsky, Sutskever, Hinton - 2012 - ImageNet classification with deep convolutional neural networks.pdf:pdf},
isbn = {9781627480031},
issn = {00010782},
journal = {Advances in neural information processing systems},
keywords = {ConvNets,FormicID CNN},
mendeley-tags = {ConvNets,FormicID CNN},
month = {may},
pages = {1097--1105},
title = {{ImageNet classification with deep convolutional neural networks}},
url = {http://dl.acm.org/citation.cfm?doid=3098997.3065386},
year = {2012}
}
@article{Liew2016,
abstract = {An approach using a convolutional neural network (CNN) is proposed for real-time gender classification based on facial images. The proposed CNN architecture exhibits a much reduced design complexity when compared with other CNN solutions applied in pattern recognition. The number of processing layers in the CNN is reduced to only four by fusing the convolutional and subsampling layers. Unlike in conventional CNNs, we replace the convolution operation with cross-correlation, hence reducing the computational load. The network is trained using a second-order backpropagation learning algorithm with annealed global learning rates. Performance evaluation of the proposed CNN solution is conducted on two publicly available face databases of SUMS and AT{\&}T. We achieve classification accuracies of 98.75{\%} and 99.38{\%} on the SUMS and AT{\&}T databases, respectively. The neural network is able to process and classify a 32 × 32 pixel face image in less than 0.27 ms, which corresponds to a very high throughput of over 3700 images per second. Training converges within less than 20 epochs. These results correspond to a superior classification performance, verifying that the proposed CNN is an effective real-time solution for gender recognition. 1. Introduction Gender classification was first perceived as an issue in psychophysical studies; it focuses on the efforts of understanding human visual processing and identifying key features used to categorize between male and female individuals [1]. Research has shown that the disparity between facial masculinity and femininity can be utilized to improve performances of face recognition applications in biometrics, human–computer interactions, surveillance, and computer vision. However, in a real-world environment, the challenge is how to deal with the facial image being affected by the variance in factors such as illumination, pose, facial expression, occlusion, background information, and noise. This is then also the challenge in the development of a robust face-based gender classification system that has high classification accuracy and real-time performance. The conventional approach applied in face recognition, including face-based gender recognition, typically involves the stages of image acquisition and processing, dimensionality reduction, feature extraction, and classification, in that order. Prior knowledge of the application domain is required to determine the best feature extractor to design. In addition, the performance of the recognition system is highly dependent on the type of classifier chosen, which is in turn dependent on the feature extraction method applied. It is difficult},
author = {Liew, Shan Sung and Khallil-Hani, Mohamed and {Adhmad Radzi}, Syafeeza and Bakhteri, Rabia},
doi = {10.3906/elk-1311-58},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Liew et al/2016/Liew et al. - 2016 - Gender classification a convolutional neural network approach.pdf:pdf},
isbn = {1300-0632},
issn = {13000632},
journal = {Turkish Journal of Electrical Engineering {\&} Computer Sciences},
keywords = {ConvNets,FormicID CNN,Gender classification,backprop-agation,convolutional neural network,fused convolutional and subsampling layers},
mendeley-tags = {ConvNets,FormicID CNN},
pages = {1248--1264},
title = {{Gender classification: a convolutional neural network approach}},
url = {http://online.journals.tubitak.gov.tr/openDoiPdf.htm?mKodu=elk-1311-58},
volume = {24},
year = {2016}
}
@article{Mohanty2016,
abstract = {Crop diseases are a major threat to food security, but their rapid identification remains difficult in many parts of the world due to the lack of the necessary infrastructure. The combination of increasing global smartphone penetration and recent advances in computer vision made possible by deep learning has paved the way for smartphone-assisted disease diagnosis. Using a public dataset of 54,306 images of diseased and healthy plant leaves collected under controlled conditions, we train a deep convolutional neural network to identify 14 crop species and 26 diseases (or absence thereof). The trained model achieves an accuracy of 99.35{\%} on a held-out test set, demonstrating the feasibility of this approach. Overall, the approach of training deep learning models on increasingly large and publicly available image datasets presents a clear path towards smartphone-assisted crop disease diagnosis on a massive global scale.},
archivePrefix = {arXiv},
arxivId = {1604.03169},
author = {Mohanty, Sharada P. and Hughes, David P. and Salath{\'{e}}, Marcel},
doi = {10.3389/fpls.2016.01419},
eprint = {1604.03169},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Mohanty, Hughes, Salath{\'{e}}/2016/Mohanty, Hughes, Salath{\'{e}} - 2016 - Using Deep Learning for Image-Based Plant Disease Detection.pdf:pdf},
isbn = {1664-462X},
issn = {1664-462X},
journal = {Frontiers in Plant Science},
keywords = {FormicID CNN,Other ID methods,crop diseases,deep learning,digital epidemiology,machine learning},
mendeley-tags = {FormicID CNN,Other ID methods},
month = {sep},
number = {September},
pages = {1419},
pmid = {27713752},
title = {{Using Deep Learning for Image-Based Plant Disease Detection}},
url = {http://journal.frontiersin.org/Article/10.3389/fpls.2016.01419/abstract},
volume = {7},
year = {2016}
}
@article{Weeks1997a,
abstract = {In this paper we describe a semi-automated digital image analysis system which is capable of discriminating five closely related species of Ichneumonidae. Specimens were distinguished by differences in their wings. The system functions by (a) extracting the significant variation (principal components) among a training set of images of the same species, (b) using these principal components to efficiently represent the morphology of wings of that species, and (c) exploiting the fact that images of the same species will share characteristic principal components, while images of different species will not. Such an approach allows the construction of modular species classifiers, to which like species correlate strongly, while dissimilar species do not. A recognition accuracy of 94{\%} was achieved when the system was tested on 175 images of wings of the five ichneumonids. The wing images were caricatured to accentuate their venation and pigmentation patterns.},
author = {Weeks, P.J.D. J. D. and Gauld, I.D. Ian David and Gaston, Kevin J. K.J. and O'Neill, Mark A. M.a.},
doi = {10.1017/S000748530002736X},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Weeks et al/1997/Weeks et al. - 1997 - Automating the identification of insects a new solution to an old problem.pdf:pdf},
isbn = {0007-4853},
issn = {0007-4853},
journal = {Bulletin of Entomological Research},
keywords = {Background Info,FormicID CNN},
mendeley-tags = {Background Info,FormicID CNN},
month = {apr},
number = {02},
pages = {203--211},
publisher = {Library African Studies Centre},
title = {{Automating the identification of insects: a new solution to an old problem}},
url = {http://www.journals.cambridge.org/abstract{\_}S000748530002736X},
volume = {87},
year = {1997}
}
@article{Affonso2017,
abstract = {A number of industries use human inspection to visually classify the quality of their products and the raw materials used in the production process, this process could be done automatically through digital image processing. The industries are not always interested in the most accurate technique for a given problem, but most appropriate for the expected results, there must be a balance between accuracy and computational cost. This paper investigates the classification of the quality of wood boards based on their images. For such, it compares the use of deep learning, particularly Convolutional Neural Networks, with the combination of texture-based feature extraction techniques and traditional techniques: Decision tree induction algorithms, Neural Networks, Nearest neighbors and Support vector machines. Reported studies show that Deep Learning techniques applied to image processing tasks have achieved predictive performance superior to traditional classification techniques, mainly in high complex scenarios. One of the reasons pointed out is their embedded feature extraction mechanism. Deep Learning techniques directly identify and extract features, considered by them to be relevant, in a given image dataset. However, empirical results for the image data set have shown that the texture descriptor method proposed, regardless of the strategy employed is very competitive when compared with Convolutional Neural Network for all the performed experiments. The best performance of the texture descriptor method could be caused by the nature of the image dataset. Finally are pointed out some perspectives of futures developments with the application of Active learning and Semi supervised methods.},
author = {Affonso, Carlos and Rossi, Andr{\'{e}} Luis Debiaso and Vieira, F{\'{a}}bio Henrique Antunes and de Carvalho, Andr{\'{e}} Carlos Ponce de Leon Ferreira},
doi = {10.1016/j.eswa.2017.05.039},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Affonso et al/2017/Affonso et al. - 2017 - Deep learning for biological image classification.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Background Info,Deep learning,FormicID CNN,Image classification,Machine learning,Wood classification},
mendeley-tags = {Background Info,FormicID CNN},
month = {nov},
pages = {114--122},
publisher = {Elsevier Ltd},
title = {{Deep learning for biological image classification}},
url = {http://dx.doi.org/10.1016/j.eswa.2017.05.039 http://linkinghub.elsevier.com/retrieve/pii/S0957417417303627},
volume = {85},
year = {2017}
}
@inproceedings{Apriyanti2013,
abstract = {In this paper, we developed the system for recognizing the orchid$\backslash$nspecies by using the images of flower. We used MSRM (Maximal Similarity$\backslash$nbased on Region Merging) method for segmenting the flower object$\backslash$nfrom the background and extracting the shape feature such as the$\backslash$ndistance from the edge to the centroid point of the flower, aspect$\backslash$nratio, roundness, moment invariant, fractal dimension and also extract$\backslash$ncolor feature. We used HSV color feature with ignoring the V value.$\backslash$nTo retrieve the image, we used Support Vector Machine (SVM) method.$\backslash$nOrchid is a unique flower. It has a part of flower called lip (labellum)$\backslash$nthat distinguishes it from other flowers even from other types of$\backslash$norchids. Thus, in this paper, we proposed to do feature extraction$\backslash$nnot only on flower region but also on lip (labellum) region. The$\backslash$nresult shows that our proposed method can increase the accuracy value$\backslash$nof content based flower image retrieval for orchid species up to$\backslash$n� 14{\%}. The most dominant feature is Centroid Contour Distance, Moment$\backslash$nInvariant and HSV Color. The system accuracy is 85,33{\%} in validation$\backslash$nphase and 79,33{\%} in testing phase.},
archivePrefix = {arXiv},
arxivId = {1406.2580},
author = {Apriyanti, Diah Harnoni and Arymurthy, Aniati Murni and Handoko, Laksana Tri},
booktitle = {2013 International Conference on Computer, Control, Informatics and Its Applications},
doi = {10.1109/IC3INA.2013.6819148},
eprint = {1406.2580},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Apriyanti, Arymurthy, Handoko/2013/Apriyanti, Arymurthy, Handoko - 2013 - Identification of orchid species using content-based flower image retrieval.pdf:pdf},
isbn = {9781479910786},
keywords = {FormicID CNN,HSV color,Other ID methods,Support Vector Machine,centroid contour distance,flower image retrieval,orchid},
mendeley-tags = {FormicID CNN,Other ID methods},
month = {nov},
pages = {53--57},
publisher = {IEEE},
title = {{Identification of orchid species using content-based flower image retrieval}},
url = {http://arxiv.org/abs/1406.2580 http://dx.doi.org/10.1109/IC3INA.2013.6819148 http://ieeexplore.ieee.org/document/6819148/},
year = {2013}
}
@article{Watson2004,
abstract = {Two hundred and thirty-seven species of Macrolepidoptera were light trapped at Treborth Botanical Garden, Gwynedd, UK. Live adults were digitally imaged using a simple, inexpensive method suitable for field use, then released. Inconsistent lighting, variation in resting posture and inclusion of worn individuals produced image sets high in intraspecific variation. Thirty-five common species were selected to provide training images for the Digital Automated Identification SYstem (DAISY). Twenty individuals per species were pre-processed to standardize size and posture and to enhance features. The right forewing of each was highlighted manually and the pattern rendered polar and greyscale for DAISY analysis. Despite poor quality of some images, 83{\%} of unknown species were identified correctly. The best species had 100{\%} correct identification and the worst 35{\%}. The most poorly identified images were those of moths that had lost scales or been unevenly illuminated. The precision with which the forewing was highlighted affected performance. When highlighted carefully, Laothoe populi was identified correctly twice as successfully as when the same image was highlighted poorly. Size of the training set was also important. Sets of 5, 10, 15 and 20 training images, plotted against performance produced a curve of diminishing returns. Colour images and inclusion of size should improve accuracy.},
author = {Watson, Anna T. and O'Neill, Mark A. and Kitching, Ian J.},
doi = {10.1017/S1477200003001208},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Watson, O'Neill, Kitching/2004/Watson, O'Neill, Kitching - 2004 - Automated identification of live moths (Macrolepidoptera) using digital automated identification Syst.pdf:pdf},
isbn = {1478-0933},
issn = {1477-2000},
journal = {Systematics and Biodiversity},
keywords = {FormicID CNN,Other ID methods,automated identification,biodiversity inventorying,computer-aided,image analysis,macrolepidoptera,taxonomy},
mendeley-tags = {FormicID CNN,Other ID methods},
month = {feb},
number = {3},
pages = {287--300},
title = {{Automated identification of live moths (Macrolepidoptera) using digital automated identification System (DAISY)}},
url = {http://www.tandfonline.com/doi/abs/10.1017/S1477200003001208},
volume = {1},
year = {2004}
}
@article{Dyrmann2016,
abstract = {Information on which weed species are present within agricultural fields is important for site specific weed management. This paper presents a method that is capable of recognising plant species in colour images by using a convolutional neural network. The network is built from scratch trained and tested on a total of 10,413 images containing 22 weed and crop species at early growth stages. These images originate from six different data sets, which have variations with respect to lighting, resolution, and soil type. This includes images taken under controlled conditions with regard to camera stabilisation and illumination, and images shot with hand-held mobile phones in fields with changing lighting conditions and different soil types. For these 22 species, the network is able to achieve a classification accuracy of 86.2{\%}.},
author = {Dyrmann, Mads and Karstoft, Henrik and Midtiby, Henrik Skov},
doi = {10.1016/j.biosystemseng.2016.08.024},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Dyrmann, Karstoft, Midtiby/2016/Dyrmann, Karstoft, Midtiby - 2016 - Plant species classification using deep convolutional neural network.pdf:pdf},
issn = {15375110},
journal = {Biosystems Engineering},
keywords = {ConvNets,Convolutional Neural,Deep learning,FormicID CNN,Networks,Plant classification,Weed control},
mendeley-tags = {ConvNets,FormicID CNN},
month = {nov},
number = {2005},
pages = {72--80},
publisher = {Elsevier Ltd},
title = {{Plant species classification using deep convolutional neural network}},
url = {http://dx.doi.org/10.1016/j.biosystemseng.2016.08.024 http://linkinghub.elsevier.com/retrieve/pii/S1537511016301465},
volume = {151},
year = {2016}
}
@article{Jain2015,
abstract = {The purpose of this Paper is to describe our research on different feature extraction and matching techniques in designing a Content Based Image Retrieval/Processing system. Due to the enormous increase in image database sizes, as well as its vast deployment in various applications, the need for CBIR development arose. This paper outlines a description of primitive feature extraction techniques like: texture, color, and shape. Once these features are extracted and used as the basis for a similarity check between images, the various matching techniques are discussed. This paper proposes novel system architecture for CBIR system which combines techniques including content based image and color analysis, as well as data mining techniques. This is a study to propose segmentation module to build the CBIR system. It also includes concept of neighborhood color analysis module which also recognizes the side of every grids of image. The study also includes the implementation of 4 algorithms namely: -K-MEANS, SIFT, SURF and BRIEF algorithm.},
author = {Jain, Sahil and Pulaparthi, Kiranmai and Fulara, Chetan},
issn = {2309-4893},
journal = {International Journal of Advanced Engineering and Global Technology I},
keywords = {BRIEF,Content based images retrieval,Feature extraction,FormicID CNN,Image retrieval,K-Means Clustering,Others,SIFT,SURF},
mendeley-tags = {FormicID CNN,Others},
number = {10},
title = {{Content based image retrieval}},
volume = {03},
year = {2015}
}
@inproceedings{Wang2009,
abstract = {Image classification and annotation are important prob- lems in computer vision, but rarely considered together. In- tuitively, annotations provide evidence for the class label, and the class label provides evidence for annotations. For example, an image of class highway is more likely anno- tated with words “road,” “car,” and “traffic” than words “fish,” “boat,” and “scuba.” In this paper, we develop a new probabilistic model for jointly modeling the image, its class label, and its annotations. Our model treats the class label as a global description of the image, and treats an- notation terms as local descriptions of parts of the image. Its underlying probabilistic assumptions naturally integrate these two sources of information. We derive an approximate inference and estimation algorithms based on variational methods, as well as efficient approximations for classifying and annotating new images. We examine the performance of our model on two real-world image data sets, illustrating that a single model provides competitive annotation perfor- mance, and superior classification performance.},
author = {Wang, Chong and Blei, David and Fei-Fei, Li},
booktitle = {2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, CVPR Workshops 2009},
doi = {10.1109/CVPRW.2009.5206800},
isbn = {9781424439935},
issn = {1063-6919},
keywords = {FormicID CNN,Others},
mendeley-tags = {FormicID CNN,Others},
pages = {1903--1910},
pmid = {1000198810},
title = {{Simultaneous image classification and annotation}},
year = {2009}
}
@article{Francoy2008,
abstract = {Currently available morphometric and genetic techniques that can accurately identify African- ized honey bees are both costly and time consuming. We tested two new morphometric techniques (ABIS Automatic Bee Identification System and geometric morphometrics analysis) on samples consisting of digital images of five worker forewings per colony. These were collected from 394 colonies of Africanized bees from all over Brazil and from colonies of African bees, Apis mellifera scutellata (n = 14), and Euro- pean bees, A. m. ligustica (n = 10), A. m. mellifera (n = 15), and A. m. carnica (n=15) from the Ruttner collection in Oberursel, Germany (preserved specimens). Both methods required less than five minutes per sample, giving more than 99{\%} correct identifications. There was just one misidentification (based on ge- ometric morphometrics analysis) of Africanized bees compared with European subspecies, which would be the principal concern in newly-colonized areas, such as the southern USA. These new techniques are inexpensive, fast and precise.},
author = {Francoy, Tiago Mauricio and Wittmann, Dieter and Drauschke, Martin and M{\"{u}}ller, Stefan and Steinhage, Volker and Bezerra-Laure, Marcela A. F. and {De Jong}, David and Gon{\c{c}}alves, Lionel Segui},
doi = {10.1051/apido:2008028},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Tiago Mauricio et al/2008/Tiago Mauricio et al. - 2008 - Original article Identification of Africanized honey bees through wing morphometrics two fast and effici.pdf:pdf},
isbn = {0044-8435$\backslash$r1297-9678},
issn = {0044-8435},
journal = {Apidologie},
keywords = {FormicID CNN,Other ID methods},
mendeley-tags = {FormicID CNN,Other ID methods},
month = {sep},
number = {5},
pages = {488--494},
title = {{Identification of Africanized honey bees through wing morphometrics: two fast and efficient procedures}},
url = {http://www.apidologie.org http://link.springer.com/10.1051/apido:2008028},
volume = {39},
year = {2008}
}
@misc{idBee,
keywords = {FormicID CNN,Other ID methods},
mendeley-tags = {FormicID CNN,Other ID methods},
title = {{idBee - Automatic Bee Identification from Wing Venation}},
url = {https://www.engr.wisc.edu/},
urldate = {2017-09-14}
}
@book{MacLeod2007,
abstract = {The automated identification of biological objects or groups has been a dream among taxonomists and systematists for centuries. However, progress in designing and implementing practical systems for fully automated taxon identification has been frustratingly slow. Regardless, the dream has never died. Recent developments in computer architectures and innovations in software design have placed the tools needed to realize this vision in the hands of the systematics community, not several years hence, but now. And not just for DNA barcodes or other molecular data, but for digital images of organisms, digital sounds, digitized chemical data - essentially any type of digital data.Based on evidence accumulated over the last decade and written by applied researchers, Automated Taxon Identification in Systematics explores contemporary applications of quantitative approaches to the problem of taxon recognition. The book begins by reviewing the current state of systematics and placing automated taxon identification in the context of contemporary trends, needs, and opportunities. The chapters present and evaluate different aspects of current automated system designs. They then provide descriptions of case studies in which different theoretical and practical aspects of the overall group-identification problem are identified, analyzed, and discussed.A recurring theme through the chapters is the relationship between taxonomic identification, automated group identification, and morphometrics. This collection provides a bridge between these communities and between them and the wider world of applied taxonomy. The only book-length treatment that explores automated group identification in systematic context, this text also includes introductions to basic aspects of the fields of contemporary artificial intelligence and mathematical group recognition for the entire biological community.},
author = {MacLeod, Norman},
booktitle = {Systematics Association},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/MacLeod/2007/MacLeod - 2007 - Automated Taxon Identification in Systematics Theory, Approaches and Applications.pdf:pdf},
isbn = {9780849382055},
issn = {0033-5770},
keywords = {Background Info,FormicID CNN},
mendeley-tags = {Background Info,FormicID CNN},
pages = {339},
title = {{Automated Taxon Identification in Systematics: Theory, Approaches and Applications}},
url = {http://books.google.com/books?id=Fz6v8YPyQQgC{\&}pgis=1},
year = {2007}
}
@article{Wang2012a,
abstract = {A new automatic identification system has been designed to identify insect specimen images at the order level. Several relative features were designed according to the methods of digital image progressing, pattern recognition and the theory of taxonomy. Artificial neural networks (ANNs) and a support vector machine (SVM) are used as pattern recognition methods for the identification tests. During tests on nine common orders and sub-orders with an artificial neural network, the system performed with good stability and accuracy reached 93{\%}. Results from tests using the support vector machine further improved accuracy. We also did tests on eight- and nine-orders with different features and based on these results we compare the advantages and disadvantages of our system and provide some advice for future research on insect image recognition. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
author = {Wang, Jiangning and Lin, Congtian and Ji, Liqiang and Liang, Aiping},
doi = {10.1016/j.knosys.2012.03.014},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Wang et al/2012/Wang et al. - 2012 - A new automatic identification system of insect images at the order level.pdf:pdf},
isbn = {09507051},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {ANN,Feature,FormicID CNN,Insect,Order,Other ID methods,SVM},
mendeley-tags = {FormicID CNN,Other ID methods},
month = {sep},
pages = {102--110},
publisher = {Elsevier B.V.},
title = {{A new automatic identification system of insect images at the order level}},
url = {http://dx.doi.org/10.1016/j.knosys.2012.03.014 http://linkinghub.elsevier.com/retrieve/pii/S0950705112000822},
volume = {33},
year = {2012}
}
@article{Wang2012,
abstract = {There is increasing interest in the automatic identification of insect species from images. Here content-based image retrieval (CBIR) is applied because of its capacity for mass processing and operability. A series of shape, colour and texture features was developed that draw on CBIR and allow the identification of butterfly images to the taxonomic scale of family. In our test the accuracy of Papilionidae reached 84{\%} indicating that CBIR is suitable for the identification of butterflies at the family level. Furthermore, experiments with different features, feature weights and similarity matching algorithms were compared. Testing revealed that data attributes such as species diversity, image quality and resolution affected system success the most, followed by features and match algorithms; shape features are more important than colour or texture features in the identification of butterfly families. These findings are important to future improvements in this technology and its applicability. {\textcopyright} 2011 IAgrE.},
author = {Wang, Jiangning and Ji, Liqiang and Liang, Aiping and Yuan, Decheng},
doi = {10.1016/j.biosystemseng.2011.10.003},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Wang et al/2012/Wang et al. - 2012 - The identification of butterfly families using content-based image retrieval.pdf:pdf},
issn = {15375110},
journal = {Biosystems Engineering},
keywords = {FormicID CNN},
mendeley-tags = {FormicID CNN},
month = {jan},
number = {1},
pages = {24--32},
publisher = {IAgrE},
title = {{The identification of butterfly families using content-based image retrieval}},
url = {http://dx.doi.org/10.1016/j.biosystemseng.2011.10.003 http://linkinghub.elsevier.com/retrieve/pii/S1537511011001784},
volume = {111},
year = {2012}
}
@article{Wen2012,
abstract = {Insect identification and classification is time-consuming work requiring expert knowledge for integrated pest management in orchards. An image-based automated insect identification and classification method is described in the paper. The complete method includes three models. An invariant local feature model was built for insect identification and classification using affine invariant local features; a global feature model was built for insect identification and classification using 54 global features; and a hierarchical combination model was proposed based on local feature and global feature models to combine advantages of the two models and increase performance. The three models were applied and tested for insect classification on eight insect species from pest colonies and orchards. The hierarchical combination model yielded better performance over global and local models. Moreover, to study the pose change of insects on traps and the hypothesis that an optimal time to acquire and image after landing exists, advanced analysis on time-dependent pose change of insects on traps is included in this study. The experimental results on field insect image classification with field-based images for training achieved the classification rate of 86.6{\%} when testing with the combination model. This demonstrates the image-based insect identification and classification method could be a potential way for automated insect classification in integrated pest management. ?? 2012 Elsevier B.V.},
author = {Wen, Chenglu and Guyer, Daniel},
doi = {10.1016/j.compag.2012.08.008},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Wen, Guyer/2012/Wen, Guyer - 2012 - Image-based orchard insect automated identification and classification method.pdf:pdf},
issn = {01681699},
journal = {Computers and Electronics in Agriculture},
keywords = {FormicID CNN,Global feature,Image processing,Insect classification,Integrated pest management,Local feature,Other ID methods},
mendeley-tags = {FormicID CNN,Other ID methods},
month = {nov},
pages = {110--115},
publisher = {Elsevier B.V.},
title = {{Image-based orchard insect automated identification and classification method}},
url = {http://dx.doi.org/10.1016/j.compag.2012.08.008 http://linkinghub.elsevier.com/retrieve/pii/S0168169912002190},
volume = {89},
year = {2012}
}
@article{Zhou2013,
abstract = {BACKGROUND Pattern recognition algorithms are useful in bioimage informatics applications such as quantifying cellular and subcellular objects, annotating gene expressions, and classifying phenotypes. To provide effective and efficient image classification and annotation for the ever-increasing microscopic images, it is desirable to have tools that can combine and compare various algorithms, and build customizable solution for different biological problems. However, current tools often offer a limited solution in generating user-friendly and extensible tools for annotating higher dimensional images that correspond to multiple complicated categories. RESULTS We develop the BIOimage Classification and Annotation Tool (BIOCAT). It is able to apply pattern recognition algorithms to two- and three-dimensional biological image sets as well as regions of interest (ROIs) in individual images for automatic classification and annotation. We also propose a 3D anisotropic wavelet feature extractor for extracting textural features from 3D images with xy-z resolution disparity. The extractor is one of the about 20 built-in algorithms of feature extractors, selectors and classifiers in BIOCAT. The algorithms are modularized so that they can be "chained" in a customizable way to form adaptive solution for various problems, and the plugin-based extensibility gives the tool an open architecture to incorporate future algorithms. We have applied BIOCAT to classification and annotation of images and ROIs of different properties with applications in cell biology and neuroscience. CONCLUSIONS BIOCAT provides a user-friendly, portable platform for pattern recognition based biological image classification of two- and three- dimensional images and ROIs. We show, via diverse case studies, that different algorithms and their combinations have different suitability for various problems. The customizability of BIOCAT is thus expected to be useful for providing effective and efficient solutions for a variety of biological problems involving image classification and annotation. We also demonstrate the effectiveness of 3D anisotropic wavelet in classifying both 3D image sets and ROIs.},
author = {Zhou, Jie and Lamichhane, Santosh and Sterne, Gabriella and Ye, Bing and Peng, Hanchuan},
doi = {10.1186/1471-2105-14-291},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Zhou et al/2013/Zhou et al. - 2013 - BIOCAT a pattern recognition platform for customizable biological image classification and annotation.pdf:pdf},
isbn = {1471-2105 (Electronic)},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {FormicID CNN,Other ID methods},
mendeley-tags = {FormicID CNN,Other ID methods},
month = {oct},
number = {1},
pages = {291},
pmid = {24090164},
title = {{BIOCAT: a pattern recognition platform for customizable biological image classification and annotation.}},
url = {http://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-14-291 http://www.ncbi.nlm.nih.gov/pubmed/24090164 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC3854450},
volume = {14},
year = {2013}
}
@article{Yang2015,
abstract = {For some insect groups, wing outline is an important character for species identification. We have constructed a program as the integral part of an automated system to identify insects based on wing outlines (DAIIS). This program includes two main functions: (1) outline digitization and Elliptic Fourier transformation and (2) classifier model training by pattern recognition of support vector machines and model validation. To demonstrate the utility of this program, a sample of 120 owlflies (Neuroptera: Ascalaphidae) was split into training and validation sets. After training, the sample was sorted into seven species using this tool. In five repeated experiments, the mean accuracy for identification of each species ranged from 90{\%} to 98{\%}. The accuracy increased to 99{\%} when the samples were first divided into two groups based on features of their compound eyes. DAIIS can therefore be a useful tool for developing a system of automated insect identification.},
author = {Yang, He-Ping and Ma, Chun-Sen and Wen, Hui and Zhan, Qing-Bin and Wang, Xin-Li},
doi = {10.1038/srep12786},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/Yang et al/2015/Yang et al. - 2015 - A tool for developing an automatic insect identification system based on wing outlines.pdf:pdf},
issn = {2045-2322},
journal = {Scientific reports},
keywords = {FormicID CNN,Other ID methods},
mendeley-tags = {FormicID CNN,Other ID methods},
month = {aug},
number = {1},
pages = {12786},
pmid = {26251292},
publisher = {Nature Publishing Group},
title = {{A tool for developing an automatic insect identification system based on wing outlines.}},
url = {http://www.nature.com/srep/2015/150807/srep12786/full/srep12786.html http://www.nature.com/articles/srep12786 http://www.ncbi.nlm.nih.gov/pubmed/26251292 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC4528224},
volume = {5},
year = {2015}
}
@misc{AntWeb,
author = {AntWeb.org},
title = {{AntWeb}},
url = {http://www.antweb.org},
urldate = {2017-01-22}
}
@book{Holldobler1990,
abstract = {Reviews in detail all topics in the anatomy, physiology, social organization, ecology, and natural history of ants.},
address = {Cambridge, Mass.},
author = {H{\"{o}}lldobler, B. and Wilson, E.O.},
booktitle = {The Belknap Press of Harvard University Press},
file = {:/fs-smb-019.ad.naturalis.nl/homedir/Marijn.Boer/Mendeley Desktop/H{\"{o}}lldobler, Wilson/1990/H{\"{o}}lldobler, Wilson - 1990 - The Ants.pdf:pdf},
isbn = {0-674-04075-9},
issn = {00138746},
language = {English},
publisher = {Belknap Press of Harvard University Press},
title = {{The Ants}},
year = {1990}
}
